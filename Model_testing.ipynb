{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"YPpbxQ2sE_B5","executionInfo":{"status":"ok","timestamp":1765611151586,"user_tz":-330,"elapsed":6131,"user":{"displayName":"Ishu Rajput","userId":"09455974081349711484"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6d44b1bc-9db4-456f-ad34-dd5292cc1529"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/212.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m212.4/212.4 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# Cell 1: Install Dependencies\n","!pip install gradio opencv-python-headless matplotlib ultralytics supervision -q"]},{"cell_type":"code","source":["# Cell 2: Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sz9cVZlkP07b","executionInfo":{"status":"ok","timestamp":1765611194555,"user_tz":-330,"elapsed":41904,"user":{"displayName":"Ishu Rajput","userId":"09455974081349711484"}},"outputId":"9e502ccd-7c16-4082-b20b-3725441529cb"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Cell 3: Import Libraries\n","import cv2\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torchvision import models\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","from matplotlib import cm\n","import gradio as gr\n","from PIL import Image\n","import time\n","from collections import defaultdict\n","from ultralytics import YOLO\n","import supervision as sv\n","from datetime import datetime\n","import os\n","\n","# Check device\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"‚úì Using device: {DEVICE}\")\n","\n","# Model training parameters (MUST match your training config)\n","TARGET_SIZE = (512, 512)  # (Width, Height)\n","GT_DOWNSAMPLE = 8\n","\n","# ImageNet normalization constants\n","IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","IMAGENET_STD = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YMQo376UFW0V","executionInfo":{"status":"ok","timestamp":1765611219511,"user_tz":-330,"elapsed":21759,"user":{"displayName":"Ishu Rajput","userId":"09455974081349711484"}},"outputId":"980ebe33-eebb-46d3-f8cd-6026ef218fb3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","‚úì Using device: cuda\n"]}]},{"cell_type":"code","source":["# Cell 4: CSRNet Model Architecture\n","def create_csrnet():\n","    \"\"\"Create CSRNet model architecture\"\"\"\n","    print(\"Building CSRNet architecture...\")\n","\n","    vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n","    features = list(vgg.features.children())\n","\n","    frontend = nn.Sequential(*features[0:23])\n","\n","    backend = nn.Sequential(\n","        nn.Conv2d(512, 512, kernel_size=3, padding=1, dilation=1),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(512, 512, kernel_size=3, padding=4, dilation=4),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(512, 256, kernel_size=3, padding=1, dilation=1),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(256, 128, kernel_size=3, padding=1, dilation=1),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(128, 1, kernel_size=1, padding=0),\n","    )\n","\n","    model = nn.Sequential(frontend, backend)\n","    print(\"‚úì CSRNet architecture created\")\n","\n","    return model\n","\n","\n","def load_trained_model(model_path, device=DEVICE):\n","    \"\"\"Load trained CSRNet model\"\"\"\n","    print(f\"Loading CSRNet from: {model_path}\")\n","\n","    model = create_csrnet()\n","    checkpoint = torch.load(model_path, map_location=device)\n","\n","    if 'model_state' in checkpoint:\n","        model.load_state_dict(checkpoint['model_state'])\n","        print(f\"‚úì Loaded checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n","    else:\n","        model.load_state_dict(checkpoint)\n","        print(\"‚úì Loaded model weights\")\n","\n","    model.to(device)\n","    model.eval()\n","\n","    print(f\"‚úì CSRNet ready on {device}\")\n","    return model"],"metadata":{"id":"PCp6DzIoFYUQ","executionInfo":{"status":"ok","timestamp":1765611237311,"user_tz":-330,"elapsed":10,"user":{"displayName":"Ishu Rajput","userId":"09455974081349711484"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Cell 5: Person Tracker Class\n","class PersonTracker:\n","    \"\"\"\n","    Multi-object tracker to prevent re-counting same people\n","    Uses ByteTrack algorithm via supervision library\n","    \"\"\"\n","\n","    def __init__(self):\n","        # Initialize ByteTrack tracker\n","        self.tracker = sv.ByteTrack(\n","            track_activation_threshold=0.25,\n","            lost_track_buffer=30,\n","            minimum_matching_threshold=0.8,\n","            frame_rate=30\n","        )\n","\n","        # Track statistics\n","        self.unique_ids = set()\n","        self.track_history = defaultdict(list)\n","\n","    def update(self, detections):\n","        \"\"\"\n","        Update tracker with new detections\n","        Args:\n","            detections: supervision Detections object\n","        Returns:\n","            tracked_detections: Detections with tracker_id\n","        \"\"\"\n","        tracked_detections = self.tracker.update_with_detections(detections)\n","\n","        # Update unique IDs\n","        if tracked_detections.tracker_id is not None:\n","            for track_id in tracked_detections.tracker_id:\n","                self.unique_ids.add(int(track_id))\n","\n","        return tracked_detections\n","\n","    def get_total_unique_count(self):\n","        \"\"\"Get total unique people seen\"\"\"\n","        return len(self.unique_ids)\n","\n","    def reset(self):\n","        \"\"\"Reset tracker\"\"\"\n","        self.tracker = sv.ByteTrack(\n","            track_activation_threshold=0.25,\n","            lost_track_buffer=30,\n","            minimum_matching_threshold=0.8,\n","            frame_rate=30\n","        )\n","        self.unique_ids = set()\n","        self.track_history.clear()"],"metadata":{"id":"SJk-t2rhFeyD","executionInfo":{"status":"ok","timestamp":1765611241547,"user_tz":-330,"elapsed":11,"user":{"displayName":"Ishu Rajput","userId":"09455974081349711484"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Cell 6: Enhanced Crowd Counter with Detection\n","class EnhancedCrowdCounter:\n","    \"\"\"\n","    Enhanced counter with both density estimation (CSRNet)\n","    and object detection (YOLOv8) + tracking\n","    \"\"\"\n","\n","    def __init__(self, csrnet_model, device=DEVICE):\n","        self.csrnet = csrnet_model\n","        self.device = device\n","        self.mean = IMAGENET_MEAN\n","        self.std = IMAGENET_STD\n","        self.target_size = TARGET_SIZE\n","\n","        # Load YOLOv8 for person detection\n","        print(\"Loading YOLOv8 person detector...\")\n","        self.yolo = YOLO('yolov8n.pt')  # Nano model for speed\n","        print(\"‚úì YOLOv8 loaded\")\n","\n","        # Initialize tracker\n","        self.tracker = PersonTracker()\n","\n","        # Annotators for visualization\n","        self.box_annotator = sv.BoxAnnotator(thickness=2)\n","        self.trace_annotator = sv.TraceAnnotator(thickness=2, trace_length=50)\n","\n","    def preprocess_frame(self, frame):\n","        \"\"\"Preprocess frame for CSRNet - resize to training size\"\"\"\n","        # Resize to TARGET_SIZE used during training\n","        frame_resized = cv2.resize(frame, self.target_size, interpolation=cv2.INTER_LINEAR)\n","\n","        img_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n","        img_normalized = img_rgb.astype(np.float32) / 255.0\n","        img_normalized = (img_normalized - self.mean) / self.std\n","        img_tensor = torch.from_numpy(img_normalized).permute(2, 0, 1).unsqueeze(0)\n","        img_tensor = img_tensor.to(self.device, dtype=torch.float32)\n","        return img_tensor\n","\n","    def predict_density(self, frame):\n","        \"\"\"Get density map from CSRNet\"\"\"\n","        with torch.no_grad():\n","            img_tensor = self.preprocess_frame(frame)\n","            density_map = self.csrnet(img_tensor)\n","            density_np = density_map.squeeze().cpu().numpy()\n","            count = float(density_np.sum())\n","        return density_np, count\n","\n","    def detect_people(self, frame):\n","        \"\"\"\n","        Detect people using YOLOv8\n","        Returns: supervision Detections object\n","        \"\"\"\n","        results = self.yolo(frame, classes=[0], verbose=False)[0]  # class 0 = person\n","        detections = sv.Detections.from_ultralytics(results)\n","        return detections\n","\n","    def predict_with_tracking(self, frame):\n","        \"\"\"\n","        Complete prediction pipeline:\n","        1. Detect people with YOLOv8\n","        2. Track people to avoid re-counting\n","        3. Get density map from CSRNet\n","\n","        Returns:\n","            annotated_frame: Frame with bounding boxes and IDs\n","            density_map: CSRNet density map\n","            detection_count: Current frame person count\n","            unique_count: Total unique people tracked\n","            density_count: CSRNet estimate\n","        \"\"\"\n","        # Detect people\n","        detections = self.detect_people(frame)\n","\n","        # Track people\n","        tracked_detections = self.tracker.update(detections)\n","\n","        # Get counts\n","        detection_count = len(tracked_detections)\n","        unique_count = self.tracker.get_total_unique_count()\n","\n","        # Get density map\n","        density_map, density_count = self.predict_density(frame)\n","\n","        # Annotate frame\n","        annotated_frame = frame.copy()\n","\n","        # Draw bounding boxes and labels\n","        if len(tracked_detections) > 0:\n","            # Draw boxes first\n","            annotated_frame = self.box_annotator.annotate(\n","                scene=annotated_frame,\n","                detections=tracked_detections\n","            )\n","\n","            # Draw tracking traces\n","            annotated_frame = self.trace_annotator.annotate(\n","                scene=annotated_frame,\n","                detections=tracked_detections\n","            )\n","\n","            # Draw labels manually using OpenCV\n","            if tracked_detections.tracker_id is not None:\n","                for i, (bbox, track_id) in enumerate(zip(tracked_detections.xyxy, tracked_detections.tracker_id)):\n","                    x1, y1, x2, y2 = map(int, bbox)\n","                    label = f\"ID:{track_id}\"\n","\n","                    # Draw label background\n","                    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n","                    cv2.rectangle(annotated_frame,\n","                                (x1, y1 - label_size[1] - 10),\n","                                (x1 + label_size[0], y1),\n","                                (0, 255, 0), -1)\n","\n","                    # Draw label text\n","                    cv2.putText(annotated_frame, label,\n","                              (x1, y1 - 5),\n","                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n","\n","        return annotated_frame, density_map, detection_count, unique_count, density_count\n","\n","    def create_heatmap_overlay(self, density_map, original_frame, alpha=0.5):\n","        \"\"\"Create density heatmap overlay\"\"\"\n","        h, w = original_frame.shape[:2]\n","        density_resized = cv2.resize(density_map, (w, h), interpolation=cv2.INTER_CUBIC)\n","\n","        density_max = density_resized.max()\n","        if density_max > 0:\n","            density_normalized = density_resized / density_max\n","        else:\n","            density_normalized = density_resized\n","\n","        heatmap = cm.jet(density_normalized)[:, :, :3]\n","        heatmap = (heatmap * 255).astype(np.uint8)\n","\n","        overlay = cv2.addWeighted(original_frame, 1-alpha, heatmap, alpha, 0)\n","\n","        return overlay, heatmap\n","\n","    def reset_tracker(self):\n","        \"\"\"Reset tracker for new video\"\"\"\n","        self.tracker.reset()"],"metadata":{"id":"-mp5sVY3FmPs","executionInfo":{"status":"ok","timestamp":1765611294997,"user_tz":-330,"elapsed":8,"user":{"displayName":"Ishu Rajput","userId":"09455974081349711484"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Cell 7: Video Processing with Save Functionality\n","def process_video_with_save(video_path, counter, alert_threshold=50, frame_skip=1,\n","                            output_path=None, save_video=True):\n","    \"\"\"\n","    Process video file with detection + tracking + density\n","    Save annotated output video\n","    \"\"\"\n","    cap = cv2.VideoCapture(video_path)\n","\n","    if not cap.isOpened():\n","        raise ValueError(f\"Cannot open video: {video_path}\")\n","\n","    # Reset tracker for new video\n","    counter.reset_tracker()\n","\n","    # Get video properties\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","    # Setup output video writer\n","    if save_video:\n","        if output_path is None:\n","            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","            output_path = f\"/content/output_annotated_{timestamp}.mp4\"\n","\n","        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n","        print(f\"üìπ Saving output to: {output_path}\")\n","\n","    frame_idx = 0\n","    processed_idx = 0\n","\n","    # Statistics\n","    detection_counts = []\n","    density_counts = []\n","    alert_frames = []\n","\n","    print(f\"Processing video: {total_frames} frames @ {fps:.1f} FPS\")\n","    print(f\"Frame skip: {frame_skip}\")\n","\n","    start_time = time.time()\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        frame_idx += 1\n","\n","        if frame_idx % frame_skip != 0:\n","            continue\n","\n","        # Process frame with detection + tracking + density\n","        annotated, density_map, det_count, unique_count, dens_count = counter.predict_with_tracking(frame)\n","\n","        # Create heatmap overlay\n","        overlay, heatmap = counter.create_heatmap_overlay(density_map, annotated, alpha=0.3)\n","\n","        # Add info text overlay\n","        info_y = 30\n","        cv2.rectangle(overlay, (10, 10), (400, 120), (0, 0, 0), -1)\n","        cv2.putText(overlay, f\"Frame: {frame_idx}/{total_frames}\", (20, info_y),\n","                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n","        cv2.putText(overlay, f\"Detected: {det_count} | Unique: {unique_count}\", (20, info_y + 30),\n","                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n","        cv2.putText(overlay, f\"Density Est: {int(dens_count)}\", (20, info_y + 60),\n","                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n","\n","        # Alert check\n","        is_alert = det_count > alert_threshold\n","        if is_alert:\n","            alert_frames.append(frame_idx)\n","            # Add alert banner\n","            cv2.rectangle(overlay, (0, height - 50), (width, height), (0, 0, 255), -1)\n","            cv2.putText(overlay, f\"ALERT! Count: {det_count} > Threshold: {alert_threshold}\",\n","                       (20, height - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n","\n","        # Write frame to output video\n","        if save_video:\n","            out.write(overlay)\n","\n","        # Update statistics\n","        detection_counts.append(det_count)\n","        density_counts.append(dens_count)\n","\n","        processed_idx += 1\n","\n","        # Progress update every 30 frames\n","        if processed_idx % 30 == 0:\n","            elapsed = time.time() - start_time\n","            fps_processing = processed_idx / elapsed\n","            eta = (total_frames // frame_skip - processed_idx) / fps_processing\n","            print(f\"Progress: {processed_idx}/{total_frames // frame_skip} frames | \"\n","                  f\"FPS: {fps_processing:.1f} | ETA: {eta:.1f}s\")\n","\n","    cap.release()\n","    if save_video:\n","        out.release()\n","\n","    # Final statistics\n","    avg_det = np.mean(detection_counts) if detection_counts else 0\n","    max_det = max(detection_counts) if detection_counts else 0\n","    avg_dens = np.mean(density_counts) if density_counts else 0\n","\n","    print(f\"\\n{'='*60}\")\n","    print(f\"‚úì Processing complete: {processed_idx} frames\")\n","    print(f\"  Total unique people: {unique_count}\")\n","    print(f\"  Avg detection: {avg_det:.1f}\")\n","    print(f\"  Max detection: {max_det}\")\n","    print(f\"  Avg density: {avg_dens:.1f}\")\n","    print(f\"  Alert frames: {len(alert_frames)}\")\n","    if save_video:\n","        print(f\"  Output saved: {output_path}\")\n","    print(f\"{'='*60}\\n\")\n","\n","    return {\n","        'output_path': output_path if save_video else None,\n","        'unique_count': unique_count,\n","        'avg_detection': avg_det,\n","        'max_detection': max_det,\n","        'avg_density': avg_dens,\n","        'alert_frames': alert_frames,\n","        'processed_frames': processed_idx\n","    }\n"],"metadata":{"id":"f13LUetHFrdV","executionInfo":{"status":"ok","timestamp":1765611314802,"user_tz":-330,"elapsed":111,"user":{"displayName":"Ishu Rajput","userId":"09455974081349711484"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Cell 8: Gradio Interface for Video Processing\n","def create_video_interface(model_path):\n","    \"\"\"Create Gradio interface for video processing\"\"\"\n","\n","    print(\"=\"*60)\n","    print(\"Loading models...\")\n","    print(\"=\"*60)\n","\n","    # Load CSRNet\n","    csrnet = load_trained_model(model_path)\n","\n","    # Create enhanced counter\n","    counter = EnhancedCrowdCounter(csrnet)\n","\n","    print(\"‚úì All models ready for inference\\n\")\n","\n","    # Video processing function for Gradio\n","    def process_video_gradio(video_file, threshold, skip_frames, save_output):\n","        if video_file is None:\n","            return None, \"‚ùå No video uploaded\", \"\"\n","\n","        try:\n","            # Generate output path\n","            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","            output_path = f\"/content/output_{timestamp}.mp4\" if save_output else None\n","\n","            # Process video\n","            stats = process_video_with_save(\n","                video_file,\n","                counter,\n","                alert_threshold=int(threshold),\n","                frame_skip=int(skip_frames),\n","                output_path=output_path,\n","                save_video=save_output\n","            )\n","\n","            # Create stats message\n","            stats_msg = f\"\"\"\n","‚úì Processing Complete!\n","\n","üìä Statistics:\n","- Processed Frames: {stats['processed_frames']}\n","- Total Unique People: {stats['unique_count']}\n","- Average Detection: {stats['avg_detection']:.1f}\n","- Max Detection: {stats['max_detection']}\n","- Average Density: {stats['avg_density']:.1f}\n","- Alert Frames: {len(stats['alert_frames'])}\n","\"\"\"\n","\n","            if save_output and stats['output_path']:\n","                stats_msg += f\"\\nüíæ Output Video: {stats['output_path']}\"\n","                # Also save to Google Drive for easy access\n","                drive_output = f\"/content/drive/MyDrive/Testing/outputs/Processed video/{os.path.basename(stats['output_path'])}\"\n","                os.makedirs(os.path.dirname(drive_output), exist_ok=True)\n","                import shutil\n","                shutil.copy(stats['output_path'], drive_output)\n","                stats_msg += f\"\\nüíæ Also saved to Drive: {drive_output}\"\n","                print(f\"‚úì Video saved to: {stats['output_path']}\")\n","                print(f\"‚úì Also copied to Drive: {drive_output}\")\n","                return stats['output_path'], \"‚úì Video processed successfully!\", stats_msg\n","            else:\n","                return None, \"‚úì Video processed successfully!\", stats_msg\n","\n","        except Exception as e:\n","            import traceback\n","            traceback.print_exc()\n","            return None, f\"‚ùå Error: {str(e)}\", \"\"\n","\n","    # Build Gradio UI\n","    with gr.Blocks(title=\"CSRNet Video Processor\") as demo:\n","\n","        gr.Markdown(\"\"\"\n","        # üé• CSRNet Crowd Counter - Video Processor\n","\n","        **Process MP4/MOV videos with:**\n","        - üéØ YOLOv8 Person Detection & Tracking\n","        - üó∫Ô∏è CSRNet Density Heatmaps\n","        - üö® Threshold-based Alerts\n","        - üíæ Save Annotated Output Video\n","        \"\"\")\n","\n","        with gr.Row():\n","            with gr.Column(scale=1):\n","                gr.Markdown(\"### üì§ Input\")\n","\n","                video_input = gr.Video(\n","                    label=\"Upload Video (MP4/MOV)\",\n","                    sources=[\"upload\"]\n","                )\n","\n","                gr.Markdown(\"### ‚öôÔ∏è Settings\")\n","\n","                threshold_slider = gr.Slider(\n","                    minimum=5,\n","                    maximum=100,\n","                    value=30,\n","                    step=5,\n","                    label=\"Alert Threshold\",\n","                    info=\"Trigger alert when detected count exceeds this\"\n","                )\n","\n","                skip_frames_slider = gr.Slider(\n","                    minimum=1,\n","                    maximum=10,\n","                    value=2,\n","                    step=1,\n","                    label=\"Frame Skip\",\n","                    info=\"Process every Nth frame (higher = faster)\"\n","                )\n","\n","                save_checkbox = gr.Checkbox(\n","                    label=\"Save Output Video\",\n","                    value=True,\n","                    info=\"Save annotated video to /content/\"\n","                )\n","\n","                process_btn = gr.Button(\n","                    \"üöÄ Process Video\",\n","                    variant=\"primary\",\n","                    size=\"lg\"\n","                )\n","\n","            with gr.Column(scale=2):\n","                gr.Markdown(\"### üìä Results\")\n","\n","                output_video = gr.Video(\n","                    label=\"Processed Video Output\"\n","                )\n","\n","                status_text = gr.Textbox(\n","                    label=\"üîî Status\",\n","                    interactive=False\n","                )\n","\n","                stats_text = gr.Textbox(\n","                    label=\"üìà Statistics\",\n","                    interactive=False,\n","                    lines=10\n","                )\n","\n","        process_btn.click(\n","            fn=process_video_gradio,\n","            inputs=[video_input, threshold_slider, skip_frames_slider, save_checkbox],\n","            outputs=[output_video, status_text, stats_text]\n","        )\n","\n","        gr.Markdown(\"\"\"\n","        ---\n","        ## üìñ How It Works\n","\n","        ### üéØ Detection & Tracking\n","        - **YOLOv8** detects people and draws bounding boxes\n","        - **ByteTrack** assigns unique IDs to prevent re-counting\n","        - Green boxes and tracking lines show movement paths\n","\n","        ### üó∫Ô∏è Density Heatmap\n","        - **CSRNet** provides density estimation overlay\n","        - Blue = low density, Red = high density\n","        - Trained on 512x512 images with GT_DOWNSAMPLE=8\n","\n","        ### üö® Alerts\n","        - Red banner appears when count > threshold\n","        - Alert frames are logged for review\n","\n","        ### üí° Tips\n","        - Frame skip 2-3 for good speed/accuracy balance\n","        - Lower threshold for crowded areas (10-20)\n","        - Higher threshold for sparse areas (30-50)\n","        - Output videos saved to /content/ folder\n","        \"\"\")\n","\n","    return demo"],"metadata":{"id":"SpfMtiaGF4aw","executionInfo":{"status":"ok","timestamp":1765611412708,"user_tz":-330,"elapsed":19,"user":{"displayName":"Ishu Rajput","userId":"09455974081349711484"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Cell 9: Launch Interface\n","if __name__ == \"__main__\":\n","    # ‚öôÔ∏è CONFIGURE YOUR MODEL PATH HERE\n","    MODEL_PATH = \"/content/drive/MyDrive/Testing/outputs/best_crowd_counter_unified.pth\"\n","\n","    if not Path(MODEL_PATH).exists():\n","        print(\"=\"*60)\n","        print(\"‚ùå MODEL NOT FOUND\")\n","        print(\"=\"*60)\n","        print(f\"Expected path: {MODEL_PATH}\")\n","        print(\"\\nPlease update MODEL_PATH to point to your trained model.\")\n","        print(\"=\"*60)\n","    else:\n","        print(\"=\"*60)\n","        print(\"üöÄ LAUNCHING VIDEO PROCESSOR\")\n","        print(\"=\"*60)\n","\n","        demo = create_video_interface(MODEL_PATH)\n","\n","        demo.launch(\n","            share=True,\n","            debug=True,\n","            server_name=\"0.0.0.0\",\n","            server_port=7860\n","        )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"6FD6jhTOLL11","executionInfo":{"status":"ok","timestamp":1765612190433,"user_tz":-330,"elapsed":772192,"user":{"displayName":"Ishu Rajput","userId":"09455974081349711484"}},"outputId":"35f303fa-3883-481e-cc50-14872d34c6af"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","üöÄ LAUNCHING VIDEO PROCESSOR\n","============================================================\n","============================================================\n","Loading models...\n","============================================================\n","Loading CSRNet from: /content/drive/MyDrive/Testing/outputs/best_crowd_counter_unified.pth\n","Building CSRNet architecture...\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 528M/528M [00:03<00:00, 143MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["‚úì CSRNet architecture created\n","‚úì Loaded checkpoint from epoch 191\n","‚úì CSRNet ready on cuda\n","Loading YOLOv8 person detector...\n","\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.2MB 323.1MB/s 0.0s\n","‚úì YOLOv8 loaded\n","‚úì All models ready for inference\n","\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://73a823411ceb1940bb.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://73a823411ceb1940bb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["üìπ Saving output to: /content/output_20251213_073820.mp4\n","Processing video: 170 frames @ 30.0 FPS\n","Frame skip: 1\n","Progress: 30/170 frames | FPS: 4.2 | ETA: 33.7s\n","Progress: 60/170 frames | FPS: 4.4 | ETA: 25.0s\n","Progress: 90/170 frames | FPS: 4.7 | ETA: 17.0s\n","Progress: 120/170 frames | FPS: 4.7 | ETA: 10.6s\n","Progress: 150/170 frames | FPS: 4.8 | ETA: 4.2s\n","\n","============================================================\n","‚úì Processing complete: 170 frames\n","  Total unique people: 79\n","  Avg detection: 10.0\n","  Max detection: 15\n","  Avg density: 428.0\n","  Alert frames: 0\n","  Output saved: /content/output_20251213_073820.mp4\n","============================================================\n","\n","‚úì Video saved to: /content/output_20251213_073820.mp4\n","‚úì Also copied to Drive: /content/drive/MyDrive/Testing/outputs/Processed video/output_20251213_073820.mp4\n"]},{"output_type":"stream","name":"stderr","text":["UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n"]},{"output_type":"stream","name":"stdout","text":["üìπ Saving output to: /content/output_20251213_074247.mp4\n","Processing video: 403 frames @ 30.0 FPS\n","Frame skip: 1\n","Progress: 30/403 frames | FPS: 4.9 | ETA: 76.6s\n","Progress: 60/403 frames | FPS: 5.4 | ETA: 63.8s\n","Progress: 90/403 frames | FPS: 5.6 | ETA: 55.4s\n","Progress: 120/403 frames | FPS: 5.5 | ETA: 51.5s\n","Progress: 150/403 frames | FPS: 5.6 | ETA: 45.1s\n","Progress: 180/403 frames | FPS: 5.6 | ETA: 39.9s\n","Progress: 210/403 frames | FPS: 5.6 | ETA: 34.5s\n","Progress: 240/403 frames | FPS: 5.7 | ETA: 28.8s\n","Progress: 270/403 frames | FPS: 5.6 | ETA: 23.8s\n","Progress: 300/403 frames | FPS: 5.6 | ETA: 18.3s\n","Progress: 330/403 frames | FPS: 5.6 | ETA: 13.0s\n","Progress: 360/403 frames | FPS: 5.6 | ETA: 7.7s\n","Progress: 390/403 frames | FPS: 5.7 | ETA: 2.3s\n","\n","============================================================\n","‚úì Processing complete: 403 frames\n","  Total unique people: 2\n","  Avg detection: 0.0\n","  Max detection: 1\n","  Avg density: 433.7\n","  Alert frames: 0\n","  Output saved: /content/output_20251213_074247.mp4\n","============================================================\n","\n","‚úì Video saved to: /content/output_20251213_074247.mp4\n","‚úì Also copied to Drive: /content/drive/MyDrive/Testing/outputs/Processed video/output_20251213_074247.mp4\n"]},{"output_type":"stream","name":"stderr","text":["UserWarning: Video does not have browser-compatible container or codec. Converting to mp4.\n"]},{"output_type":"stream","name":"stdout","text":["Keyboard interruption in main thread... closing server.\n","Killing tunnel 0.0.0.0:7860 <> https://73a823411ceb1940bb.gradio.live\n"]}]},{"cell_type":"code","source":["# live webcam with gradio interface...\n","\n","# ============================================================================\n","# SECTION 1: INSTALL DEPENDENCIES\n","# ============================================================================\n","\n","!pip install -q torch torchvision opencv-python matplotlib ultralytics supervision gradio\n","\n","# ============================================================================\n","# SECTION 2: IMPORTS\n","# ============================================================================\n","\n","import cv2\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torchvision import models\n","from matplotlib import cm\n","from collections import defaultdict\n","from ultralytics import YOLO\n","import supervision as sv\n","import gradio as gr\n","from PIL import Image\n","import time\n","\n","# ============================================================================\n","# SECTION 3: CONFIGURATION\n","# ============================================================================\n","\n","# Check GPU availability\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"üöÄ Using device: {DEVICE}\")\n","if DEVICE.type == 'cuda':\n","    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"‚úÖ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","\n","# Configuration\n","ALERT_THRESHOLD = 30\n","IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","IMAGENET_STD = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n","\n","# ============================================================================\n","# SECTION 4: MODEL ARCHITECTURE\n","# ============================================================================\n","\n","def create_csrnet():\n","    \"\"\"Create CSRNet model architecture\"\"\"\n","    print(\"Building CSRNet architecture...\")\n","\n","    vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n","    features = list(vgg.features.children())\n","\n","    frontend = nn.Sequential(*features[0:23])\n","\n","    backend = nn.Sequential(\n","        nn.Conv2d(512, 512, kernel_size=3, padding=1, dilation=1),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(512, 512, kernel_size=3, padding=4, dilation=4),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(512, 256, kernel_size=3, padding=1, dilation=1),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(256, 128, kernel_size=3, padding=1, dilation=1),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(128, 1, kernel_size=1, padding=0),\n","    )\n","\n","    model = nn.Sequential(frontend, backend)\n","    print(\"‚úÖ CSRNet architecture created\")\n","\n","    return model\n","\n","def load_trained_model(model_path, device=DEVICE):\n","    \"\"\"Load trained CSRNet model\"\"\"\n","    print(f\"Loading CSRNet from: {model_path}\")\n","\n","    model = create_csrnet()\n","    checkpoint = torch.load(model_path, map_location=device)\n","\n","    if 'model_state' in checkpoint:\n","        model.load_state_dict(checkpoint['model_state'])\n","        print(f\"‚úÖ Loaded checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n","    else:\n","        model.load_state_dict(checkpoint)\n","        print(\"‚úÖ Loaded model weights\")\n","\n","    model.to(device)\n","    model.eval()\n","\n","    print(f\"‚úÖ CSRNet ready on {device}\")\n","    return model\n","\n","# ============================================================================\n","# SECTION 5: PERSON TRACKER\n","# ============================================================================\n","\n","class PersonTracker:\n","    \"\"\"Multi-object tracker to prevent re-counting\"\"\"\n","\n","    def __init__(self):\n","        self.tracker = sv.ByteTrack(\n","            track_activation_threshold=0.25,\n","            lost_track_buffer=30,\n","            minimum_matching_threshold=0.8,\n","            frame_rate=30\n","        )\n","\n","        self.unique_ids = set()\n","        self.track_history = defaultdict(list)\n","\n","    def update(self, detections):\n","        \"\"\"Update tracker with new detections\"\"\"\n","        tracked_detections = self.tracker.update_with_detections(detections)\n","\n","        if tracked_detections.tracker_id is not None:\n","            for track_id in tracked_detections.tracker_id:\n","                self.unique_ids.add(int(track_id))\n","\n","        return tracked_detections\n","\n","    def get_total_unique_count(self):\n","        \"\"\"Get total unique people seen\"\"\"\n","        return len(self.unique_ids)\n","\n","    def reset(self):\n","        \"\"\"Reset tracker\"\"\"\n","        self.tracker = sv.ByteTrack(\n","            track_activation_threshold=0.25,\n","            lost_track_buffer=30,\n","            minimum_matching_threshold=0.8,\n","            frame_rate=30\n","        )\n","        self.unique_ids = set()\n","        self.track_history.clear()\n","\n","# ============================================================================\n","# SECTION 6: ENHANCED CROWD COUNTER\n","# ============================================================================\n","\n","class EnhancedCrowdCounter:\n","    \"\"\"Enhanced counter with detection + tracking + density\"\"\"\n","\n","    def __init__(self, csrnet_model, device=DEVICE, csrnet_input_size=(512, 512)):\n","        self.csrnet = csrnet_model\n","        self.device = device\n","        self.mean = IMAGENET_MEAN\n","        self.std = IMAGENET_STD\n","\n","        self.csrnet_input_size = csrnet_input_size\n","        print(f\"‚úÖ CSRNet will process images at: {csrnet_input_size[0]}x{csrnet_input_size[1]}\")\n","\n","        # Load YOLOv8\n","        print(\"Loading YOLOv8 person detector...\")\n","        self.yolo = YOLO('yolov8n.pt')\n","        print(\"‚úÖ YOLOv8 loaded\")\n","\n","        # Initialize tracker\n","        self.tracker = PersonTracker()\n","\n","        # Annotators\n","        self.box_annotator = sv.BoxAnnotator(thickness=2)\n","        self.trace_annotator = sv.TraceAnnotator(thickness=2, trace_length=50)\n","\n","    def preprocess_frame(self, frame):\n","        \"\"\"Preprocess frame for CSRNet\"\"\"\n","        frame_resized = cv2.resize(frame,\n","                                   (self.csrnet_input_size[1], self.csrnet_input_size[0]),\n","                                   interpolation=cv2.INTER_LINEAR)\n","\n","        img_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n","        img_normalized = img_rgb.astype(np.float32) / 255.0\n","        img_normalized = (img_normalized - self.mean) / self.std\n","\n","        img_tensor = torch.from_numpy(img_normalized).permute(2, 0, 1).unsqueeze(0)\n","        img_tensor = img_tensor.to(self.device, dtype=torch.float32)\n","\n","        return img_tensor\n","\n","    def predict_density(self, frame):\n","        \"\"\"Get density map from CSRNet\"\"\"\n","        with torch.no_grad():\n","            img_tensor = self.preprocess_frame(frame)\n","            density_map = self.csrnet(img_tensor)\n","            density_np = density_map.squeeze().cpu().numpy()\n","            count = float(density_np.sum())\n","        return density_np, count\n","\n","    def detect_people(self, frame):\n","        \"\"\"Detect people using YOLOv8\"\"\"\n","        results = self.yolo(frame, classes=[0], verbose=False)[0]\n","        detections = sv.Detections.from_ultralytics(results)\n","        return detections\n","\n","    def predict_with_tracking(self, frame):\n","        \"\"\"Complete prediction pipeline\"\"\"\n","        detections = self.detect_people(frame)\n","        tracked_detections = self.tracker.update(detections)\n","\n","        detection_count = len(tracked_detections)\n","        unique_count = self.tracker.get_total_unique_count()\n","\n","        density_map, density_count = self.predict_density(frame)\n","\n","        annotated_frame = frame.copy()\n","\n","        if len(tracked_detections) > 0:\n","            annotated_frame = self.box_annotator.annotate(\n","                scene=annotated_frame,\n","                detections=tracked_detections\n","            )\n","\n","            annotated_frame = self.trace_annotator.annotate(\n","                scene=annotated_frame,\n","                detections=tracked_detections\n","            )\n","\n","            if tracked_detections.tracker_id is not None:\n","                for bbox, track_id in zip(tracked_detections.xyxy, tracked_detections.tracker_id):\n","                    x1, y1, x2, y2 = map(int, bbox)\n","                    label = f\"ID:{track_id}\"\n","\n","                    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n","                    cv2.rectangle(annotated_frame,\n","                                (x1, y1 - label_size[1] - 10),\n","                                (x1 + label_size[0] + 10, y1),\n","                                (0, 255, 0), -1)\n","\n","                    cv2.putText(annotated_frame, label,\n","                              (x1 + 5, y1 - 5),\n","                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n","\n","        return annotated_frame, density_map, detection_count, unique_count, density_count\n","\n","    def create_heatmap_overlay(self, density_map, original_frame, alpha=0.4):\n","        \"\"\"Create density heatmap overlay\"\"\"\n","        h, w = original_frame.shape[:2]\n","        density_resized = cv2.resize(density_map, (w, h), interpolation=cv2.INTER_CUBIC)\n","\n","        density_max = density_resized.max()\n","        if density_max > 0:\n","            density_normalized = density_resized / density_max\n","        else:\n","            density_normalized = density_resized\n","\n","        heatmap = cm.jet(density_normalized)[:, :, :3]\n","        heatmap = (heatmap * 255).astype(np.uint8)\n","\n","        overlay = cv2.addWeighted(original_frame, 1-alpha, heatmap, alpha, 0)\n","\n","        return overlay\n","\n","# ============================================================================\n","# SECTION 7: GRADIO INTERFACE\n","# ============================================================================\n","\n","def process_webcam_frame(frame, show_heatmap):\n","    \"\"\"Process single webcam frame in real-time\"\"\"\n","    if frame is None:\n","        return None\n","\n","    # Convert to BGR for OpenCV processing\n","    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n","\n","    start_time = time.time()\n","\n","    # Process frame\n","    annotated, density_map, det_count, unique_count, dens_count = counter.predict_with_tracking(frame_bgr)\n","\n","    # Apply heatmap if requested\n","    if show_heatmap:\n","        output = counter.create_heatmap_overlay(density_map, annotated, alpha=0.4)\n","    else:\n","        output = annotated\n","\n","    # Calculate FPS\n","    fps = 1.0 / (time.time() - start_time)\n","\n","    # Add overlay info panel\n","    h, w = output.shape[:2]\n","\n","    # Semi-transparent background for info\n","    overlay = output.copy()\n","    cv2.rectangle(overlay, (0, 0), (w, 120), (0, 0, 0), -1)\n","    cv2.addWeighted(overlay, 0.6, output, 0.4, 0, output)\n","\n","    # Alert status\n","    is_alert = det_count > ALERT_THRESHOLD\n","    if is_alert:\n","        alert_text = f\"ALERT! Count: {det_count} > Threshold: {ALERT_THRESHOLD}\"\n","        color = (0, 0, 255)  # Red\n","    else:\n","        alert_text = f\"Normal - Count: {det_count}\"\n","        color = (0, 255, 0)  # Green\n","\n","    # Draw text\n","    cv2.putText(output, \"CSRNet Live Feed\", (10, 25),\n","                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n","    cv2.putText(output, alert_text, (10, 55),\n","                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n","    cv2.putText(output, f\"Detected: {det_count} | Unique: {unique_count} | Density: {int(dens_count)} | FPS: {fps:.1f}\",\n","                (10, 85), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n","\n","    # Convert back to RGB\n","    output_rgb = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n","\n","    return output_rgb\n","\n","def reset_tracker():\n","    \"\"\"Reset the tracker\"\"\"\n","    counter.tracker.reset()\n","    return \"‚úÖ Tracker reset successfully!\"\n","\n","# ============================================================================\n","# SECTION 8: MOUNT GOOGLE DRIVE & LOAD MODEL\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"üìÇ MOUNTING GOOGLE DRIVE\")\n","print(\"=\"*60)\n","print(\"\\nTo use this notebook:\")\n","print(\"1. Mount your Google Drive (authorize when prompted)\")\n","print(\"2. Update MODEL_PATH with your model location\")\n","print(\"3. Use the Gradio interface to test images/videos\")\n","print(\"\\nüí° TIP: Enable GPU in Runtime > Change runtime type > T4 GPU\")\n","print(\"=\"*60)\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","print(\"\\n‚úÖ Google Drive mounted successfully!\")\n","\n","# ============================================================================\n","# üîß CONFIGURE YOUR MODEL PATH HERE\n","# ============================================================================\n","# Update this path to match your model location in Google Drive\n","# Example paths:\n","# - '/content/drive/MyDrive/models/best_crowd_counter_unified.pth'\n","# - '/content/drive/MyDrive/DeepVision/best_crowd_counter_unified.pth'\n","# - '/content/drive/MyDrive/best_crowd_counter_unified.pth'\n","\n","MODEL_PATH = '/content/drive/MyDrive/Testing/outputs/best_crowd_counter_unified.pth'  # üëà UPDATE THIS\n","\n","print(f\"\\nüìç Model Path: {MODEL_PATH}\")\n","\n","# Check if model exists\n","import os\n","if os.path.exists(MODEL_PATH):\n","    print(f\"‚úÖ Model found! Size: {os.path.getsize(MODEL_PATH) / (1024*1024):.2f} MB\")\n","else:\n","    print(f\"‚ùå Model not found at: {MODEL_PATH}\")\n","    print(\"\\nüí° TIP: Check your Google Drive and update MODEL_PATH above\")\n","    print(\"   Right-click on your model file in Drive > Get link to see the path\")\n","\n","# ============================================================================\n","# SECTION 9: INITIALIZE SYSTEM\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"üöÄ INITIALIZING SYSTEM\")\n","print(\"=\"*60)\n","\n","# Load model\n","csrnet = load_trained_model(MODEL_PATH)\n","\n","# Create counter\n","counter = EnhancedCrowdCounter(csrnet, csrnet_input_size=(512, 512))\n","\n","print(\"\\n‚úÖ System ready!\")\n","\n","# ============================================================================\n","# SECTION 10: LAUNCH GRADIO INTERFACE\n","# ============================================================================\n","\n","# Create Gradio interface\n","with gr.Blocks(title=\"CSRNet Crowd Counter - Live CCTV\") as demo:\n","    gr.Markdown(\"\"\"\n","    # üéØ CSRNet Crowd Counter - Live Camera Feed\n","    ### Real-time crowd counting and tracking with density estimation\n","    **üöÄ GPU-Accelerated Live Detection**\n","    \"\"\")\n","\n","    with gr.Tabs():\n","        # Live Camera Tab\n","        with gr.Tab(\"üìπ Live Camera / CCTV Feed\"):\n","            gr.Markdown(\"\"\"\n","            ### üé• Real-Time Webcam Detection\n","            - Enable your webcam below\n","            - Toggle heatmap overlay\n","            - View live crowd counts with tracking\n","            - Auto-resets tracker on stream start\n","            \"\"\")\n","\n","            with gr.Row():\n","                with gr.Column():\n","                    webcam_input = gr.Image(sources=[\"webcam\"], streaming=True, label=\"Live Camera Feed\")\n","                    webcam_heatmap = gr.Checkbox(label=\"Show Density Heatmap\", value=True)\n","\n","                    with gr.Row():\n","                        reset_btn = gr.Button(\"üîÑ Reset Tracker\", variant=\"secondary\", size=\"sm\")\n","                        reset_status = gr.Textbox(label=\"Status\", scale=2, show_label=False)\n","\n","                with gr.Column():\n","                    webcam_output = gr.Image(label=\"Processed Live Feed\", streaming=True)\n","\n","                    gr.Markdown(\"\"\"\n","                    ### üìä Live Statistics\n","                    - **Green** = Normal count (below threshold)\n","                    - **Red** = Alert! (exceeds threshold)\n","                    - **Detected**: Current frame detections\n","                    - **Unique**: Total unique people tracked in session\n","                    - **Density**: CSRNet density map estimation\n","                    - **FPS**: Processing speed\n","                    \"\"\")\n","\n","            # Real-time processing\n","            webcam_input.stream(\n","                fn=process_webcam_frame,\n","                inputs=[webcam_input, webcam_heatmap],\n","                outputs=webcam_output,\n","                show_progress=\"hidden\"\n","            )\n","\n","            reset_btn.click(fn=reset_tracker, outputs=reset_status)\n","\n","        # Settings Tab\n","        with gr.Tab(\"‚öôÔ∏è Settings & Info\"):\n","            gr.Markdown(\"\"\"\n","            ### Current Configuration\n","            \"\"\")\n","\n","            with gr.Row():\n","                with gr.Column():\n","                    gr.Markdown(f\"\"\"\n","                    **System Info:**\n","                    - **Device:** {DEVICE}\n","                    - **GPU:** {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\n","                    - **Alert Threshold:** {ALERT_THRESHOLD} people\n","                    - **CSRNet Input Size:** 512x512\n","                    - **YOLOv8 Model:** yolov8n.pt\n","                    \"\"\")\n","\n","                with gr.Column():\n","                    gr.Markdown(\"\"\"\n","                    **Performance Tips:**\n","                    - Enable GPU for best performance (Runtime > GPU)\n","                    - Disable heatmap for faster processing\n","                    - Use good lighting for better detection\n","                    - Position camera for clear view of people\n","                    - Reset tracker when starting new monitoring session\n","                    \"\"\")\n","\n","            gr.Markdown(\"\"\"\n","            ---\n","            ### üéØ Features:\n","            - ‚úÖ Real-time person detection & tracking\n","            - ‚úÖ Density heatmap visualization\n","            - ‚úÖ Multi-object tracking (prevents re-counting)\n","            - ‚úÖ Alert system for crowd threshold\n","            - ‚úÖ FPS monitoring\n","            - ‚úÖ Session-based unique count tracking\n","\n","            ### üí° Use Cases:\n","            - CCTV monitoring\n","            - Event crowd management\n","            - Retail foot traffic analysis\n","            - Public space monitoring\n","            - Queue management\n","            \"\"\")\n","\n","# Launch the interface\n","print(\"\\n\" + \"=\"*60)\n","print(\"üöÄ LAUNCHING GRADIO INTERFACE\")\n","print(\"=\"*60)\n","print(\"\\nüí° TIPS:\")\n","print(\"  - Click 'Allow' when browser asks for webcam permission\")\n","print(\"  - Position camera with good lighting\")\n","print(\"  - Toggle heatmap on/off as needed\")\n","print(\"  - Reset tracker when starting new sessions\")\n","print(\"\\nüåê Interface will open in a new window...\")\n","print(\"=\"*60)\n","\n","demo.launch(share=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"K9pz0beQsBjJ","executionInfo":{"status":"ok","timestamp":1765613043555,"user_tz":-330,"elapsed":14877,"user":{"displayName":"Ishu Rajput","userId":"09455974081349711484"}},"outputId":"9f1a763f-6c41-4dd9-e1f9-52c95ce570b2"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ Using device: cuda\n","‚úÖ GPU: Tesla T4\n","‚úÖ Memory: 15.83 GB\n","\n","============================================================\n","üìÇ MOUNTING GOOGLE DRIVE\n","============================================================\n","\n","To use this notebook:\n","1. Mount your Google Drive (authorize when prompted)\n","2. Update MODEL_PATH with your model location\n","3. Use the Gradio interface to test images/videos\n","\n","üí° TIP: Enable GPU in Runtime > Change runtime type > T4 GPU\n","============================================================\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n","‚úÖ Google Drive mounted successfully!\n","\n","üìç Model Path: /content/drive/MyDrive/Testing/outputs/best_crowd_counter_unified.pth\n","‚úÖ Model found! Size: 185.33 MB\n","\n","============================================================\n","üöÄ INITIALIZING SYSTEM\n","============================================================\n","Loading CSRNet from: /content/drive/MyDrive/Testing/outputs/best_crowd_counter_unified.pth\n","Building CSRNet architecture...\n","‚úÖ CSRNet architecture created\n","‚úÖ Loaded checkpoint from epoch 191\n","‚úÖ CSRNet ready on cuda\n","‚úÖ CSRNet will process images at: 512x512\n","Loading YOLOv8 person detector...\n","‚úÖ YOLOv8 loaded\n","\n","‚úÖ System ready!\n","\n","============================================================\n","üöÄ LAUNCHING GRADIO INTERFACE\n","============================================================\n","\n","üí° TIPS:\n","  - Click 'Allow' when browser asks for webcam permission\n","  - Position camera with good lighting\n","  - Toggle heatmap on/off as needed\n","  - Reset tracker when starting new sessions\n","\n","üåê Interface will open in a new window...\n","============================================================\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://92be8f8ed38887e84c.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://92be8f8ed38887e84c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":17}]}]}