{"cells":[{"cell_type":"markdown","id":"9ca19179","metadata":{"id":"9ca19179"},"source":["train_csrnet_colab.py\n","Colab instructions:\n","1) Upload this file to Colab or paste into a notebook cell.\n","2) Mount Google Drive if you want to read/write there:\n","   from google.colab import drive\n","   drive.mount('/content/drive')\n","3) Adjust `DATA_ROOT` and `CHECKPOINT_DIR`."]},{"cell_type":"code","source":["# --- Mount Google Drive ---\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# --- Check if GPU is available ---\n","import torch\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", DEVICE)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZAeCqdqPVtZ","executionInfo":{"status":"ok","timestamp":1764437587969,"user_tz":-330,"elapsed":27372,"user":{"displayName":"Mahalakshmi","userId":"17771931168946639535"}},"outputId":"36deb6ea-3690-42b7-f306-1fca57622c0a"},"id":"SZAeCqdqPVtZ","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Device: cuda\n"]}]},{"cell_type":"code","execution_count":null,"id":"d934ba87","metadata":{"id":"d934ba87"},"outputs":[],"source":["# --- Imports ---\n","import os, glob, time\n","from pathlib import Path\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import torchvision.models as models\n","\n","# --- Config paths (EDIT HERE) ---\n","DATA_ROOT = \"/content/drive/MyDrive/Deepvision_Project\"\n","\n","IMG_GLOB = os.path.join(DATA_ROOT, \"torch_images_trainA\", \"*.pt\")\n","GT_GLOB  = os.path.join(DATA_ROOT, \"torch_density_trainA\", \"*.pt\")\n","\n","CHECKPOINT_DIR = \"/content/drive/MyDrive/Deepvision_Project/checkpoints\"\n","os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n","\n","# --- Hyperparameters ---\n","BATCH_SIZE = 8\n","NUM_EPOCHS = 100\n","LR = 1e-5\n","NUM_WORKERS = 2 if torch.cuda.is_available() else 0\n","SAVE_EVERY = 5\n","PRINT_FREQ = 10\n"]},{"cell_type":"code","execution_count":null,"id":"c8c5e005","metadata":{"id":"c8c5e005"},"outputs":[],"source":["# --- Dataset class that matches filename stems ---\n","class PtDataset(Dataset):\n","    def __init__(self, img_glob, gt_glob, transform=None):\n","        img_paths = sorted(glob.glob(img_glob))\n","        gt_paths  = sorted(glob.glob(gt_glob))\n","\n","        img_map = {Path(p).stem: p for p in img_paths}\n","        gt_map  = {Path(p).stem: p for p in gt_paths}\n","\n","        common = sorted(set(img_map.keys()) & set(gt_map.keys()))\n","\n","        if len(common) == 0:\n","            raise ValueError(\n","                f\"No matching .pt pairs found.\\n\"\n","                f\"Images: {len(img_paths)}, GT: {len(gt_paths)}.\\n\"\n","                f\"Check filenames and DATA_ROOT path.\"\n","            )\n","\n","        self.pairs = [(img_map[s], gt_map[s]) for s in common]\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, idx):\n","        img_path, gt_path = self.pairs[idx]\n","        img = torch.load(img_path).float()\n","        gt  = torch.load(gt_path).float()\n","\n","        if gt.ndim == 2:\n","            gt = gt.unsqueeze(0)\n","\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        return img, gt\n"]},{"cell_type":"code","execution_count":null,"id":"e7d0a327","metadata":{"id":"e7d0a327"},"outputs":[],"source":["# --- CSRNet model (VGG frontend + Dilated backend) ---\n","class CSRNet(nn.Module):\n","    def __init__(self, load_weights=True):\n","        super(CSRNet, self).__init__()\n","\n","        vgg = models.vgg16_bn(pretrained=load_weights)\n","        features = list(vgg.features.children())\n","\n","        # frontend (same as CSRNet paper)\n","        self.frontend = nn.Sequential(*features[:34])\n","\n","        # backend: dilated convolutions\n","        self.backend = nn.Sequential(\n","            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 256, 3, padding=2, dilation=2),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 128, 3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 64, 3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 1, 1)\n","        )\n","\n","        self._initialize_weights()\n","\n","    def forward(self, x):\n","        x = self.frontend(x)\n","        x = self.backend(x)\n","        return x\n","\n","    def _initialize_weights(self):\n","        for m in self.backend.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.normal_(m.weight, std=0.01)\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n"]},{"cell_type":"code","execution_count":null,"id":"b69f421e","metadata":{"id":"b69f421e"},"outputs":[],"source":["# --- Train one epoch (resize GT to match pred size with count preservation) ---\n","def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):\n","    model.train()\n","    running_loss = 0.0\n","    t0 = time.time()\n","\n","    for i, (img, gt) in enumerate(dataloader, 1):\n","        img = img.to(device)\n","        gt  = gt.to(device)\n","\n","        pred = model(img)  # (B,1,Hp,Wp)\n","\n","        # GT resize block\n","        if pred.shape != gt.shape:\n","            _, _, Hp, Wp = pred.shape\n","            _, _, Hg, Wg = gt.shape\n","\n","            if Hg % Hp == 0 and Wg % Wp == 0:\n","                scale = Hg // Hp\n","                gt_resized = F.interpolate(gt, size=(Hp, Wp), mode=\"area\") * (scale * scale)\n","            else:\n","                gt_resized = F.interpolate(gt, size=(Hp, Wp), mode=\"bilinear\", align_corners=False)\n","\n","            loss = criterion(pred, gt_resized)\n","        else:\n","            loss = criterion(pred, gt)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        if i % PRINT_FREQ == 0:\n","            print(f\"[Epoch {epoch}] Batch {i}/{len(dataloader)}  Loss: {running_loss/i:.6f}\")\n","\n","    epoch_loss = running_loss / len(dataloader)\n","    print(f\"Epoch {epoch} completed. Loss: {epoch_loss:.6f}\")\n","    return epoch_loss\n","\n","\n","# --- Validation function ---\n","def validate(model, dataloader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","\n","    with torch.no_grad():\n","        for img, gt in dataloader:\n","            img = img.to(device)\n","            gt  = gt.to(device)\n","\n","            pred = model(img)\n","\n","            if pred.shape != gt.shape:\n","                _, _, Hp, Wp = pred.shape\n","                _, _, Hg, Wg = gt.shape\n","                if Hg % Hp == 0:\n","                    scale = Hg // Hp\n","                    gt_resized = F.interpolate(gt, size=(Hp, Wp), mode=\"area\") * (scale * scale)\n","                else:\n","                    gt_resized = F.interpolate(gt, size=(Hp, Wp), mode=\"bilinear\", align_corners=False)\n","\n","                loss = criterion(pred, gt_resized)\n","            else:\n","                loss = criterion(pred, gt)\n","\n","            running_loss += loss.item()\n","\n","    return running_loss / len(dataloader)\n"]},{"cell_type":"code","execution_count":null,"id":"96ebf364","metadata":{"lines_to_next_cell":1,"colab":{"base_uri":"https://localhost:8080/"},"id":"96ebf364","executionInfo":{"status":"ok","timestamp":1764420733334,"user_tz":-330,"elapsed":1920374,"user":{"displayName":"Mahalakshmi","userId":"17771931168946639535"}},"outputId":"fd3166a1-97bf-4c5c-d79f-e394835fcdc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total matched samples: 300\n","[Epoch 1] Batch 10/34  Loss: 1.430713\n","[Epoch 1] Batch 20/34  Loss: 2.087058\n","[Epoch 1] Batch 30/34  Loss: 1.894538\n","Epoch 1 completed. Loss: 1.876620\n","Epoch 1: Train=1.876620  Val=1.826885\n","Saved BEST model.\n","[Epoch 2] Batch 10/34  Loss: 1.639744\n","[Epoch 2] Batch 20/34  Loss: 1.563399\n","[Epoch 2] Batch 30/34  Loss: 1.709218\n","Epoch 2 completed. Loss: 1.585354\n","Epoch 2: Train=1.585354  Val=1.256913\n","Saved BEST model.\n","[Epoch 3] Batch 10/34  Loss: 1.292880\n","[Epoch 3] Batch 20/34  Loss: 1.250453\n","[Epoch 3] Batch 30/34  Loss: 1.252015\n","Epoch 3 completed. Loss: 1.167400\n","Epoch 3: Train=1.167400  Val=1.169355\n","Saved BEST model.\n","[Epoch 4] Batch 10/34  Loss: 1.246922\n","[Epoch 4] Batch 20/34  Loss: 0.951809\n","[Epoch 4] Batch 30/34  Loss: 1.020140\n","Epoch 4 completed. Loss: 1.026025\n","Epoch 4: Train=1.026025  Val=1.144753\n","Saved BEST model.\n","[Epoch 5] Batch 10/34  Loss: 0.775790\n","[Epoch 5] Batch 20/34  Loss: 0.872756\n","[Epoch 5] Batch 30/34  Loss: 1.023157\n","Epoch 5 completed. Loss: 0.988547\n","Epoch 5: Train=0.988547  Val=1.173916\n","Saved checkpoint.\n","[Epoch 6] Batch 10/34  Loss: 0.914303\n","[Epoch 6] Batch 20/34  Loss: 0.798965\n","[Epoch 6] Batch 30/34  Loss: 1.001062\n","Epoch 6 completed. Loss: 0.965681\n","Epoch 6: Train=0.965681  Val=1.164291\n","[Epoch 7] Batch 10/34  Loss: 0.826398\n","[Epoch 7] Batch 20/34  Loss: 0.905877\n","[Epoch 7] Batch 30/34  Loss: 0.947085\n","Epoch 7 completed. Loss: 0.948457\n","Epoch 7: Train=0.948457  Val=1.070394\n","Saved BEST model.\n","[Epoch 8] Batch 10/34  Loss: 1.024650\n","[Epoch 8] Batch 20/34  Loss: 0.973510\n","[Epoch 8] Batch 30/34  Loss: 0.916595\n","Epoch 8 completed. Loss: 0.876480\n","Epoch 8: Train=0.876480  Val=1.019533\n","Saved BEST model.\n","[Epoch 9] Batch 10/34  Loss: 0.981105\n","[Epoch 9] Batch 20/34  Loss: 0.818294\n","[Epoch 9] Batch 30/34  Loss: 0.892272\n","Epoch 9 completed. Loss: 0.865288\n","Epoch 9: Train=0.865288  Val=0.974840\n","Saved BEST model.\n","[Epoch 10] Batch 10/34  Loss: 1.102276\n","[Epoch 10] Batch 20/34  Loss: 0.836910\n","[Epoch 10] Batch 30/34  Loss: 0.811439\n","Epoch 10 completed. Loss: 0.813717\n","Epoch 10: Train=0.813717  Val=0.963256\n","Saved BEST model.\n","Saved checkpoint.\n","[Epoch 11] Batch 10/34  Loss: 0.897919\n","[Epoch 11] Batch 20/34  Loss: 0.773981\n","[Epoch 11] Batch 30/34  Loss: 0.787788\n","Epoch 11 completed. Loss: 0.779449\n","Epoch 11: Train=0.779449  Val=0.877006\n","Saved BEST model.\n","[Epoch 12] Batch 10/34  Loss: 0.748695\n","[Epoch 12] Batch 20/34  Loss: 0.665087\n","[Epoch 12] Batch 30/34  Loss: 0.695843\n","Epoch 12 completed. Loss: 0.680614\n","Epoch 12: Train=0.680614  Val=0.720288\n","Saved BEST model.\n","[Epoch 13] Batch 10/34  Loss: 0.643150\n","[Epoch 13] Batch 20/34  Loss: 0.701785\n","[Epoch 13] Batch 30/34  Loss: 0.714820\n","Epoch 13 completed. Loss: 0.694497\n","Epoch 13: Train=0.694497  Val=0.677716\n","Saved BEST model.\n","[Epoch 14] Batch 10/34  Loss: 0.613955\n","[Epoch 14] Batch 20/34  Loss: 0.601443\n","[Epoch 14] Batch 30/34  Loss: 0.661286\n","Epoch 14 completed. Loss: 0.651542\n","Epoch 14: Train=0.651542  Val=0.742788\n","[Epoch 15] Batch 10/34  Loss: 0.573574\n","[Epoch 15] Batch 20/34  Loss: 0.486694\n","[Epoch 15] Batch 30/34  Loss: 0.619684\n","Epoch 15 completed. Loss: 0.619397\n","Epoch 15: Train=0.619397  Val=0.700489\n","Saved checkpoint.\n","[Epoch 16] Batch 10/34  Loss: 0.470404\n","[Epoch 16] Batch 20/34  Loss: 0.614072\n","[Epoch 16] Batch 30/34  Loss: 0.629120\n","Epoch 16 completed. Loss: 0.624726\n","Epoch 16: Train=0.624726  Val=0.585862\n","Saved BEST model.\n","[Epoch 17] Batch 10/34  Loss: 0.486283\n","[Epoch 17] Batch 20/34  Loss: 0.525740\n","[Epoch 17] Batch 30/34  Loss: 0.512013\n","Epoch 17 completed. Loss: 0.578047\n","Epoch 17: Train=0.578047  Val=0.568740\n","Saved BEST model.\n","[Epoch 18] Batch 10/34  Loss: 0.544739\n","[Epoch 18] Batch 20/34  Loss: 0.632068\n","[Epoch 18] Batch 30/34  Loss: 0.570453\n","Epoch 18 completed. Loss: 0.554210\n","Epoch 18: Train=0.554210  Val=0.536432\n","Saved BEST model.\n","[Epoch 19] Batch 10/34  Loss: 0.685906\n","[Epoch 19] Batch 20/34  Loss: 0.566529\n","[Epoch 19] Batch 30/34  Loss: 0.546332\n","Epoch 19 completed. Loss: 0.560567\n","Epoch 19: Train=0.560567  Val=0.584435\n","[Epoch 20] Batch 10/34  Loss: 0.707428\n","[Epoch 20] Batch 20/34  Loss: 0.547298\n","[Epoch 20] Batch 30/34  Loss: 0.557049\n","Epoch 20 completed. Loss: 0.566332\n","Epoch 20: Train=0.566332  Val=0.602165\n","Saved checkpoint.\n","[Epoch 21] Batch 10/34  Loss: 0.626476\n","[Epoch 21] Batch 20/34  Loss: 0.520145\n","[Epoch 21] Batch 30/34  Loss: 0.497902\n","Epoch 21 completed. Loss: 0.509234\n","Epoch 21: Train=0.509234  Val=0.511315\n","Saved BEST model.\n","[Epoch 22] Batch 10/34  Loss: 0.383272\n","[Epoch 22] Batch 20/34  Loss: 0.508006\n","[Epoch 22] Batch 30/34  Loss: 0.515150\n","Epoch 22 completed. Loss: 0.528349\n","Epoch 22: Train=0.528349  Val=0.496098\n","Saved BEST model.\n","[Epoch 23] Batch 10/34  Loss: 0.461971\n","[Epoch 23] Batch 20/34  Loss: 0.487104\n","[Epoch 23] Batch 30/34  Loss: 0.452702\n","Epoch 23 completed. Loss: 0.489292\n","Epoch 23: Train=0.489292  Val=0.498243\n","[Epoch 24] Batch 10/34  Loss: 0.517816\n","[Epoch 24] Batch 20/34  Loss: 0.528941\n","[Epoch 24] Batch 30/34  Loss: 0.506709\n","Epoch 24 completed. Loss: 0.498039\n","Epoch 24: Train=0.498039  Val=0.521012\n","[Epoch 25] Batch 10/34  Loss: 0.538402\n","[Epoch 25] Batch 20/34  Loss: 0.473222\n","[Epoch 25] Batch 30/34  Loss: 0.487104\n","Epoch 25 completed. Loss: 0.484947\n","Epoch 25: Train=0.484947  Val=0.492136\n","Saved BEST model.\n","Saved checkpoint.\n","[Epoch 26] Batch 10/34  Loss: 0.458203\n","[Epoch 26] Batch 20/34  Loss: 0.456840\n","[Epoch 26] Batch 30/34  Loss: 0.438326\n","Epoch 26 completed. Loss: 0.429021\n","Epoch 26: Train=0.429021  Val=0.472751\n","Saved BEST model.\n","[Epoch 27] Batch 10/34  Loss: 0.578838\n","[Epoch 27] Batch 20/34  Loss: 0.507136\n","[Epoch 27] Batch 30/34  Loss: 0.448895\n","Epoch 27 completed. Loss: 0.447139\n","Epoch 27: Train=0.447139  Val=0.468430\n","Saved BEST model.\n","[Epoch 28] Batch 10/34  Loss: 0.371478\n","[Epoch 28] Batch 20/34  Loss: 0.430952\n","[Epoch 28] Batch 30/34  Loss: 0.442907\n","Epoch 28 completed. Loss: 0.434104\n","Epoch 28: Train=0.434104  Val=0.466117\n","Saved BEST model.\n","[Epoch 29] Batch 10/34  Loss: 0.454607\n","[Epoch 29] Batch 20/34  Loss: 0.419772\n","[Epoch 29] Batch 30/34  Loss: 0.435267\n","Epoch 29 completed. Loss: 0.428472\n","Epoch 29: Train=0.428472  Val=0.459587\n","Saved BEST model.\n","[Epoch 30] Batch 10/34  Loss: 0.456185\n","[Epoch 30] Batch 20/34  Loss: 0.472959\n","[Epoch 30] Batch 30/34  Loss: 0.460165\n","Epoch 30 completed. Loss: 0.464811\n","Epoch 30: Train=0.464811  Val=0.491163\n","Saved checkpoint.\n","[Epoch 31] Batch 10/34  Loss: 0.410637\n","[Epoch 31] Batch 20/34  Loss: 0.410278\n","[Epoch 31] Batch 30/34  Loss: 0.417144\n","Epoch 31 completed. Loss: 0.416915\n","Epoch 31: Train=0.416915  Val=0.431250\n","Saved BEST model.\n","[Epoch 32] Batch 10/34  Loss: 0.640093\n","[Epoch 32] Batch 20/34  Loss: 0.491146\n","[Epoch 32] Batch 30/34  Loss: 0.475933\n","Epoch 32 completed. Loss: 0.458577\n","Epoch 32: Train=0.458577  Val=0.464805\n","[Epoch 33] Batch 10/34  Loss: 0.444294\n","[Epoch 33] Batch 20/34  Loss: 0.410147\n","[Epoch 33] Batch 30/34  Loss: 0.402941\n","Epoch 33 completed. Loss: 0.390920\n","Epoch 33: Train=0.390920  Val=0.429075\n","Saved BEST model.\n","[Epoch 34] Batch 10/34  Loss: 0.334450\n","[Epoch 34] Batch 20/34  Loss: 0.476874\n","[Epoch 34] Batch 30/34  Loss: 0.432612\n","Epoch 34 completed. Loss: 0.420902\n","Epoch 34: Train=0.420902  Val=0.443880\n","[Epoch 35] Batch 10/34  Loss: 0.392971\n","[Epoch 35] Batch 20/34  Loss: 0.408119\n","[Epoch 35] Batch 30/34  Loss: 0.404610\n","Epoch 35 completed. Loss: 0.389057\n","Epoch 35: Train=0.389057  Val=0.429587\n","Saved checkpoint.\n","[Epoch 36] Batch 10/34  Loss: 0.331596\n","[Epoch 36] Batch 20/34  Loss: 0.355912\n","[Epoch 36] Batch 30/34  Loss: 0.359097\n","Epoch 36 completed. Loss: 0.384946\n","Epoch 36: Train=0.384946  Val=0.421805\n","Saved BEST model.\n","[Epoch 37] Batch 10/34  Loss: 0.500091\n","[Epoch 37] Batch 20/34  Loss: 0.442889\n","[Epoch 37] Batch 30/34  Loss: 0.395123\n","Epoch 37 completed. Loss: 0.404598\n","Epoch 37: Train=0.404598  Val=0.441224\n","[Epoch 38] Batch 10/34  Loss: 0.454354\n","[Epoch 38] Batch 20/34  Loss: 0.389223\n","[Epoch 38] Batch 30/34  Loss: 0.376616\n","Epoch 38 completed. Loss: 0.398301\n","Epoch 38: Train=0.398301  Val=0.418880\n","Saved BEST model.\n","[Epoch 39] Batch 10/34  Loss: 0.480977\n","[Epoch 39] Batch 20/34  Loss: 0.394748\n","[Epoch 39] Batch 30/34  Loss: 0.374167\n","Epoch 39 completed. Loss: 0.388598\n","Epoch 39: Train=0.388598  Val=0.425817\n","[Epoch 40] Batch 10/34  Loss: 0.330196\n","[Epoch 40] Batch 20/34  Loss: 0.384199\n","[Epoch 40] Batch 30/34  Loss: 0.385168\n","Epoch 40 completed. Loss: 0.376179\n","Epoch 40: Train=0.376179  Val=0.404574\n","Saved BEST model.\n","Saved checkpoint.\n","[Epoch 41] Batch 10/34  Loss: 0.333855\n","[Epoch 41] Batch 20/34  Loss: 0.332396\n","[Epoch 41] Batch 30/34  Loss: 0.344116\n","Epoch 41 completed. Loss: 0.352366\n","Epoch 41: Train=0.352366  Val=0.425989\n","[Epoch 42] Batch 10/34  Loss: 0.320003\n","[Epoch 42] Batch 20/34  Loss: 0.305582\n","[Epoch 42] Batch 30/34  Loss: 0.348515\n","Epoch 42 completed. Loss: 0.369196\n","Epoch 42: Train=0.369196  Val=0.420506\n","[Epoch 43] Batch 10/34  Loss: 0.404135\n","[Epoch 43] Batch 20/34  Loss: 0.377521\n","[Epoch 43] Batch 30/34  Loss: 0.350643\n","Epoch 43 completed. Loss: 0.353476\n","Epoch 43: Train=0.353476  Val=0.413490\n","[Epoch 44] Batch 10/34  Loss: 0.373018\n","[Epoch 44] Batch 20/34  Loss: 0.366010\n","[Epoch 44] Batch 30/34  Loss: 0.351012\n","Epoch 44 completed. Loss: 0.351493\n","Epoch 44: Train=0.351493  Val=0.449940\n","[Epoch 45] Batch 10/34  Loss: 0.311147\n","[Epoch 45] Batch 20/34  Loss: 0.388522\n","[Epoch 45] Batch 30/34  Loss: 0.390741\n","Epoch 45 completed. Loss: 0.377214\n","Epoch 45: Train=0.377214  Val=0.420905\n","Saved checkpoint.\n","[Epoch 46] Batch 10/34  Loss: 0.257687\n","[Epoch 46] Batch 20/34  Loss: 0.312060\n","[Epoch 46] Batch 30/34  Loss: 0.354835\n","Epoch 46 completed. Loss: 0.353545\n","Epoch 46: Train=0.353545  Val=0.439245\n","[Epoch 47] Batch 10/34  Loss: 0.322550\n","[Epoch 47] Batch 20/34  Loss: 0.335800\n","[Epoch 47] Batch 30/34  Loss: 0.364689\n","Epoch 47 completed. Loss: 0.352817\n","Epoch 47: Train=0.352817  Val=0.432922\n","[Epoch 48] Batch 10/34  Loss: 0.306735\n","[Epoch 48] Batch 20/34  Loss: 0.367278\n","[Epoch 48] Batch 30/34  Loss: 0.344885\n","Epoch 48 completed. Loss: 0.333952\n","Epoch 48: Train=0.333952  Val=0.407802\n","[Epoch 49] Batch 10/34  Loss: 0.349349\n","[Epoch 49] Batch 20/34  Loss: 0.357235\n","[Epoch 49] Batch 30/34  Loss: 0.352606\n","Epoch 49 completed. Loss: 0.346731\n","Epoch 49: Train=0.346731  Val=0.449149\n","[Epoch 50] Batch 10/34  Loss: 0.312235\n","[Epoch 50] Batch 20/34  Loss: 0.333427\n","[Epoch 50] Batch 30/34  Loss: 0.339144\n","Epoch 50 completed. Loss: 0.318250\n","Epoch 50: Train=0.318250  Val=0.408404\n","Saved checkpoint.\n","[Epoch 51] Batch 10/34  Loss: 0.430683\n","[Epoch 51] Batch 20/34  Loss: 0.396238\n","[Epoch 51] Batch 30/34  Loss: 0.348605\n","Epoch 51 completed. Loss: 0.365508\n","Epoch 51: Train=0.365508  Val=0.368486\n","Saved BEST model.\n","[Epoch 52] Batch 10/34  Loss: 0.287009\n","[Epoch 52] Batch 20/34  Loss: 0.296212\n","[Epoch 52] Batch 30/34  Loss: 0.320635\n","Epoch 52 completed. Loss: 0.323495\n","Epoch 52: Train=0.323495  Val=0.405312\n","[Epoch 53] Batch 10/34  Loss: 0.405328\n","[Epoch 53] Batch 20/34  Loss: 0.380142\n","[Epoch 53] Batch 30/34  Loss: 0.351197\n","Epoch 53 completed. Loss: 0.335058\n","Epoch 53: Train=0.335058  Val=0.414142\n","[Epoch 54] Batch 10/34  Loss: 0.422497\n","[Epoch 54] Batch 20/34  Loss: 0.344598\n","[Epoch 54] Batch 30/34  Loss: 0.333643\n","Epoch 54 completed. Loss: 0.336189\n","Epoch 54: Train=0.336189  Val=0.398592\n","[Epoch 55] Batch 10/34  Loss: 0.347042\n","[Epoch 55] Batch 20/34  Loss: 0.282684\n","[Epoch 55] Batch 30/34  Loss: 0.302693\n","Epoch 55 completed. Loss: 0.307681\n","Epoch 55: Train=0.307681  Val=0.404474\n","Saved checkpoint.\n","[Epoch 56] Batch 10/34  Loss: 0.319389\n","[Epoch 56] Batch 20/34  Loss: 0.339699\n","[Epoch 56] Batch 30/34  Loss: 0.335272\n","Epoch 56 completed. Loss: 0.317780\n","Epoch 56: Train=0.317780  Val=0.422044\n","[Epoch 57] Batch 10/34  Loss: 0.343958\n","[Epoch 57] Batch 20/34  Loss: 0.305208\n","[Epoch 57] Batch 30/34  Loss: 0.321767\n","Epoch 57 completed. Loss: 0.317503\n","Epoch 57: Train=0.317503  Val=0.472691\n","[Epoch 58] Batch 10/34  Loss: 0.335894\n","[Epoch 58] Batch 20/34  Loss: 0.327875\n","[Epoch 58] Batch 30/34  Loss: 0.324967\n","Epoch 58 completed. Loss: 0.310963\n","Epoch 58: Train=0.310963  Val=0.528535\n","[Epoch 59] Batch 10/34  Loss: 0.367342\n","[Epoch 59] Batch 20/34  Loss: 0.381715\n","[Epoch 59] Batch 30/34  Loss: 0.343950\n","Epoch 59 completed. Loss: 0.334993\n","Epoch 59: Train=0.334993  Val=0.421052\n","[Epoch 60] Batch 10/34  Loss: 0.317350\n","[Epoch 60] Batch 20/34  Loss: 0.318583\n","[Epoch 60] Batch 30/34  Loss: 0.313970\n","Epoch 60 completed. Loss: 0.323744\n","Epoch 60: Train=0.323744  Val=0.402488\n","Saved checkpoint.\n","[Epoch 61] Batch 10/34  Loss: 0.256061\n","[Epoch 61] Batch 20/34  Loss: 0.273058\n","[Epoch 61] Batch 30/34  Loss: 0.297746\n","Epoch 61 completed. Loss: 0.314035\n","Epoch 61: Train=0.314035  Val=0.440101\n","[Epoch 62] Batch 10/34  Loss: 0.332787\n","[Epoch 62] Batch 20/34  Loss: 0.300015\n","[Epoch 62] Batch 30/34  Loss: 0.308286\n","Epoch 62 completed. Loss: 0.297570\n","Epoch 62: Train=0.297570  Val=0.411791\n","[Epoch 63] Batch 10/34  Loss: 0.263596\n","[Epoch 63] Batch 20/34  Loss: 0.254136\n","[Epoch 63] Batch 30/34  Loss: 0.281312\n","Epoch 63 completed. Loss: 0.290786\n","Epoch 63: Train=0.290786  Val=0.408986\n","[Epoch 64] Batch 10/34  Loss: 0.299954\n","[Epoch 64] Batch 20/34  Loss: 0.347317\n","[Epoch 64] Batch 30/34  Loss: 0.341215\n","Epoch 64 completed. Loss: 0.328260\n","Epoch 64: Train=0.328260  Val=0.432451\n","[Epoch 65] Batch 10/34  Loss: 0.362509\n","[Epoch 65] Batch 20/34  Loss: 0.326368\n","[Epoch 65] Batch 30/34  Loss: 0.295965\n","Epoch 65 completed. Loss: 0.289305\n","Epoch 65: Train=0.289305  Val=0.420382\n","Saved checkpoint.\n","[Epoch 66] Batch 10/34  Loss: 0.375414\n","[Epoch 66] Batch 20/34  Loss: 0.324830\n","[Epoch 66] Batch 30/34  Loss: 0.308859\n","Epoch 66 completed. Loss: 0.307096\n","Epoch 66: Train=0.307096  Val=0.426936\n","[Epoch 67] Batch 10/34  Loss: 0.342501\n","[Epoch 67] Batch 20/34  Loss: 0.296195\n","[Epoch 67] Batch 30/34  Loss: 0.271207\n","Epoch 67 completed. Loss: 0.283350\n","Epoch 67: Train=0.283350  Val=0.410572\n","[Epoch 68] Batch 10/34  Loss: 0.311399\n","[Epoch 68] Batch 20/34  Loss: 0.317901\n","[Epoch 68] Batch 30/34  Loss: 0.305560\n","Epoch 68 completed. Loss: 0.300092\n","Epoch 68: Train=0.300092  Val=0.430503\n","[Epoch 69] Batch 10/34  Loss: 0.237620\n","[Epoch 69] Batch 20/34  Loss: 0.270962\n","[Epoch 69] Batch 30/34  Loss: 0.275070\n","Epoch 69 completed. Loss: 0.276108\n","Epoch 69: Train=0.276108  Val=0.407534\n","[Epoch 70] Batch 10/34  Loss: 0.293368\n","[Epoch 70] Batch 20/34  Loss: 0.267174\n","[Epoch 70] Batch 30/34  Loss: 0.263002\n","Epoch 70 completed. Loss: 0.271345\n","Epoch 70: Train=0.271345  Val=0.398666\n","Saved checkpoint.\n","[Epoch 71] Batch 10/34  Loss: 0.243633\n","[Epoch 71] Batch 20/34  Loss: 0.270555\n","[Epoch 71] Batch 30/34  Loss: 0.282534\n","Epoch 71 completed. Loss: 0.278482\n","Epoch 71: Train=0.278482  Val=0.390294\n","[Epoch 72] Batch 10/34  Loss: 0.301808\n","[Epoch 72] Batch 20/34  Loss: 0.273324\n","[Epoch 72] Batch 30/34  Loss: 0.285150\n","Epoch 72 completed. Loss: 0.272332\n","Epoch 72: Train=0.272332  Val=0.439509\n","[Epoch 73] Batch 10/34  Loss: 0.254973\n","[Epoch 73] Batch 20/34  Loss: 0.256802\n","[Epoch 73] Batch 30/34  Loss: 0.284784\n","Epoch 73 completed. Loss: 0.279182\n","Epoch 73: Train=0.279182  Val=0.381978\n","[Epoch 74] Batch 10/34  Loss: 0.270980\n","[Epoch 74] Batch 20/34  Loss: 0.255800\n","[Epoch 74] Batch 30/34  Loss: 0.269371\n","Epoch 74 completed. Loss: 0.261103\n","Epoch 74: Train=0.261103  Val=0.402611\n","[Epoch 75] Batch 10/34  Loss: 0.288372\n","[Epoch 75] Batch 20/34  Loss: 0.284137\n","[Epoch 75] Batch 30/34  Loss: 0.266463\n","Epoch 75 completed. Loss: 0.275947\n","Epoch 75: Train=0.275947  Val=0.397361\n","Saved checkpoint.\n","[Epoch 76] Batch 10/34  Loss: 0.275161\n","[Epoch 76] Batch 20/34  Loss: 0.236908\n","[Epoch 76] Batch 30/34  Loss: 0.261649\n","Epoch 76 completed. Loss: 0.259781\n","Epoch 76: Train=0.259781  Val=0.392319\n","[Epoch 77] Batch 10/34  Loss: 0.293550\n","[Epoch 77] Batch 20/34  Loss: 0.277956\n","[Epoch 77] Batch 30/34  Loss: 0.281572\n","Epoch 77 completed. Loss: 0.270981\n","Epoch 77: Train=0.270981  Val=0.404322\n","[Epoch 78] Batch 10/34  Loss: 0.267565\n","[Epoch 78] Batch 20/34  Loss: 0.248341\n","[Epoch 78] Batch 30/34  Loss: 0.254615\n","Epoch 78 completed. Loss: 0.260217\n","Epoch 78: Train=0.260217  Val=0.428984\n","[Epoch 79] Batch 10/34  Loss: 0.275805\n","[Epoch 79] Batch 20/34  Loss: 0.253511\n","[Epoch 79] Batch 30/34  Loss: 0.267913\n","Epoch 79 completed. Loss: 0.259422\n","Epoch 79: Train=0.259422  Val=0.441922\n","[Epoch 80] Batch 10/34  Loss: 0.219007\n","[Epoch 80] Batch 20/34  Loss: 0.246981\n","[Epoch 80] Batch 30/34  Loss: 0.249006\n","Epoch 80 completed. Loss: 0.251659\n","Epoch 80: Train=0.251659  Val=0.386937\n","Saved checkpoint.\n","[Epoch 81] Batch 10/34  Loss: 0.264247\n","[Epoch 81] Batch 20/34  Loss: 0.243164\n","[Epoch 81] Batch 30/34  Loss: 0.240475\n","Epoch 81 completed. Loss: 0.249487\n","Epoch 81: Train=0.249487  Val=0.403541\n","[Epoch 82] Batch 10/34  Loss: 0.219589\n","[Epoch 82] Batch 20/34  Loss: 0.252354\n","[Epoch 82] Batch 30/34  Loss: 0.251435\n","Epoch 82 completed. Loss: 0.243287\n","Epoch 82: Train=0.243287  Val=0.410770\n","[Epoch 83] Batch 10/34  Loss: 0.281965\n","[Epoch 83] Batch 20/34  Loss: 0.310525\n","[Epoch 83] Batch 30/34  Loss: 0.294096\n","Epoch 83 completed. Loss: 0.280516\n","Epoch 83: Train=0.280516  Val=0.385049\n","[Epoch 84] Batch 10/34  Loss: 0.293816\n","[Epoch 84] Batch 20/34  Loss: 0.286129\n","[Epoch 84] Batch 30/34  Loss: 0.283987\n","Epoch 84 completed. Loss: 0.276235\n","Epoch 84: Train=0.276235  Val=0.449741\n","[Epoch 85] Batch 10/34  Loss: 0.204810\n","[Epoch 85] Batch 20/34  Loss: 0.282084\n","[Epoch 85] Batch 30/34  Loss: 0.272447\n","Epoch 85 completed. Loss: 0.258184\n","Epoch 85: Train=0.258184  Val=0.464472\n","Saved checkpoint.\n","[Epoch 86] Batch 10/34  Loss: 0.291430\n","[Epoch 86] Batch 20/34  Loss: 0.246033\n","[Epoch 86] Batch 30/34  Loss: 0.249893\n","Epoch 86 completed. Loss: 0.254893\n","Epoch 86: Train=0.254893  Val=0.406128\n","[Epoch 87] Batch 10/34  Loss: 0.274009\n","[Epoch 87] Batch 20/34  Loss: 0.273424\n","[Epoch 87] Batch 30/34  Loss: 0.273012\n","Epoch 87 completed. Loss: 0.269727\n","Epoch 87: Train=0.269727  Val=0.451301\n","[Epoch 88] Batch 10/34  Loss: 0.260905\n","[Epoch 88] Batch 20/34  Loss: 0.254372\n","[Epoch 88] Batch 30/34  Loss: 0.250994\n","Epoch 88 completed. Loss: 0.248772\n","Epoch 88: Train=0.248772  Val=0.438217\n","[Epoch 89] Batch 10/34  Loss: 0.271639\n","[Epoch 89] Batch 20/34  Loss: 0.241586\n","[Epoch 89] Batch 30/34  Loss: 0.245584\n","Epoch 89 completed. Loss: 0.240982\n","Epoch 89: Train=0.240982  Val=0.409596\n","[Epoch 90] Batch 10/34  Loss: 0.233357\n","[Epoch 90] Batch 20/34  Loss: 0.224992\n","[Epoch 90] Batch 30/34  Loss: 0.241260\n","Epoch 90 completed. Loss: 0.239473\n","Epoch 90: Train=0.239473  Val=0.423925\n","Saved checkpoint.\n","[Epoch 91] Batch 10/34  Loss: 0.237737\n","[Epoch 91] Batch 20/34  Loss: 0.228841\n","[Epoch 91] Batch 30/34  Loss: 0.240582\n","Epoch 91 completed. Loss: 0.235892\n","Epoch 91: Train=0.235892  Val=0.448118\n","[Epoch 92] Batch 10/34  Loss: 0.223271\n","[Epoch 92] Batch 20/34  Loss: 0.250675\n","[Epoch 92] Batch 30/34  Loss: 0.244911\n","Epoch 92 completed. Loss: 0.238512\n","Epoch 92: Train=0.238512  Val=0.423882\n","[Epoch 93] Batch 10/34  Loss: 0.209759\n","[Epoch 93] Batch 20/34  Loss: 0.228361\n","[Epoch 93] Batch 30/34  Loss: 0.243970\n","Epoch 93 completed. Loss: 0.257293\n","Epoch 93: Train=0.257293  Val=0.476762\n","[Epoch 94] Batch 10/34  Loss: 0.248507\n","[Epoch 94] Batch 20/34  Loss: 0.248978\n","[Epoch 94] Batch 30/34  Loss: 0.236132\n","Epoch 94 completed. Loss: 0.254882\n","Epoch 94: Train=0.254882  Val=0.469410\n","[Epoch 95] Batch 10/34  Loss: 0.320519\n","[Epoch 95] Batch 20/34  Loss: 0.306668\n","[Epoch 95] Batch 30/34  Loss: 0.286744\n","Epoch 95 completed. Loss: 0.276426\n","Epoch 95: Train=0.276426  Val=0.396672\n","Saved checkpoint.\n","[Epoch 96] Batch 10/34  Loss: 0.284567\n","[Epoch 96] Batch 20/34  Loss: 0.261664\n","[Epoch 96] Batch 30/34  Loss: 0.247562\n","Epoch 96 completed. Loss: 0.243690\n","Epoch 96: Train=0.243690  Val=0.446366\n","[Epoch 97] Batch 10/34  Loss: 0.206955\n","[Epoch 97] Batch 20/34  Loss: 0.221405\n","[Epoch 97] Batch 30/34  Loss: 0.239191\n","Epoch 97 completed. Loss: 0.241399\n","Epoch 97: Train=0.241399  Val=0.478556\n","[Epoch 98] Batch 10/34  Loss: 0.318166\n","[Epoch 98] Batch 20/34  Loss: 0.289948\n","[Epoch 98] Batch 30/34  Loss: 0.263942\n","Epoch 98 completed. Loss: 0.251127\n","Epoch 98: Train=0.251127  Val=0.440358\n","[Epoch 99] Batch 10/34  Loss: 0.189884\n","[Epoch 99] Batch 20/34  Loss: 0.228523\n","[Epoch 99] Batch 30/34  Loss: 0.233664\n","Epoch 99 completed. Loss: 0.235078\n","Epoch 99: Train=0.235078  Val=0.408804\n","[Epoch 100] Batch 10/34  Loss: 0.235673\n","[Epoch 100] Batch 20/34  Loss: 0.254821\n","[Epoch 100] Batch 30/34  Loss: 0.241622\n","Epoch 100 completed. Loss: 0.236867\n","Epoch 100: Train=0.236867  Val=0.408589\n","Saved checkpoint.\n","Training complete. Final model saved.\n"]}],"source":["# --- MAIN TRAINING LOOP ---\n","def main():\n","    # Dataset\n","    dataset = PtDataset(IMG_GLOB, GT_GLOB, transform=None)\n","    print(\"Total matched samples:\", len(dataset))\n","\n","    # Split train/val\n","    val_split = 0.1\n","    n_val = max(1, int(len(dataset) * val_split))\n","    n_train = len(dataset) - n_val\n","    train_set, val_set = random_split(dataset, [n_train, n_val])\n","\n","    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,\n","                              num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n","    val_loader   = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False,\n","                              num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n","\n","    # Model\n","    model = CSRNet(load_weights=True).to(DEVICE)\n","    criterion = nn.MSELoss().to(DEVICE)\n","    optimizer = optim.Adam(model.parameters(), lr=LR)\n","\n","    best_val = float(\"inf\")\n","\n","    for epoch in range(1, NUM_EPOCHS + 1):\n","        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE, epoch)\n","        val_loss   = validate(model, val_loader, criterion, DEVICE)\n","\n","        print(f\"Epoch {epoch}: Train={train_loss:.6f}  Val={val_loss:.6f}\")\n","\n","        # Save best\n","        if val_loss < best_val:\n","            best_val = val_loss\n","            torch.save(model.state_dict(), f\"{CHECKPOINT_DIR}/csrnet_best.pth\")\n","            print(\"Saved BEST model.\")\n","\n","        # Save periodic checkpoints\n","        if epoch % SAVE_EVERY == 0:\n","            torch.save(model.state_dict(), f\"{CHECKPOINT_DIR}/csrnet_epoch{epoch}.pth\")\n","            print(\"Saved checkpoint.\")\n","\n","    # Final save\n","    torch.save(model.state_dict(), f\"{CHECKPOINT_DIR}/csrnet_final.pth\")\n","    print(\"Training complete. Final model saved.\")\n","\n","# Run\n","main()\n"]}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[],"gpuType":"T4"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}