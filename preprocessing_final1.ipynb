{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1ef441",
   "metadata": {},
   "source": [
    "preprocess_tensors_with_robust_mat_reader.py\n",
    "Full preprocessing pipeline that saves final PyTorch tensors (.pt).\n",
    "- Uses a robust read_points_from_mat to handle different .mat nestings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd38a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f3ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- CONFIG - edit these ----------------\n",
    "RAW_IMG_DIR = Path(r\"C:\\Users\\mahal\\OneDrive\\Desktop\\DL\\archive\\ShanghaiTech\\part_A\\train_data\\images\")\n",
    "RAW_GT_DIR  = Path(r\"C:\\Users\\mahal\\OneDrive\\Desktop\\DL\\archive\\ShanghaiTech\\part_A\\train_data\\ground-truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a270fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_IMG_TORCH_DIR = Path(r\"C:\\Users\\mahal\\OneDrive\\Desktop\\DL\\torch_images_trainA\")\n",
    "OUT_GT_TORCH_DIR  = Path(r\"C:\\Users\\mahal\\OneDrive\\Desktop\\DL\\torch_density_trainA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d751a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_W, TARGET_H = 512, 512\n",
    "DOWNSAMPLE_FACTOR = 8\n",
    "GAUSSIAN_SIGMA = 4\n",
    "CLEAN_METHOD = \"none\"        # \"none\", \"denoise\", \"clahe\", \"denoise+clahe\"\n",
    "NORMALIZE_METHOD = \"imagenet\" # \"imagenet\" recommended\n",
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "IMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\")\n",
    "# ----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e27506",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_IMG_TORCH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_GT_TORCH_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b77e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- robust mat reader ----------\n",
    "def read_points_from_mat(mat_path):\n",
    "    \"\"\"\n",
    "    Robust reader for ShanghaiTech-style GT .mat files.\n",
    "    Returns Nx2 float32 array of (x,y) points or empty array if nothing found.\n",
    "    \"\"\"\n",
    "    mat = sio.loadmat(mat_path)\n",
    "    # if common 'image_info' key exists try multiple nestings\n",
    "    if \"image_info\" in mat:\n",
    "        info = mat[\"image_info\"]\n",
    "        # try several common access patterns\n",
    "        candidates = []\n",
    "        try:\n",
    "            candidates.append(info[0][0][0][0])\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            candidates.append(info[0][0][0][0][0])\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            candidates.append(info[0][0])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        for cand in candidates:\n",
    "            if isinstance(cand, np.ndarray) and cand.ndim == 2 and cand.shape[1] == 2:\n",
    "                return cand.astype(np.float32)\n",
    "            if isinstance(cand, np.ndarray) and cand.dtype == object:\n",
    "                for item in cand.ravel():\n",
    "                    if isinstance(item, np.ndarray) and item.ndim == 2 and item.shape[1] == 2:\n",
    "                        return item.astype(np.float32)\n",
    "\n",
    "    # fallback: scan all keys for a 2-column numeric array or inside object arrays\n",
    "    for k, v in mat.items():\n",
    "        if isinstance(v, np.ndarray) and v.ndim == 2 and v.shape[1] == 2:\n",
    "            return v.astype(np.float32)\n",
    "        if isinstance(v, np.ndarray) and v.dtype == object:\n",
    "            for item in v.ravel():\n",
    "                if isinstance(item, np.ndarray) and item.ndim == 2 and item.shape[1] == 2:\n",
    "                    return item.astype(np.float32)\n",
    "\n",
    "    # nothing found\n",
    "    return np.zeros((0, 2), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833740d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- optional cleaning ----------\n",
    "def denoise(img_bgr):\n",
    "    return cv2.fastNlMeansDenoisingColored(img_bgr, None, 10, 10, 7, 21)\n",
    "def clahe(img_bgr):\n",
    "    ycrcb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2YCrCb)\n",
    "    y, cr, cb = cv2.split(ycrcb)\n",
    "    c = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    y2 = c.apply(y)\n",
    "    merged = cv2.merge((y2, cr, cb))\n",
    "    return cv2.cvtColor(merged, cv2.COLOR_YCrCb2BGR)\n",
    "def clean_image(img_bgr, method):\n",
    "    if method == \"none\":\n",
    "        return img_bgr\n",
    "    if method == \"denoise\":\n",
    "        return denoise(img_bgr)\n",
    "    if method == \"clahe\":\n",
    "        return clahe(img_bgr)\n",
    "    if method == \"denoise+clahe\":\n",
    "        return clahe(denoise(img_bgr))\n",
    "    return img_bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a8def",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---------- helpers ----------\n",
    "def list_images(folder):\n",
    "    return sorted([p.name for p in folder.iterdir() if p.suffix.lower() in EXTS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image_rgb(img_rgb, method=NORMALIZE_METHOD):\n",
    "    img = img_rgb.astype(np.float32) / 255.0\n",
    "    if method == \"imagenet\":\n",
    "        return (img - IMAGENET_MEAN) / IMAGENET_STD\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d859f08",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_density_map(img_h, img_w, points, sigma=GAUSSIAN_SIGMA):\n",
    "    density = np.zeros((img_h, img_w), dtype=np.float32)\n",
    "    for p in points:\n",
    "        x = int(min(img_w - 1, max(0, round(p[0]))))\n",
    "        y = int(min(img_h - 1, max(0, round(p[1]))))\n",
    "        density[y, x] += 1.0\n",
    "    density = gaussian_filter(density, sigma=sigma)\n",
    "    return density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27398aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_density(density, factor):\n",
    "    H, W = density.shape\n",
    "    new_h, new_w = H // factor, W // factor\n",
    "    if new_h <= 0 or new_w <= 0:\n",
    "        raise ValueError(\"Downsample factor too large for density size.\")\n",
    "    small = cv2.resize(density, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "    small = small * (factor * factor)  # preserve counts\n",
    "    return small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- main preprocessing ----------\n",
    "def preprocess_save_torch(\n",
    "    raw_img_dir=RAW_IMG_DIR,\n",
    "    raw_gt_dir=RAW_GT_DIR,\n",
    "    out_img_torch_dir=OUT_IMG_TORCH_DIR,\n",
    "    out_gt_torch_dir=OUT_GT_TORCH_DIR,\n",
    "    target_w=TARGET_W,\n",
    "    target_h=TARGET_H,\n",
    "    clean_method=CLEAN_METHOD,\n",
    "    normalize_method=NORMALIZE_METHOD,\n",
    "    downsample_factor=DOWNSAMPLE_FACTOR\n",
    "):\n",
    "    img_files = list_images(raw_img_dir)\n",
    "    print(f\"Found {len(img_files)} images in {raw_img_dir}\")\n",
    "\n",
    "    for i, fname in enumerate(img_files, 1):\n",
    "        base = os.path.splitext(fname)[0]\n",
    "        img_path = raw_img_dir / fname\n",
    "\n",
    "        # try GT filename patterns used by ShanghaiTech\n",
    "        gt_path1 = raw_gt_dir / f\"GT_{base}.mat\"\n",
    "        gt_path2 = raw_gt_dir / f\"{base}.mat\"\n",
    "        gt_path = gt_path1 if gt_path1.exists() else (gt_path2 if gt_path2.exists() else None)\n",
    "\n",
    "        if gt_path is None:\n",
    "            print(f\"[{i}/{len(img_files)}] Missing GT for {fname} (expected {gt_path1.name} or {gt_path2.name}) -> skipping\")\n",
    "            continue\n",
    "\n",
    "        img_bgr = cv2.imread(str(img_path))\n",
    "        if img_bgr is None:\n",
    "            print(f\"[{i}/{len(img_files)}] Failed to read image {img_path} -> skipping\")\n",
    "            continue\n",
    "\n",
    "        orig_h, orig_w = img_bgr.shape[:2]\n",
    "        sx = target_w / float(orig_w)\n",
    "        sy = target_h / float(orig_h)\n",
    "\n",
    "        # optional cleaning\n",
    "        img_bgr = clean_image(img_bgr, clean_method)\n",
    "\n",
    "        # resize\n",
    "        resized_bgr = cv2.resize(img_bgr, (target_w, target_h), interpolation=cv2.INTER_AREA)\n",
    "        resized_rgb = cv2.cvtColor(resized_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # normalize\n",
    "        img_norm = normalize_image_rgb(resized_rgb, method=normalize_method)\n",
    "\n",
    "        # to torch tensor [C,H,W]\n",
    "        img_t = torch.from_numpy(img_norm.astype(np.float32)).permute(2, 0, 1).contiguous()\n",
    "\n",
    "        # read points robustly and resize them to target\n",
    "        pts = read_points_from_mat(str(gt_path))\n",
    "        if pts.size != 0:\n",
    "            pts_resized = pts.copy()\n",
    "            pts_resized[:, 0] = pts[:, 0] * sx\n",
    "            pts_resized[:, 1] = pts[:, 1] * sy\n",
    "        else:\n",
    "            pts_resized = pts\n",
    "\n",
    "        # generate density full-res and downsample\n",
    "        density_full = generate_density_map(target_h, target_w, pts_resized, sigma=GAUSSIAN_SIGMA)\n",
    "        density_down = downsample_density(density_full, downsample_factor)\n",
    "\n",
    "        # to torch density tensor [1, H_down, W_down]\n",
    "        dens_t = torch.from_numpy(density_down.astype(np.float32)).unsqueeze(0).contiguous()\n",
    "\n",
    "        # save .pt files\n",
    "        img_out = out_img_torch_dir / (base + \".pt\")\n",
    "        gt_out  = out_gt_torch_dir  / (base + \".pt\")\n",
    "        torch.save(img_t, str(img_out))\n",
    "        torch.save(dens_t, str(gt_out))\n",
    "\n",
    "        print(f\"[{i}/{len(img_files)}] Saved {base}  pts:{pts_resized.shape[0]}  img:{img_t.shape} gt:{dens_t.shape}\")\n",
    "\n",
    "    print(\"\\nâœ” Preprocessing completed.\")\n",
    "    print(\"Images saved to:\", out_img_torch_dir)\n",
    "    print(\"GTs saved to   :\", out_gt_torch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8498cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Torch Dataset ----------\n",
    "class TorchCrowdDataset(Dataset):\n",
    "    def __init__(self, images_pt_dir, gt_pt_dir):\n",
    "        self.images_dir = Path(images_pt_dir)\n",
    "        self.gt_dir = Path(gt_pt_dir)\n",
    "        self.files = sorted([p.stem for p in self.images_dir.glob(\"*.pt\")])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        base = self.files[idx]\n",
    "        img_t = torch.load(self.images_dir.joinpath(base + \".pt\"))\n",
    "        gt_t  = torch.load(self.gt_dir.joinpath(base + \".pt\"))\n",
    "        return img_t.float(), gt_t.float(), base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- quick test when run as script ----------\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_save_torch()\n",
    "\n",
    "    # verify loader shapes\n",
    "    ds = TorchCrowdDataset(OUT_IMG_TORCH_DIR, OUT_GT_TORCH_DIR)\n",
    "    loader = DataLoader(ds, batch_size=4, shuffle=True, num_workers=0, pin_memory=False)\n",
    "    for imgs, gts, names in loader:\n",
    "        print(\"Batch imgs:\", imgs.shape)   # [B, C, H, W]\n",
    "        print(\"Batch gts :\", gts.shape)    # [B, 1, H_down, W_down]\n",
    "        print(\"Example file:\", names[0])\n",
    "        break\n",
    "\n",
    "    print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
