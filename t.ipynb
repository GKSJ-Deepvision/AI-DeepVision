{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d073a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ===================== DATASET =====================\n",
    "class CrowdDataset(Dataset):\n",
    "    def __init__(self, img_paths, dens_paths, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.dens_paths = dens_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.img_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        dens = np.load(self.dens_paths[idx]).astype(np.float32)\n",
    "        dens = torch.from_numpy(dens).unsqueeze(0)\n",
    "\n",
    "        return img, dens\n",
    "\n",
    "\n",
    "# ===================== TRANSFORMS =====================\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# ===================== PATHS =====================\n",
    "img_dir = \"processed_B/images\"\n",
    "dens_dir = \"processed_B/density\"\n",
    "\n",
    "img_paths = [os.path.join(img_dir, f) for f in sorted(os.listdir(img_dir))]\n",
    "dens_paths = [os.path.join(dens_dir, f) for f in sorted(os.listdir(dens_dir))]\n",
    "\n",
    "dataset = CrowdDataset(img_paths, dens_paths, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbef3df4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drive_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     62\u001b[39m train_loader = DataLoader(dataset, batch_size=\u001b[32m1\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# ----------------- CHECKPOINT PATHS -----------------\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# from google.colab import drive\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# drive.mount('/content/drive')\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# drive_path = \"/content/drive/MyDrive/CSRNet_training\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# os.makedirs(drive_path, exist_ok=True)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m checkpoint_path = os.path.join(\u001b[43mdrive_path\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mcsrnet_checkpoint.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m stats_path = os.path.join(drive_path, \u001b[33m\"\u001b[39m\u001b[33mtraining_stats.npz\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# ----------------- MODEL + OPTIMIZER -----------------\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'drive_path' is not defined"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# CSRNet Training Script with Resume + Stats Saving\n",
    "# =====================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ----------------- DEVICE -----------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------- MODEL -----------------\n",
    "class CSRNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CSRNet, self).__init__()\n",
    "\n",
    "        # Frontend\n",
    "        self.frontend = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Backend\n",
    "        self.backend = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, dilation=2, padding=2), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, dilation=2, padding=2), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, dilation=2, padding=2), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 3, dilation=2, padding=2), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, 3, dilation=2, padding=2), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, 3, dilation=2, padding=2), nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# ----------------- DATASET & DATALOADER -----------------\n",
    "# Assuming you have a Dataset object called \"dataset\"\n",
    "# Example: dataset = CrowdDataset(img_paths, dens_paths, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# ----------------- CHECKPOINT PATHS -----------------\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# drive_path = \"/content/drive/MyDrive/CSRNet_training\"\n",
    "# os.makedirs(drive_path, exist_ok=True)\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "# checkpoint_path = os.path.join(drive_path, \"csrnet_checkpoint.pth\")\n",
    "stats_path = os.path.join(drive_path, \"training_stats.npz\")\n",
    "\n",
    "# ----------------- MODEL + OPTIMIZER -----------------\n",
    "model = CSRNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# ----------------- RESUME -----------------\n",
    "start_epoch = 1\n",
    "loss_history = []\n",
    "gt_history = []\n",
    "pred_history = []\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"ðŸ”„ Resuming training from checkpoint...\")\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    start_epoch = ckpt[\"epoch\"] + 1\n",
    "\n",
    "    if os.path.exists(stats_path):\n",
    "        data = np.load(stats_path, allow_pickle=True)\n",
    "        loss_history = list(data[\"loss_history\"])\n",
    "        gt_history = list(data[\"gt_history\"])\n",
    "        pred_history = list(data[\"pred_history\"])\n",
    "else:\n",
    "    print(\"ðŸ†• Starting training from scratch!\")\n",
    "\n",
    "# ----------------- TRAIN LOOP -----------------\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    total_gt = 0.0\n",
    "    total_pred = 0.0\n",
    "\n",
    "    pbar = tqdm(train_loader, total=len(train_loader))\n",
    "\n",
    "    for img, dens in pbar:\n",
    "        img = img.to(device)\n",
    "        dens = dens.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(img)\n",
    "\n",
    "        if pred.shape != dens.shape:\n",
    "            pred = F.interpolate(pred, size=dens.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        loss = F.mse_loss(pred, dens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        epoch_loss += batch_loss\n",
    "\n",
    "        gt_count = dens.sum().item()\n",
    "        pred_count = pred.sum().item()\n",
    "\n",
    "        total_gt += gt_count\n",
    "        total_pred += pred_count\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"Epoch {epoch}/{num_epochs} | batch_loss={batch_loss:.10f} | gt={gt_count:.2f} | pred={pred_count:.2f}\"\n",
    "        )\n",
    "\n",
    "    # Save epoch stats\n",
    "    loss_history.append(epoch_loss / len(train_loader))\n",
    "    gt_history.append(total_gt)\n",
    "    pred_history.append(total_pred)\n",
    "\n",
    "    print(f\"\\n--- Epoch {epoch}/{num_epochs} | AvgLoss={loss_history[-1]:.10f} \"\n",
    "          f\"| GT={total_gt:.2f} | Pred={total_pred:.2f} | Diff={abs(total_gt-total_pred):.2f} ---\\n\")\n",
    "\n",
    "    # Save checkpoint & stats directly to Drive\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict()\n",
    "    }, checkpoint_path)\n",
    "\n",
    "    np.savez(stats_path,\n",
    "             loss_history=np.array(loss_history, dtype=float),\n",
    "             gt_history=np.array(gt_history, dtype=float),\n",
    "             pred_history=np.array(pred_history, dtype=float))\n",
    "\n",
    "    print(\"ðŸ’¾ Checkpoint + stats saved to Drive!\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
