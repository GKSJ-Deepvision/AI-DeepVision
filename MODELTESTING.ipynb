{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goNCUVm5WlAv",
        "outputId": "512072b7-80ae-4084-af5c-d1ee3e035405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Run this if your data is in Google Drive. If your files are already in /content/DEEPVISION, skip.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qirovco7Yh92",
        "outputId": "b57ed6bd-04b4-4992-8b6f-9e1e9739f573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ11U8psdV_q",
        "outputId": "931006e4-4abc-429e-a50b-3099c965e1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 3.2G\n",
            "drwx------ 2 root root 4.0K Nov 28 07:25 'Colab Notebooks'\n",
            "-rw------- 1 root root 3.2G Nov 28 07:09  DEEPVISION.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q DEEPVISION.zip -d /content"
      ],
      "metadata": {
        "id": "oVYAk6U3dc5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve2DPcPLd7rQ",
        "outputId": "0810c74e-bbf9-41ba-dd3c-f27427497d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEEPVISION  drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/DEEPVISION -type f -iname \"*.jpg\" | wc -l\n",
        "!find /content/DEEPVISION -type f -iname \"*.npy\" | wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvfZZW6DeBzZ",
        "outputId": "c74c8706-063a-4e2d-b68f-492fa876b48d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1201\n",
            "332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/DEEPVISION -type d -maxdepth 5 | sort"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJdcu-0ReV91",
        "outputId": "54f62dab-144d-45dc-c58b-1c55105ff902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "find: warning: you have specified the global option -maxdepth after the argument -type, but global options are not positional, i.e., -maxdepth affects tests specified before it as well as those specified after it.  Please specify global options before other arguments.\n",
            "/content/DEEPVISION\n",
            "/content/DEEPVISION/DATA\n",
            "/content/DEEPVISION/DATA/processed\n",
            "/content/DEEPVISION/DATA/processed/part_A\n",
            "/content/DEEPVISION/DATA/processed/part_A/density\n",
            "/content/DEEPVISION/DATA/ShanghaiTech\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_A\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_A/test_data\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_A/test_data/ground-truth\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_A/test_data/images\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_A/train_data\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_A/train_data/ground-truth\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_A/train_data/images\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/ground-truth\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/train_data\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/train_data/ground-truth\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/train_data/images\n",
            "/content/DEEPVISION/.git\n",
            "/content/DEEPVISION/.git/hooks\n",
            "/content/DEEPVISION/.git/info\n",
            "/content/DEEPVISION/.git/logs\n",
            "/content/DEEPVISION/.git/logs/refs\n",
            "/content/DEEPVISION/.git/logs/refs/heads\n",
            "/content/DEEPVISION/.git/logs/refs/remotes\n",
            "/content/DEEPVISION/.git/logs/refs/remotes/origin\n",
            "/content/DEEPVISION/.git/objects\n",
            "/content/DEEPVISION/.git/objects/00\n",
            "/content/DEEPVISION/.git/objects/01\n",
            "/content/DEEPVISION/.git/objects/02\n",
            "/content/DEEPVISION/.git/objects/03\n",
            "/content/DEEPVISION/.git/objects/04\n",
            "/content/DEEPVISION/.git/objects/05\n",
            "/content/DEEPVISION/.git/objects/06\n",
            "/content/DEEPVISION/.git/objects/07\n",
            "/content/DEEPVISION/.git/objects/08\n",
            "/content/DEEPVISION/.git/objects/09\n",
            "/content/DEEPVISION/.git/objects/0a\n",
            "/content/DEEPVISION/.git/objects/0b\n",
            "/content/DEEPVISION/.git/objects/0c\n",
            "/content/DEEPVISION/.git/objects/0d\n",
            "/content/DEEPVISION/.git/objects/0e\n",
            "/content/DEEPVISION/.git/objects/0f\n",
            "/content/DEEPVISION/.git/objects/10\n",
            "/content/DEEPVISION/.git/objects/11\n",
            "/content/DEEPVISION/.git/objects/12\n",
            "/content/DEEPVISION/.git/objects/13\n",
            "/content/DEEPVISION/.git/objects/14\n",
            "/content/DEEPVISION/.git/objects/15\n",
            "/content/DEEPVISION/.git/objects/16\n",
            "/content/DEEPVISION/.git/objects/17\n",
            "/content/DEEPVISION/.git/objects/18\n",
            "/content/DEEPVISION/.git/objects/19\n",
            "/content/DEEPVISION/.git/objects/1a\n",
            "/content/DEEPVISION/.git/objects/1b\n",
            "/content/DEEPVISION/.git/objects/1c\n",
            "/content/DEEPVISION/.git/objects/1d\n",
            "/content/DEEPVISION/.git/objects/1e\n",
            "/content/DEEPVISION/.git/objects/1f\n",
            "/content/DEEPVISION/.git/objects/20\n",
            "/content/DEEPVISION/.git/objects/21\n",
            "/content/DEEPVISION/.git/objects/22\n",
            "/content/DEEPVISION/.git/objects/23\n",
            "/content/DEEPVISION/.git/objects/24\n",
            "/content/DEEPVISION/.git/objects/25\n",
            "/content/DEEPVISION/.git/objects/26\n",
            "/content/DEEPVISION/.git/objects/27\n",
            "/content/DEEPVISION/.git/objects/28\n",
            "/content/DEEPVISION/.git/objects/29\n",
            "/content/DEEPVISION/.git/objects/2a\n",
            "/content/DEEPVISION/.git/objects/2b\n",
            "/content/DEEPVISION/.git/objects/2c\n",
            "/content/DEEPVISION/.git/objects/2d\n",
            "/content/DEEPVISION/.git/objects/2e\n",
            "/content/DEEPVISION/.git/objects/2f\n",
            "/content/DEEPVISION/.git/objects/30\n",
            "/content/DEEPVISION/.git/objects/31\n",
            "/content/DEEPVISION/.git/objects/32\n",
            "/content/DEEPVISION/.git/objects/33\n",
            "/content/DEEPVISION/.git/objects/34\n",
            "/content/DEEPVISION/.git/objects/35\n",
            "/content/DEEPVISION/.git/objects/36\n",
            "/content/DEEPVISION/.git/objects/37\n",
            "/content/DEEPVISION/.git/objects/38\n",
            "/content/DEEPVISION/.git/objects/39\n",
            "/content/DEEPVISION/.git/objects/3a\n",
            "/content/DEEPVISION/.git/objects/3b\n",
            "/content/DEEPVISION/.git/objects/3c\n",
            "/content/DEEPVISION/.git/objects/3d\n",
            "/content/DEEPVISION/.git/objects/3e\n",
            "/content/DEEPVISION/.git/objects/3f\n",
            "/content/DEEPVISION/.git/objects/40\n",
            "/content/DEEPVISION/.git/objects/41\n",
            "/content/DEEPVISION/.git/objects/42\n",
            "/content/DEEPVISION/.git/objects/43\n",
            "/content/DEEPVISION/.git/objects/44\n",
            "/content/DEEPVISION/.git/objects/45\n",
            "/content/DEEPVISION/.git/objects/46\n",
            "/content/DEEPVISION/.git/objects/47\n",
            "/content/DEEPVISION/.git/objects/48\n",
            "/content/DEEPVISION/.git/objects/49\n",
            "/content/DEEPVISION/.git/objects/4a\n",
            "/content/DEEPVISION/.git/objects/4b\n",
            "/content/DEEPVISION/.git/objects/4c\n",
            "/content/DEEPVISION/.git/objects/4d\n",
            "/content/DEEPVISION/.git/objects/4e\n",
            "/content/DEEPVISION/.git/objects/4f\n",
            "/content/DEEPVISION/.git/objects/50\n",
            "/content/DEEPVISION/.git/objects/51\n",
            "/content/DEEPVISION/.git/objects/52\n",
            "/content/DEEPVISION/.git/objects/53\n",
            "/content/DEEPVISION/.git/objects/54\n",
            "/content/DEEPVISION/.git/objects/55\n",
            "/content/DEEPVISION/.git/objects/56\n",
            "/content/DEEPVISION/.git/objects/57\n",
            "/content/DEEPVISION/.git/objects/58\n",
            "/content/DEEPVISION/.git/objects/59\n",
            "/content/DEEPVISION/.git/objects/5a\n",
            "/content/DEEPVISION/.git/objects/5b\n",
            "/content/DEEPVISION/.git/objects/5c\n",
            "/content/DEEPVISION/.git/objects/5d\n",
            "/content/DEEPVISION/.git/objects/5e\n",
            "/content/DEEPVISION/.git/objects/5f\n",
            "/content/DEEPVISION/.git/objects/60\n",
            "/content/DEEPVISION/.git/objects/61\n",
            "/content/DEEPVISION/.git/objects/62\n",
            "/content/DEEPVISION/.git/objects/63\n",
            "/content/DEEPVISION/.git/objects/64\n",
            "/content/DEEPVISION/.git/objects/65\n",
            "/content/DEEPVISION/.git/objects/66\n",
            "/content/DEEPVISION/.git/objects/67\n",
            "/content/DEEPVISION/.git/objects/68\n",
            "/content/DEEPVISION/.git/objects/69\n",
            "/content/DEEPVISION/.git/objects/6a\n",
            "/content/DEEPVISION/.git/objects/6b\n",
            "/content/DEEPVISION/.git/objects/6c\n",
            "/content/DEEPVISION/.git/objects/6d\n",
            "/content/DEEPVISION/.git/objects/6e\n",
            "/content/DEEPVISION/.git/objects/6f\n",
            "/content/DEEPVISION/.git/objects/70\n",
            "/content/DEEPVISION/.git/objects/71\n",
            "/content/DEEPVISION/.git/objects/72\n",
            "/content/DEEPVISION/.git/objects/73\n",
            "/content/DEEPVISION/.git/objects/74\n",
            "/content/DEEPVISION/.git/objects/75\n",
            "/content/DEEPVISION/.git/objects/76\n",
            "/content/DEEPVISION/.git/objects/77\n",
            "/content/DEEPVISION/.git/objects/78\n",
            "/content/DEEPVISION/.git/objects/79\n",
            "/content/DEEPVISION/.git/objects/7a\n",
            "/content/DEEPVISION/.git/objects/7b\n",
            "/content/DEEPVISION/.git/objects/7c\n",
            "/content/DEEPVISION/.git/objects/7d\n",
            "/content/DEEPVISION/.git/objects/7e\n",
            "/content/DEEPVISION/.git/objects/7f\n",
            "/content/DEEPVISION/.git/objects/80\n",
            "/content/DEEPVISION/.git/objects/81\n",
            "/content/DEEPVISION/.git/objects/82\n",
            "/content/DEEPVISION/.git/objects/83\n",
            "/content/DEEPVISION/.git/objects/84\n",
            "/content/DEEPVISION/.git/objects/85\n",
            "/content/DEEPVISION/.git/objects/86\n",
            "/content/DEEPVISION/.git/objects/87\n",
            "/content/DEEPVISION/.git/objects/88\n",
            "/content/DEEPVISION/.git/objects/89\n",
            "/content/DEEPVISION/.git/objects/8a\n",
            "/content/DEEPVISION/.git/objects/8b\n",
            "/content/DEEPVISION/.git/objects/8c\n",
            "/content/DEEPVISION/.git/objects/8d\n",
            "/content/DEEPVISION/.git/objects/8e\n",
            "/content/DEEPVISION/.git/objects/8f\n",
            "/content/DEEPVISION/.git/objects/90\n",
            "/content/DEEPVISION/.git/objects/91\n",
            "/content/DEEPVISION/.git/objects/92\n",
            "/content/DEEPVISION/.git/objects/93\n",
            "/content/DEEPVISION/.git/objects/94\n",
            "/content/DEEPVISION/.git/objects/95\n",
            "/content/DEEPVISION/.git/objects/96\n",
            "/content/DEEPVISION/.git/objects/97\n",
            "/content/DEEPVISION/.git/objects/98\n",
            "/content/DEEPVISION/.git/objects/99\n",
            "/content/DEEPVISION/.git/objects/9a\n",
            "/content/DEEPVISION/.git/objects/9b\n",
            "/content/DEEPVISION/.git/objects/9c\n",
            "/content/DEEPVISION/.git/objects/9d\n",
            "/content/DEEPVISION/.git/objects/9e\n",
            "/content/DEEPVISION/.git/objects/9f\n",
            "/content/DEEPVISION/.git/objects/a0\n",
            "/content/DEEPVISION/.git/objects/a1\n",
            "/content/DEEPVISION/.git/objects/a2\n",
            "/content/DEEPVISION/.git/objects/a3\n",
            "/content/DEEPVISION/.git/objects/a4\n",
            "/content/DEEPVISION/.git/objects/a5\n",
            "/content/DEEPVISION/.git/objects/a6\n",
            "/content/DEEPVISION/.git/objects/a7\n",
            "/content/DEEPVISION/.git/objects/a8\n",
            "/content/DEEPVISION/.git/objects/a9\n",
            "/content/DEEPVISION/.git/objects/aa\n",
            "/content/DEEPVISION/.git/objects/ab\n",
            "/content/DEEPVISION/.git/objects/ac\n",
            "/content/DEEPVISION/.git/objects/ad\n",
            "/content/DEEPVISION/.git/objects/ae\n",
            "/content/DEEPVISION/.git/objects/af\n",
            "/content/DEEPVISION/.git/objects/b0\n",
            "/content/DEEPVISION/.git/objects/b1\n",
            "/content/DEEPVISION/.git/objects/b2\n",
            "/content/DEEPVISION/.git/objects/b3\n",
            "/content/DEEPVISION/.git/objects/b4\n",
            "/content/DEEPVISION/.git/objects/b5\n",
            "/content/DEEPVISION/.git/objects/b6\n",
            "/content/DEEPVISION/.git/objects/b7\n",
            "/content/DEEPVISION/.git/objects/b8\n",
            "/content/DEEPVISION/.git/objects/b9\n",
            "/content/DEEPVISION/.git/objects/ba\n",
            "/content/DEEPVISION/.git/objects/bb\n",
            "/content/DEEPVISION/.git/objects/bc\n",
            "/content/DEEPVISION/.git/objects/bd\n",
            "/content/DEEPVISION/.git/objects/be\n",
            "/content/DEEPVISION/.git/objects/bf\n",
            "/content/DEEPVISION/.git/objects/c0\n",
            "/content/DEEPVISION/.git/objects/c1\n",
            "/content/DEEPVISION/.git/objects/c2\n",
            "/content/DEEPVISION/.git/objects/c3\n",
            "/content/DEEPVISION/.git/objects/c4\n",
            "/content/DEEPVISION/.git/objects/c5\n",
            "/content/DEEPVISION/.git/objects/c6\n",
            "/content/DEEPVISION/.git/objects/c7\n",
            "/content/DEEPVISION/.git/objects/c8\n",
            "/content/DEEPVISION/.git/objects/c9\n",
            "/content/DEEPVISION/.git/objects/ca\n",
            "/content/DEEPVISION/.git/objects/cb\n",
            "/content/DEEPVISION/.git/objects/cc\n",
            "/content/DEEPVISION/.git/objects/cd\n",
            "/content/DEEPVISION/.git/objects/ce\n",
            "/content/DEEPVISION/.git/objects/cf\n",
            "/content/DEEPVISION/.git/objects/d0\n",
            "/content/DEEPVISION/.git/objects/d1\n",
            "/content/DEEPVISION/.git/objects/d2\n",
            "/content/DEEPVISION/.git/objects/d3\n",
            "/content/DEEPVISION/.git/objects/d4\n",
            "/content/DEEPVISION/.git/objects/d5\n",
            "/content/DEEPVISION/.git/objects/d6\n",
            "/content/DEEPVISION/.git/objects/d7\n",
            "/content/DEEPVISION/.git/objects/d8\n",
            "/content/DEEPVISION/.git/objects/d9\n",
            "/content/DEEPVISION/.git/objects/da\n",
            "/content/DEEPVISION/.git/objects/db\n",
            "/content/DEEPVISION/.git/objects/dc\n",
            "/content/DEEPVISION/.git/objects/dd\n",
            "/content/DEEPVISION/.git/objects/de\n",
            "/content/DEEPVISION/.git/objects/df\n",
            "/content/DEEPVISION/.git/objects/e0\n",
            "/content/DEEPVISION/.git/objects/e1\n",
            "/content/DEEPVISION/.git/objects/e2\n",
            "/content/DEEPVISION/.git/objects/e3\n",
            "/content/DEEPVISION/.git/objects/e4\n",
            "/content/DEEPVISION/.git/objects/e5\n",
            "/content/DEEPVISION/.git/objects/e6\n",
            "/content/DEEPVISION/.git/objects/e7\n",
            "/content/DEEPVISION/.git/objects/e8\n",
            "/content/DEEPVISION/.git/objects/e9\n",
            "/content/DEEPVISION/.git/objects/ea\n",
            "/content/DEEPVISION/.git/objects/eb\n",
            "/content/DEEPVISION/.git/objects/ec\n",
            "/content/DEEPVISION/.git/objects/ed\n",
            "/content/DEEPVISION/.git/objects/ee\n",
            "/content/DEEPVISION/.git/objects/ef\n",
            "/content/DEEPVISION/.git/objects/f0\n",
            "/content/DEEPVISION/.git/objects/f1\n",
            "/content/DEEPVISION/.git/objects/f2\n",
            "/content/DEEPVISION/.git/objects/f3\n",
            "/content/DEEPVISION/.git/objects/f4\n",
            "/content/DEEPVISION/.git/objects/f5\n",
            "/content/DEEPVISION/.git/objects/f6\n",
            "/content/DEEPVISION/.git/objects/f7\n",
            "/content/DEEPVISION/.git/objects/f8\n",
            "/content/DEEPVISION/.git/objects/f9\n",
            "/content/DEEPVISION/.git/objects/fa\n",
            "/content/DEEPVISION/.git/objects/fb\n",
            "/content/DEEPVISION/.git/objects/fc\n",
            "/content/DEEPVISION/.git/objects/fd\n",
            "/content/DEEPVISION/.git/objects/fe\n",
            "/content/DEEPVISION/.git/objects/ff\n",
            "/content/DEEPVISION/.git/objects/info\n",
            "/content/DEEPVISION/.git/objects/pack\n",
            "/content/DEEPVISION/.git/refs\n",
            "/content/DEEPVISION/.git/refs/heads\n",
            "/content/DEEPVISION/.git/refs/remotes\n",
            "/content/DEEPVISION/.git/refs/remotes/origin\n",
            "/content/DEEPVISION/.git/refs/tags\n",
            "/content/DEEPVISION/.ipynb_checkpoints\n",
            "/content/DEEPVISION/processed\n",
            "/content/DEEPVISION/training\n",
            "/content/DEEPVISION/training/checkpoints\n",
            "/content/DEEPVISION/training/.ipynb_checkpoints\n",
            "/content/DEEPVISION/venv\n",
            "/content/DEEPVISION/venv/Include\n",
            "/content/DEEPVISION/venv/Lib\n",
            "/content/DEEPVISION/venv/Lib/site-packages\n",
            "/content/DEEPVISION/venv/Lib/site-packages/contourpy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/contourpy-1.3.3.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/contourpy/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/contourpy/util\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/aruco\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/barcode\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/cuda\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/data\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/detail\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/dnn\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/Error\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/fisheye\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/flann\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/gapi\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/ipp\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/mat_wrapper\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/misc\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/ml\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/ocl\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/ogl\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/parallel\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/samples\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/segmentation\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/typing\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/utils\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cv2/videoio_registry\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cycler\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cycler-0.12.1.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/cycler/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/dateutil\n",
            "/content/DEEPVISION/venv/Lib/site-packages/dateutil/parser\n",
            "/content/DEEPVISION/venv/Lib/site-packages/dateutil/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/dateutil/tz\n",
            "/content/DEEPVISION/venv/Lib/site-packages/dateutil/zoneinfo\n",
            "/content/DEEPVISION/venv/Lib/site-packages/_distutils_hack\n",
            "/content/DEEPVISION/venv/Lib/site-packages/_distutils_hack/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/filelock\n",
            "/content/DEEPVISION/venv/Lib/site-packages/filelock-3.19.1.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/filelock-3.19.1.dist-info/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/filelock/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fonttools-4.60.1.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fonttools-4.60.1.dist-info/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/cffLib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/colorLib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/config\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/cu2qu\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/designspaceLib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/encodings\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/feaLib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/merge\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/misc\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/mtiLib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/otlLib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/pens\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/qu2cu\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/subset\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/svgLib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/t1Lib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/ttLib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/ufoLib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/unicodedata\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/varLib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fontTools/voltLib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fsspec\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fsspec-2025.9.0.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fsspec-2025.9.0.dist-info/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fsspec/implementations\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fsspec/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/fsspec/tests\n",
            "/content/DEEPVISION/venv/Lib/site-packages/functorch\n",
            "/content/DEEPVISION/venv/Lib/site-packages/functorch/compile\n",
            "/content/DEEPVISION/venv/Lib/site-packages/functorch/dim\n",
            "/content/DEEPVISION/venv/Lib/site-packages/functorch/einops\n",
            "/content/DEEPVISION/venv/Lib/site-packages/functorch/experimental\n",
            "/content/DEEPVISION/venv/Lib/site-packages/functorch/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/functorch/_src\n",
            "/content/DEEPVISION/venv/Lib/site-packages/jinja2\n",
            "/content/DEEPVISION/venv/Lib/site-packages/jinja2-3.1.6.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/jinja2-3.1.6.dist-info/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/jinja2/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib-1.5.2.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib-1.5.2.dist-info/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/externals\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/test\n",
            "/content/DEEPVISION/venv/Lib/site-packages/kiwisolver\n",
            "/content/DEEPVISION/venv/Lib/site-packages/kiwisolver-1.4.9.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/kiwisolver-1.4.9.dist-info/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/kiwisolver/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/markupsafe\n",
            "/content/DEEPVISION/venv/Lib/site-packages/MarkupSafe-2.1.5.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/markupsafe/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/matplotlib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/matplotlib-3.10.7.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/matplotlib/_api\n",
            "/content/DEEPVISION/venv/Lib/site-packages/matplotlib/axes\n",
            "/content/DEEPVISION/venv/Lib/site-packages/matplotlib/backends\n",
            "/content/DEEPVISION/venv/Lib/site-packages/matplotlib/mpl-data\n",
            "/content/DEEPVISION/venv/Lib/site-packages/matplotlib/projections\n",
            "/content/DEEPVISION/venv/Lib/site-packages/matplotlib/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/matplotlib/sphinxext\n",
            "/content/DEEPVISION/venv/Lib/site-packages/matplotlib/style\n",
            "/content/DEEPVISION/venv/Lib/site-packages/matplotlib/testing\n",
            "/content/DEEPVISION/venv/Lib/site-packages/matplotlib/tests\n",
            "/content/DEEPVISION/venv/Lib/site-packages/matplotlib/tri\n",
            "/content/DEEPVISION/venv/Lib/site-packages/mpl_toolkits\n",
            "/content/DEEPVISION/venv/Lib/site-packages/mpl_toolkits/axes_grid1\n",
            "/content/DEEPVISION/venv/Lib/site-packages/mpl_toolkits/axisartist\n",
            "/content/DEEPVISION/venv/Lib/site-packages/mpl_toolkits/mplot3d\n",
            "/content/DEEPVISION/venv/Lib/site-packages/mpmath\n",
            "/content/DEEPVISION/venv/Lib/site-packages/mpmath-1.3.0.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/mpmath/calculus\n",
            "/content/DEEPVISION/venv/Lib/site-packages/mpmath/functions\n",
            "/content/DEEPVISION/venv/Lib/site-packages/mpmath/libmp\n",
            "/content/DEEPVISION/venv/Lib/site-packages/mpmath/matrices\n",
            "/content/DEEPVISION/venv/Lib/site-packages/mpmath/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/mpmath/tests\n",
            "/content/DEEPVISION/venv/Lib/site-packages/networkx\n",
            "/content/DEEPVISION/venv/Lib/site-packages/networkx-3.5.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/networkx-3.5.dist-info/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/networkx/algorithms\n",
            "/content/DEEPVISION/venv/Lib/site-packages/networkx/classes\n",
            "/content/DEEPVISION/venv/Lib/site-packages/networkx/drawing\n",
            "/content/DEEPVISION/venv/Lib/site-packages/networkx/generators\n",
            "/content/DEEPVISION/venv/Lib/site-packages/networkx/linalg\n",
            "/content/DEEPVISION/venv/Lib/site-packages/networkx/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/networkx/readwrite\n",
            "/content/DEEPVISION/venv/Lib/site-packages/networkx/tests\n",
            "/content/DEEPVISION/venv/Lib/site-packages/networkx/utils\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy-2.2.6.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/char\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/compat\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/_core\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/core\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/doc\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/f2py\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/fft\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/lib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy.libs\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/linalg\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/ma\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/matrixlib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/polynomial\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/_pyinstaller\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/random\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/rec\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/strings\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/testing\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/tests\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/_typing\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/typing\n",
            "/content/DEEPVISION/venv/Lib/site-packages/numpy/_utils\n",
            "/content/DEEPVISION/venv/Lib/site-packages/opencv_python-4.12.0.88.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/packaging\n",
            "/content/DEEPVISION/venv/Lib/site-packages/packaging-25.0.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/packaging-25.0.dist-info/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/packaging/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/packaging/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas-2.3.3.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas/api\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas/arrays\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas/compat\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas/_config\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas/core\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas/errors\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas/io\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas.libs\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas/_libs\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas/plotting\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas/_testing\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas/tests\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas/tseries\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pandas/util\n",
            "/content/DEEPVISION/venv/Lib/site-packages/PIL\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pillow-12.0.0.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pillow-12.0.0.dist-info/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/PIL/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pip\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pip-24.3.1.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pip/_internal\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pip/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pip/_vendor\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pkg_resources\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pkg_resources/extern\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pkg_resources/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pkg_resources/_vendor\n",
            "/content/DEEPVISION/venv/Lib/site-packages/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pyparsing\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pyparsing-3.2.5.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pyparsing-3.2.5.dist-info/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pyparsing/diagram\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pyparsing/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pyparsing/tools\n",
            "/content/DEEPVISION/venv/Lib/site-packages/python_dateutil-2.9.0.post0.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pytz\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pytz-2025.2.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pytz/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/pytz/zoneinfo\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scikit_learn-1.7.2.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scikit_learn-1.7.2.dist-info/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy-1.16.3.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/cluster\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/constants\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/datasets\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/differentiate\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/fft\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/fftpack\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/integrate\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/interpolate\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/io\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/_lib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy.libs\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/linalg\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/misc\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/ndimage\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/odr\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/optimize\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/signal\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/sparse\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/spatial\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/special\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/stats\n",
            "/content/DEEPVISION/venv/Lib/site-packages/setuptools\n",
            "/content/DEEPVISION/venv/Lib/site-packages/setuptools-70.2.0.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/setuptools/command\n",
            "/content/DEEPVISION/venv/Lib/site-packages/setuptools/compat\n",
            "/content/DEEPVISION/venv/Lib/site-packages/setuptools/config\n",
            "/content/DEEPVISION/venv/Lib/site-packages/setuptools/_distutils\n",
            "/content/DEEPVISION/venv/Lib/site-packages/setuptools/extern\n",
            "/content/DEEPVISION/venv/Lib/site-packages/setuptools/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/setuptools/_vendor\n",
            "/content/DEEPVISION/venv/Lib/site-packages/six-1.17.0.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/_build_utils\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/__check_build\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/cluster\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/compose\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/covariance\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/cross_decomposition\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/datasets\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/decomposition\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/ensemble\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/experimental\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/externals\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/feature_extraction\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/feature_selection\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/frozen\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/gaussian_process\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/impute\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/inspection\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/.libs\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/linear_model\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/_loss\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/manifold\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/metrics\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/mixture\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/model_selection\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/neighbors\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/neural_network\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/preprocessing\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/semi_supervised\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/svm\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/tests\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/tree\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/utils\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy-1.14.0.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy-1.14.0.dist-info/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/algebras\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/assumptions\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/benchmarks\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/calculus\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/categories\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/codegen\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/combinatorics\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/concrete\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/core\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/crypto\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/diffgeom\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/discrete\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/external\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/functions\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/geometry\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/holonomic\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/integrals\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/interactive\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/liealgebras\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/logic\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/matrices\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/multipledispatch\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/ntheory\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/parsing\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/physics\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/plotting\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/polys\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/printing\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/sandbox\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/series\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/sets\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/simplify\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/solvers\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/stats\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/strategies\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/tensor\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/testing\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/unify\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/utilities\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sympy/vector\n",
            "/content/DEEPVISION/venv/Lib/site-packages/threadpoolctl-3.6.0.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/threadpoolctl-3.6.0.dist-info/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch-2.7.1+cu118.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/accelerator\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/amp\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/ao\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio-2.7.1+cu118.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio-2.7.1+cu118.dist-info/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio/_backend\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio/backend\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio/compliance\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio/datasets\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio/_extension\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio/functional\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio/_internal\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio/io\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio/lib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio/models\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio/pipelines\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio/prototype\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio/sox_effects\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio/transforms\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchaudio/utils\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/autograd\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_awaits\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/backends\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/bin\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_C\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/compiler\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/contrib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/cpu\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/cuda\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_custom_op\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_decomp\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_dispatch\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/distributed\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/distributions\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_dynamo\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_export\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/export\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/fft\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/func\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_functorch\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/futures\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/fx\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchgen\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchgen/aoti\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchgen/api\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchgen/dest\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchgen/executorch\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchgen/operator_versions\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchgen/packaged\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchgen/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchgen/selective_build\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchgen/static_runtime\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_higher_order_ops\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/include\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_inductor\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/jit\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_lazy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/lib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_library\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/linalg\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_logging\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/masked\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/monitor\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/mps\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/mtia\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/multiprocessing\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/nested\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/nn\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_numpy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/onnx\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/optim\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/package\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_prims\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_prims_common\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/profiler\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/quantization\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_refs\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/share\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/signal\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/sparse\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/special\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_strobelight\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_subclasses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/testing\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/utils\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/_vendor\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchvision\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchvision-0.22.1+cu118.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchvision/datasets\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchvision/io\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchvision/models\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchvision/ops\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchvision/prototype\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchvision/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchvision/transforms\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torchvision/tv_tensors\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torch/xpu\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torio\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torio/_extension\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torio/io\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torio/lib\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torio/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/torio/utils\n",
            "/content/DEEPVISION/venv/Lib/site-packages/typing_extensions-4.15.0.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/typing_extensions-4.15.0.dist-info/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/tzdata\n",
            "/content/DEEPVISION/venv/Lib/site-packages/tzdata-2025.2.dist-info\n",
            "/content/DEEPVISION/venv/Lib/site-packages/tzdata-2025.2.dist-info/licenses\n",
            "/content/DEEPVISION/venv/Lib/site-packages/tzdata/__pycache__\n",
            "/content/DEEPVISION/venv/Lib/site-packages/tzdata/zoneinfo\n",
            "/content/DEEPVISION/venv/Scripts\n",
            "/content/DEEPVISION/venv/share\n",
            "/content/DEEPVISION/venv/share/man\n",
            "/content/DEEPVISION/venv/share/man/man1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"---- IMAGES ----\"\n",
        "!find /content/DEEPVISION -type f -iname \"*.jpg\" | head -20\n",
        "\n",
        "!echo \"---- NPY ----\"\n",
        "!find /content/DEEPVISION -type f -iname \"*.npy\" | head -20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq2_H1b9eYDi",
        "outputId": "b1cfbb52-f6f0-4e8e-f5d1-2d68a857e895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- IMAGES ----\n",
            "/content/DEEPVISION/venv/Lib/site-packages/matplotlib/mpl-data/sample_data/grace_hopper.jpg\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/datasets/images/china.jpg\n",
            "/content/DEEPVISION/venv/Lib/site-packages/sklearn/datasets/images/flower.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_2.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_207.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_17.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_93.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_41.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_114.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_267.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_32.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_199.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_264.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_231.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_285.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_156.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_165.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_49.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_112.jpg\n",
            "/content/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images/IMG_296.jpg\n",
            "---- NPY ----\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/interpolate/tests/data/estimate_gradients_hang.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/stats/tests/data/rel_breitwigner_pdf_sample_data_ROOT.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/stats/tests/data/levy_stable/stable-loc-scale-sample-data.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/stats/tests/data/levy_stable/stable-Z1-cdf-sample-data.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/stats/tests/data/levy_stable/stable-Z1-pdf-sample-data.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/scipy/stats/tests/data/jf_skew_t_gamlss_pdf_data.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/test/data/joblib_0.9.2_pickle_py34_np19.pkl_02.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/test/data/joblib_0.9.2_pickle_py35_np19.pkl_04.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/test/data/joblib_0.9.2_pickle_py33_np18.pkl_02.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/test/data/joblib_0.9.2_pickle_py33_np18.pkl_03.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/test/data/joblib_0.9.2_pickle_py34_np19.pkl_04.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/test/data/joblib_0.9.2_pickle_py27_np16.pkl_01.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/test/data/joblib_0.9.2_pickle_py27_np16.pkl_04.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/test/data/joblib_0.9.2_pickle_py27_np16.pkl_03.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/test/data/joblib_0.9.2_pickle_py33_np18.pkl_01.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/test/data/joblib_0.9.2_pickle_py34_np19.pkl_03.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/test/data/joblib_0.9.2_pickle_py27_np17.pkl_02.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/test/data/joblib_0.9.2_pickle_py35_np19.pkl_01.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/test/data/joblib_0.9.2_pickle_py27_np17.pkl_04.npy\n",
            "/content/DEEPVISION/venv/Lib/site-packages/joblib/test/data/joblib_0.9.2_pickle_py27_np17.pkl_03.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths for your dataset\n",
        "IMG_DIR = \"/content/DEEPVISION/DATA/ShanghaiTech/part_A/train_data/images\"\n",
        "DEN_DIR = \"/content/DEEPVISION/DATA/processed/part_A/density\"\n",
        "\n",
        "# Check both exist and count files\n",
        "import os\n",
        "\n",
        "print(\"Images folder exists:\", os.path.exists(IMG_DIR))\n",
        "print(\"Density folder exists:\", os.path.exists(DEN_DIR))\n",
        "\n",
        "print(\"Images count:\", len([f for f in os.listdir(IMG_DIR) if f.endswith(\".jpg\")]))\n",
        "print(\"Density maps count:\", len([f for f in os.listdir(DEN_DIR) if f.endswith(\".npy\")]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKoVZWDigG8O",
        "outputId": "53cf6463-468b-44df-bb7f-050d37995a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images folder exists: True\n",
            "Density folder exists: True\n",
            "Images count: 300\n",
            "Density maps count: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/DEEPVISION -type d -iname \"*density*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_365XXjygvt9",
        "outputId": "c11ad1e3-8fb2-47a3-d5d3-128d1012eacb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DEEPVISION/DATA/processed/part_A/density\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wt7R8UQhOsi",
        "outputId": "488e4802-d972-46af-9b79-060b5b13719d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CrowdDataset(Dataset):\n",
        "    def __init__(self, img_dir, den_dir):\n",
        "        self.img_dir = img_dir\n",
        "        self.den_dir = den_dir\n",
        "\n",
        "        self.images = sorted([f for f in os.listdir(img_dir) if f.endswith(\".jpg\")])\n",
        "        self.dens   = sorted([f for f in os.listdir(den_dir) if f.endswith(\".npy\")])\n",
        "\n",
        "        assert len(self.images) == len(self.dens), \"ImageDensity count mismatch!\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
        "        den_path = os.path.join(self.den_dir, self.dens[idx])\n",
        "\n",
        "        # Load image  512x512\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (512, 512))\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        img = torch.tensor(img).permute(2, 0, 1)\n",
        "\n",
        "        # Load density  MUST BE 32x32\n",
        "        den = np.load(den_path).astype(np.float32)\n",
        "        den = cv2.resize(den, (32, 32))\n",
        "        den = torch.tensor(den).unsqueeze(0)\n",
        "\n",
        "        return img, den"
      ],
      "metadata": {
        "id": "mXqtYGYthluH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "ds = CrowdDataset(IMG_DIR, DEN_DIR)\n",
        "train_loader = DataLoader(ds, batch_size=2, shuffle=True)\n",
        "\n",
        "print(\"Total training samples:\", len(ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z4Jr5I1hq6P",
        "outputId": "9d25a60e-3e6a-4a48-f010-42f5ad867257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training samples: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torchvision.models import vgg16\n",
        "\n",
        "class CrowdCounter(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = vgg16(weights=\"DEFAULT\")\n",
        "        self.features = nn.Sequential(*list(vgg.features.children())[:30])\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 1, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.regressor(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "SsRuHDUMhq2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = CrowdCounter().to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for imgs, dens in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        dens = dens.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(imgs)\n",
        "        loss = criterion(preds, dens)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS}  Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"model_epoch_50.pth\")\n",
        "print(\"Saved model_epoch_50.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zl3AJ6ah8Hp",
        "outputId": "dbb808c3-4122-4a01-ae81-7aaebf621d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50  Loss: 0.0168\n",
            "Epoch 2/50  Loss: 0.0028\n",
            "Epoch 3/50  Loss: 0.0019\n",
            "Epoch 4/50  Loss: 0.0017\n",
            "Epoch 5/50  Loss: 0.0015\n",
            "Epoch 6/50  Loss: 0.0014\n",
            "Epoch 7/50  Loss: 0.0013\n",
            "Epoch 8/50  Loss: 0.0012\n",
            "Epoch 9/50  Loss: 0.0012\n",
            "Epoch 10/50  Loss: 0.0011\n",
            "Epoch 11/50  Loss: 0.0011\n",
            "Epoch 12/50  Loss: 0.0011\n",
            "Epoch 13/50  Loss: 0.0010\n",
            "Epoch 14/50  Loss: 0.0010\n",
            "Epoch 15/50  Loss: 0.0010\n",
            "Epoch 16/50  Loss: 0.0010\n",
            "Epoch 17/50  Loss: 0.0010\n",
            "Epoch 18/50  Loss: 0.0009\n",
            "Epoch 19/50  Loss: 0.0009\n",
            "Epoch 20/50  Loss: 0.0009\n",
            "Epoch 21/50  Loss: 0.0009\n",
            "Epoch 22/50  Loss: 0.0009\n",
            "Epoch 23/50  Loss: 0.0009\n",
            "Epoch 24/50  Loss: 0.0009\n",
            "Epoch 25/50  Loss: 0.0009\n",
            "Epoch 26/50  Loss: 0.0009\n",
            "Epoch 27/50  Loss: 0.0008\n",
            "Epoch 28/50  Loss: 0.0008\n",
            "Epoch 29/50  Loss: 0.0009\n",
            "Epoch 30/50  Loss: 0.0008\n",
            "Epoch 31/50  Loss: 0.0008\n",
            "Epoch 32/50  Loss: 0.0008\n",
            "Epoch 33/50  Loss: 0.0008\n",
            "Epoch 34/50  Loss: 0.0008\n",
            "Epoch 35/50  Loss: 0.0008\n",
            "Epoch 36/50  Loss: 0.0008\n",
            "Epoch 37/50  Loss: 0.0008\n",
            "Epoch 38/50  Loss: 0.0008\n",
            "Epoch 39/50  Loss: 0.0008\n",
            "Epoch 40/50  Loss: 0.0008\n",
            "Epoch 41/50  Loss: 0.0008\n",
            "Epoch 42/50  Loss: 0.0008\n",
            "Epoch 43/50  Loss: 0.0008\n",
            "Epoch 44/50  Loss: 0.0008\n",
            "Epoch 45/50  Loss: 0.0008\n",
            "Epoch 46/50  Loss: 0.0007\n",
            "Epoch 47/50  Loss: 0.0007\n",
            "Epoch 48/50  Loss: 0.0007\n",
            "Epoch 49/50  Loss: 0.0007\n",
            "Epoch 50/50  Loss: 0.0007\n",
            "Saved model_epoch_50.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm dataset & any checkpoint\n",
        "import os\n",
        "IMG_DIR = \"/content/DEEPVISION/DATA/ShanghaiTech/part_A/train_data/images\"\n",
        "DEN_DIR = \"/content/DEEPVISION/DATA/processed/part_A/density\"\n",
        "print(\"Images exist:\", os.path.exists(IMG_DIR), \"count:\", len(os.listdir(IMG_DIR)) if os.path.exists(IMG_DIR) else 0)\n",
        "print(\"Density exist:\", os.path.exists(DEN_DIR), \"count:\", len(os.listdir(DEN_DIR)) if os.path.exists(DEN_DIR) else 0)\n",
        "print(\"Any checkpoint files here?\")\n",
        "for f in os.listdir(\"/content\"):\n",
        "    if \"model_epoch\" in f or f.endswith(\".pth\"):\n",
        "        print(\" -\", f)\n",
        "# also check drive output if you saved there:\n",
        "DR = \"/content/drive/MyDrive\"\n",
        "if os.path.exists(DR):\n",
        "    for f in os.listdir(DR):\n",
        "        if f.endswith(\".pth\") or \"model_epoch\" in f:\n",
        "            print(\"Drive:\",f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-BX81jBoaqc",
        "outputId": "dcf1d1c5-04fe-410d-9af4-e826fbc88471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images exist: True count: 300\n",
            "Density exist: True count: 300\n",
            "Any checkpoint files here?\n",
            "Drive: model_epoch_50.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- TRAIN FROM SCRATCH FOR 100 EPOCHS ----------\n",
        "import os, cv2, numpy as np, torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import vgg16\n",
        "from tqdm import tqdm\n",
        "\n",
        "IMG_DIR = \"/content/DEEPVISION/DATA/ShanghaiTech/part_A/train_data/images\"\n",
        "DEN_DIR = \"/content/DEEPVISION/DATA/processed/part_A/density\"\n",
        "\n",
        "class CrowdDataset(Dataset):\n",
        "    def __init__(self, img_dir, den_dir):\n",
        "        self.img_dir, self.den_dir = img_dir, den_dir\n",
        "        self.images = sorted([f for f in os.listdir(img_dir) if f.endswith(\".jpg\")])\n",
        "        self.dens   = sorted([f for f in os.listdir(den_dir) if f.endswith(\".npy\")])\n",
        "        self.pairs = [(os.path.join(img_dir, os.path.splitext(f)[0]+'.jpg'),\n",
        "                       os.path.join(den_dir, os.path.splitext(f)[0]+'.npy'))\n",
        "                      for f in self.images if os.path.splitext(f)[0]+'.npy' in self.dens]\n",
        "    def __len__(self): return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        imgp, denp = self.pairs[idx]\n",
        "        img = cv2.cvtColor(cv2.imread(imgp), cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (512,512)).astype('float32')/255.0\n",
        "        img = torch.from_numpy(img).permute(2,0,1)\n",
        "        den = np.load(denp).astype('float32')\n",
        "        den = cv2.resize(den, (32,32))\n",
        "        den = torch.from_numpy(den).unsqueeze(0)\n",
        "        return img, den\n",
        "\n",
        "ds = CrowdDataset(IMG_DIR, DEN_DIR)\n",
        "loader = DataLoader(ds, batch_size=2, shuffle=True, num_workers=0)\n",
        "\n",
        "# simple CSRNet-like model\n",
        "class CrowdCounter(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = vgg16(weights=\"DEFAULT\")\n",
        "        self.backbone = nn.Sequential(*list(vgg.features.children())[:23])\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Conv2d(512,256,3,padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(256,128,3,padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(128,1,1)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.regressor(x)\n",
        "        return x\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = CrowdCounter().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "EPOCHS = 100\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    total=0.0\n",
        "    for imgs, dens in loader:\n",
        "        imgs = imgs.to(device); dens = dens.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(imgs)\n",
        "        if preds.shape[-2:] != dens.shape[-2:]:\n",
        "            dens = torch.nn.functional.interpolate(dens, size=preds.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        loss = criterion(preds, dens)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "    print(f\"Epoch {epoch}/{EPOCHS}  AvgLoss:{total/len(loader):.6f}\")\n",
        "    if epoch%10==0 or epoch==EPOCHS:\n",
        "        torch.save(model.state_dict(), f\"/content/model_epoch_{epoch}.pth\")\n",
        "        print(\"Saved checkpoint:\", f\"/content/model_epoch_{epoch}.pth\")\n",
        "\n",
        "# final save (redundant)\n",
        "torch.save(model.state_dict(), \"/content/model_epoch_100.pth\")\n",
        "print(\"Final model saved: /content/model_epoch_100.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za6hkd3GouMb",
        "outputId": "dbb36731-8466-4fb2-9bf9-3d35b2e87549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100  AvgLoss:0.000505\n",
            "Epoch 2/100  AvgLoss:0.000048\n",
            "Epoch 3/100  AvgLoss:0.000020\n",
            "Epoch 4/100  AvgLoss:0.000013\n",
            "Epoch 5/100  AvgLoss:0.000009\n",
            "Epoch 6/100  AvgLoss:0.000008\n",
            "Epoch 7/100  AvgLoss:0.000007\n",
            "Epoch 8/100  AvgLoss:0.000007\n",
            "Epoch 9/100  AvgLoss:0.000006\n",
            "Epoch 10/100  AvgLoss:0.000005\n",
            "Saved checkpoint: /content/model_epoch_10.pth\n",
            "Epoch 11/100  AvgLoss:0.000005\n",
            "Epoch 12/100  AvgLoss:0.000005\n",
            "Epoch 13/100  AvgLoss:0.000005\n",
            "Epoch 14/100  AvgLoss:0.000005\n",
            "Epoch 15/100  AvgLoss:0.000005\n",
            "Epoch 16/100  AvgLoss:0.000004\n",
            "Epoch 17/100  AvgLoss:0.000004\n",
            "Epoch 18/100  AvgLoss:0.000004\n",
            "Epoch 19/100  AvgLoss:0.000004\n",
            "Epoch 20/100  AvgLoss:0.000004\n",
            "Saved checkpoint: /content/model_epoch_20.pth\n",
            "Epoch 21/100  AvgLoss:0.000004\n",
            "Epoch 22/100  AvgLoss:0.000004\n",
            "Epoch 23/100  AvgLoss:0.000004\n",
            "Epoch 24/100  AvgLoss:0.000004\n",
            "Epoch 25/100  AvgLoss:0.000004\n",
            "Epoch 26/100  AvgLoss:0.000004\n",
            "Epoch 27/100  AvgLoss:0.000004\n",
            "Epoch 28/100  AvgLoss:0.000004\n",
            "Epoch 29/100  AvgLoss:0.000004\n",
            "Epoch 30/100  AvgLoss:0.000004\n",
            "Saved checkpoint: /content/model_epoch_30.pth\n",
            "Epoch 31/100  AvgLoss:0.000004\n",
            "Epoch 32/100  AvgLoss:0.000003\n",
            "Epoch 33/100  AvgLoss:0.000004\n",
            "Epoch 34/100  AvgLoss:0.000004\n",
            "Epoch 35/100  AvgLoss:0.000003\n",
            "Epoch 36/100  AvgLoss:0.000003\n",
            "Epoch 37/100  AvgLoss:0.000004\n",
            "Epoch 38/100  AvgLoss:0.000004\n",
            "Epoch 39/100  AvgLoss:0.000003\n",
            "Epoch 40/100  AvgLoss:0.000003\n",
            "Saved checkpoint: /content/model_epoch_40.pth\n",
            "Epoch 41/100  AvgLoss:0.000003\n",
            "Epoch 42/100  AvgLoss:0.000003\n",
            "Epoch 43/100  AvgLoss:0.000003\n",
            "Epoch 44/100  AvgLoss:0.000003\n",
            "Epoch 45/100  AvgLoss:0.000003\n",
            "Epoch 46/100  AvgLoss:0.000003\n",
            "Epoch 47/100  AvgLoss:0.000003\n",
            "Epoch 48/100  AvgLoss:0.000003\n",
            "Epoch 49/100  AvgLoss:0.000003\n",
            "Epoch 50/100  AvgLoss:0.000003\n",
            "Saved checkpoint: /content/model_epoch_50.pth\n",
            "Epoch 51/100  AvgLoss:0.000003\n",
            "Epoch 52/100  AvgLoss:0.000003\n",
            "Epoch 53/100  AvgLoss:0.000003\n",
            "Epoch 54/100  AvgLoss:0.000003\n",
            "Epoch 55/100  AvgLoss:0.000003\n",
            "Epoch 56/100  AvgLoss:0.000003\n",
            "Epoch 57/100  AvgLoss:0.000003\n",
            "Epoch 58/100  AvgLoss:0.000003\n",
            "Epoch 59/100  AvgLoss:0.000003\n",
            "Epoch 60/100  AvgLoss:0.000003\n",
            "Saved checkpoint: /content/model_epoch_60.pth\n",
            "Epoch 61/100  AvgLoss:0.000003\n",
            "Epoch 62/100  AvgLoss:0.000003\n",
            "Epoch 63/100  AvgLoss:0.000003\n",
            "Epoch 64/100  AvgLoss:0.000003\n",
            "Epoch 65/100  AvgLoss:0.000003\n",
            "Epoch 66/100  AvgLoss:0.000003\n",
            "Epoch 67/100  AvgLoss:0.000003\n",
            "Epoch 68/100  AvgLoss:0.000003\n",
            "Epoch 69/100  AvgLoss:0.000002\n",
            "Epoch 70/100  AvgLoss:0.000002\n",
            "Saved checkpoint: /content/model_epoch_70.pth\n",
            "Epoch 71/100  AvgLoss:0.000002\n",
            "Epoch 72/100  AvgLoss:0.000002\n",
            "Epoch 73/100  AvgLoss:0.000002\n",
            "Epoch 74/100  AvgLoss:0.000002\n",
            "Epoch 75/100  AvgLoss:0.000002\n",
            "Epoch 76/100  AvgLoss:0.000002\n",
            "Epoch 77/100  AvgLoss:0.000002\n",
            "Epoch 78/100  AvgLoss:0.000002\n",
            "Epoch 79/100  AvgLoss:0.000002\n",
            "Epoch 80/100  AvgLoss:0.000003\n",
            "Saved checkpoint: /content/model_epoch_80.pth\n",
            "Epoch 81/100  AvgLoss:0.000002\n",
            "Epoch 82/100  AvgLoss:0.000002\n",
            "Epoch 83/100  AvgLoss:0.000002\n",
            "Epoch 84/100  AvgLoss:0.000002\n",
            "Epoch 85/100  AvgLoss:0.000002\n",
            "Epoch 86/100  AvgLoss:0.000002\n",
            "Epoch 87/100  AvgLoss:0.000002\n",
            "Epoch 88/100  AvgLoss:0.000002\n",
            "Epoch 89/100  AvgLoss:0.000002\n",
            "Epoch 90/100  AvgLoss:0.000002\n",
            "Saved checkpoint: /content/model_epoch_90.pth\n",
            "Epoch 91/100  AvgLoss:0.000002\n",
            "Epoch 92/100  AvgLoss:0.000002\n",
            "Epoch 93/100  AvgLoss:0.000002\n",
            "Epoch 94/100  AvgLoss:0.000002\n",
            "Epoch 95/100  AvgLoss:0.000002\n",
            "Epoch 96/100  AvgLoss:0.000002\n",
            "Epoch 97/100  AvgLoss:0.000002\n",
            "Epoch 98/100  AvgLoss:0.000002\n",
            "Epoch 99/100  AvgLoss:0.000002\n",
            "Epoch 100/100  AvgLoss:0.000002\n",
            "Saved checkpoint: /content/model_epoch_100.pth\n",
            "Final model saved: /content/model_epoch_100.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"cuda:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpuKfNTyzUFB",
        "outputId": "fceaf408-73b7-4fd2-c709-a533d8074b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset size:\", len(ds))\n",
        "for imgs, dens in train_loader:\n",
        "    print(\"One batch OK:\", imgs.shape, dens.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUn1dtCezVQX",
        "outputId": "803ea9e8-74e7-4169-deeb-5326c5806d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 300\n",
            "One batch OK: torch.Size([2, 3, 512, 512]) torch.Size([2, 1, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = model(torch.randn(1,3,512,512).to(device))\n",
        "print(\"Model output shape:\", test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAsXNbLPzast",
        "outputId": "368e669a-e8c1-4616-bec4-dbc11c5f697e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output shape: torch.Size([1, 1, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = CrowdCounter().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "EPOCHS = 150\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for imgs, dens in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        dens = dens.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(imgs)\n",
        "        loss = criterion(preds, dens)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS}  Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"model_epoch_150.pth\")\n",
        "print(\"Saved model_epoch_150.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "rDY6qaJSzgQr",
        "outputId": "6d8217fb-e008-4ca1-dcce-be02c2beb757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:634: UserWarning: Using a target size (torch.Size([2, 1, 32, 32])) that is different to the input size (torch.Size([2, 1, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1675689665.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \"\"\"\n\u001b[0;32m--> 634\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3862\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3864\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3866\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "a = np.load(os.path.join(DEN_DIR, os.listdir(DEN_DIR)[0]))\n",
        "print(a.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GPD_xAUz7Y1",
        "outputId": "60cd0ea3-a37d-40e9-8ea8-721382874750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(405, 540)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- TRAIN FROM SCRATCH FOR 150 EPOCHS ----------\n",
        "import os, cv2, numpy as np, torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import vgg16\n",
        "from tqdm import tqdm\n",
        "\n",
        "IMG_DIR = \"/content/DEEPVISION/DATA/ShanghaiTech/part_A/train_data/images\"\n",
        "DEN_DIR = \"/content/DEEPVISION/DATA/processed/part_A/density\"\n",
        "\n",
        "class CrowdDataset(Dataset):\n",
        "    def __init__(self, img_dir, den_dir):\n",
        "        self.img_dir, self.den_dir = img_dir, den_dir\n",
        "        self.images = sorted([f for f in os.listdir(img_dir) if f.endswith(\".jpg\")])\n",
        "        self.dens   = sorted([f for f in os.listdir(den_dir) if f.endswith(\".npy\")])\n",
        "        self.pairs = [(os.path.join(img_dir, os.path.splitext(f)[0]+'.jpg'),\n",
        "                       os.path.join(den_dir, os.path.splitext(f)[0]+'.npy'))\n",
        "                      for f in self.images if os.path.splitext(f)[0]+'.npy' in self.dens]\n",
        "    def __len__(self): return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        imgp, denp = self.pairs[idx]\n",
        "        img = cv2.cvtColor(cv2.imread(imgp), cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (512,512)).astype('float32')/255.0\n",
        "        img = torch.from_numpy(img).permute(2,0,1)\n",
        "        den = np.load(denp).astype('float32')\n",
        "        den = cv2.resize(den, (32,32))\n",
        "        den = torch.from_numpy(den).unsqueeze(0)\n",
        "        return img, den\n",
        "\n",
        "ds = CrowdDataset(IMG_DIR, DEN_DIR)\n",
        "loader = DataLoader(ds, batch_size=2, shuffle=True, num_workers=0)\n",
        "\n",
        "# simple CSRNet-like model\n",
        "class CrowdCounter(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = vgg16(weights=\"DEFAULT\")\n",
        "        self.backbone = nn.Sequential(*list(vgg.features.children())[:23])\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Conv2d(512,256,3,padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(256,128,3,padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(128,1,1)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.regressor(x)\n",
        "        return x\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = CrowdCounter().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "EPOCHS = 150\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    total=0.0\n",
        "    for imgs, dens in loader:\n",
        "        imgs = imgs.to(device); dens = dens.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(imgs)\n",
        "        if preds.shape[-2:] != dens.shape[-2:]:\n",
        "            dens = torch.nn.functional.interpolate(dens, size=preds.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        loss = criterion(preds, dens)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "    print(f\"Epoch {epoch}/{EPOCHS}  AvgLoss:{total/len(loader):.6f}\")\n",
        "    if epoch%10==0 or epoch==EPOCHS:\n",
        "        torch.save(model.state_dict(), f\"/content/model_epoch_{epoch}.pth\")\n",
        "        print(\"Saved checkpoint:\", f\"/content/model_epoch_{epoch}.pth\")\n",
        "\n",
        "# final save (redundant)\n",
        "torch.save(model.state_dict(), \"/content/model_epoch_150.pth\")\n",
        "print(\"Final model saved: /content/model_epoch_150.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoacQ4tj0MTn",
        "outputId": "47383691-83a1-4304-f33d-7e92018b6eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150  AvgLoss:0.000355\n",
            "Epoch 2/150  AvgLoss:0.000027\n",
            "Epoch 3/150  AvgLoss:0.000014\n",
            "Epoch 4/150  AvgLoss:0.000010\n",
            "Epoch 5/150  AvgLoss:0.000008\n",
            "Epoch 6/150  AvgLoss:0.000007\n",
            "Epoch 7/150  AvgLoss:0.000006\n",
            "Epoch 8/150  AvgLoss:0.000006\n",
            "Epoch 9/150  AvgLoss:0.000005\n",
            "Epoch 10/150  AvgLoss:0.000005\n",
            "Saved checkpoint: /content/model_epoch_10.pth\n",
            "Epoch 11/150  AvgLoss:0.000005\n",
            "Epoch 12/150  AvgLoss:0.000005\n",
            "Epoch 13/150  AvgLoss:0.000005\n",
            "Epoch 14/150  AvgLoss:0.000004\n",
            "Epoch 15/150  AvgLoss:0.000004\n",
            "Epoch 16/150  AvgLoss:0.000004\n",
            "Epoch 17/150  AvgLoss:0.000004\n",
            "Epoch 18/150  AvgLoss:0.000004\n",
            "Epoch 19/150  AvgLoss:0.000004\n",
            "Epoch 20/150  AvgLoss:0.000004\n",
            "Saved checkpoint: /content/model_epoch_20.pth\n",
            "Epoch 21/150  AvgLoss:0.000004\n",
            "Epoch 22/150  AvgLoss:0.000004\n",
            "Epoch 23/150  AvgLoss:0.000004\n",
            "Epoch 24/150  AvgLoss:0.000004\n",
            "Epoch 25/150  AvgLoss:0.000004\n",
            "Epoch 26/150  AvgLoss:0.000004\n",
            "Epoch 27/150  AvgLoss:0.000003\n",
            "Epoch 28/150  AvgLoss:0.000003\n",
            "Epoch 29/150  AvgLoss:0.000004\n",
            "Epoch 30/150  AvgLoss:0.000003\n",
            "Saved checkpoint: /content/model_epoch_30.pth\n",
            "Epoch 31/150  AvgLoss:0.000003\n",
            "Epoch 32/150  AvgLoss:0.000003\n",
            "Epoch 33/150  AvgLoss:0.000003\n",
            "Epoch 34/150  AvgLoss:0.000003\n",
            "Epoch 35/150  AvgLoss:0.000003\n",
            "Epoch 36/150  AvgLoss:0.000003\n",
            "Epoch 37/150  AvgLoss:0.000003\n",
            "Epoch 38/150  AvgLoss:0.000003\n",
            "Epoch 39/150  AvgLoss:0.000003\n",
            "Epoch 40/150  AvgLoss:0.000003\n",
            "Saved checkpoint: /content/model_epoch_40.pth\n",
            "Epoch 41/150  AvgLoss:0.000003\n",
            "Epoch 42/150  AvgLoss:0.000003\n",
            "Epoch 43/150  AvgLoss:0.000003\n",
            "Epoch 44/150  AvgLoss:0.000003\n",
            "Epoch 45/150  AvgLoss:0.000003\n",
            "Epoch 46/150  AvgLoss:0.000003\n",
            "Epoch 47/150  AvgLoss:0.000003\n",
            "Epoch 48/150  AvgLoss:0.000003\n",
            "Epoch 49/150  AvgLoss:0.000003\n",
            "Epoch 50/150  AvgLoss:0.000003\n",
            "Saved checkpoint: /content/model_epoch_50.pth\n",
            "Epoch 51/150  AvgLoss:0.000003\n",
            "Epoch 52/150  AvgLoss:0.000003\n",
            "Epoch 53/150  AvgLoss:0.000003\n",
            "Epoch 54/150  AvgLoss:0.000003\n",
            "Epoch 55/150  AvgLoss:0.000003\n",
            "Epoch 56/150  AvgLoss:0.000003\n",
            "Epoch 57/150  AvgLoss:0.000003\n",
            "Epoch 58/150  AvgLoss:0.000003\n",
            "Epoch 59/150  AvgLoss:0.000003\n",
            "Epoch 60/150  AvgLoss:0.000003\n",
            "Saved checkpoint: /content/model_epoch_60.pth\n",
            "Epoch 61/150  AvgLoss:0.000003\n",
            "Epoch 62/150  AvgLoss:0.000002\n",
            "Epoch 63/150  AvgLoss:0.000003\n",
            "Epoch 64/150  AvgLoss:0.000003\n",
            "Epoch 65/150  AvgLoss:0.000002\n",
            "Epoch 66/150  AvgLoss:0.000003\n",
            "Epoch 67/150  AvgLoss:0.000002\n",
            "Epoch 68/150  AvgLoss:0.000002\n",
            "Epoch 69/150  AvgLoss:0.000002\n",
            "Epoch 70/150  AvgLoss:0.000002\n",
            "Saved checkpoint: /content/model_epoch_70.pth\n",
            "Epoch 71/150  AvgLoss:0.000002\n",
            "Epoch 72/150  AvgLoss:0.000002\n",
            "Epoch 73/150  AvgLoss:0.000002\n",
            "Epoch 74/150  AvgLoss:0.000002\n",
            "Epoch 75/150  AvgLoss:0.000002\n",
            "Epoch 76/150  AvgLoss:0.000002\n",
            "Epoch 77/150  AvgLoss:0.000002\n",
            "Epoch 78/150  AvgLoss:0.000002\n",
            "Epoch 79/150  AvgLoss:0.000002\n",
            "Epoch 80/150  AvgLoss:0.000002\n",
            "Saved checkpoint: /content/model_epoch_80.pth\n",
            "Epoch 81/150  AvgLoss:0.000002\n",
            "Epoch 82/150  AvgLoss:0.000002\n",
            "Epoch 83/150  AvgLoss:0.000002\n",
            "Epoch 84/150  AvgLoss:0.000002\n",
            "Epoch 85/150  AvgLoss:0.000002\n",
            "Epoch 86/150  AvgLoss:0.000002\n",
            "Epoch 87/150  AvgLoss:0.000002\n",
            "Epoch 88/150  AvgLoss:0.000002\n",
            "Epoch 89/150  AvgLoss:0.000002\n",
            "Epoch 90/150  AvgLoss:0.000002\n",
            "Saved checkpoint: /content/model_epoch_90.pth\n",
            "Epoch 91/150  AvgLoss:0.000002\n",
            "Epoch 92/150  AvgLoss:0.000002\n",
            "Epoch 93/150  AvgLoss:0.000002\n",
            "Epoch 94/150  AvgLoss:0.000002\n",
            "Epoch 95/150  AvgLoss:0.000002\n",
            "Epoch 96/150  AvgLoss:0.000002\n",
            "Epoch 97/150  AvgLoss:0.000002\n",
            "Epoch 98/150  AvgLoss:0.000002\n",
            "Epoch 99/150  AvgLoss:0.000002\n",
            "Epoch 100/150  AvgLoss:0.000002\n",
            "Saved checkpoint: /content/model_epoch_100.pth\n",
            "Epoch 101/150  AvgLoss:0.000002\n",
            "Epoch 102/150  AvgLoss:0.000002\n",
            "Epoch 103/150  AvgLoss:0.000002\n",
            "Epoch 104/150  AvgLoss:0.000002\n",
            "Epoch 105/150  AvgLoss:0.000002\n",
            "Epoch 106/150  AvgLoss:0.000002\n",
            "Epoch 107/150  AvgLoss:0.000002\n",
            "Epoch 108/150  AvgLoss:0.000002\n",
            "Epoch 109/150  AvgLoss:0.000002\n",
            "Epoch 110/150  AvgLoss:0.000002\n",
            "Saved checkpoint: /content/model_epoch_110.pth\n",
            "Epoch 111/150  AvgLoss:0.000002\n",
            "Epoch 112/150  AvgLoss:0.000002\n",
            "Epoch 113/150  AvgLoss:0.000002\n",
            "Epoch 114/150  AvgLoss:0.000002\n",
            "Epoch 115/150  AvgLoss:0.000002\n",
            "Epoch 116/150  AvgLoss:0.000002\n",
            "Epoch 117/150  AvgLoss:0.000002\n",
            "Epoch 118/150  AvgLoss:0.000002\n",
            "Epoch 119/150  AvgLoss:0.000002\n",
            "Epoch 120/150  AvgLoss:0.000002\n",
            "Saved checkpoint: /content/model_epoch_120.pth\n",
            "Epoch 121/150  AvgLoss:0.000002\n",
            "Epoch 122/150  AvgLoss:0.000002\n",
            "Epoch 123/150  AvgLoss:0.000002\n",
            "Epoch 124/150  AvgLoss:0.000002\n",
            "Epoch 125/150  AvgLoss:0.000002\n",
            "Epoch 126/150  AvgLoss:0.000002\n",
            "Epoch 127/150  AvgLoss:0.000002\n",
            "Epoch 128/150  AvgLoss:0.000002\n",
            "Epoch 129/150  AvgLoss:0.000002\n",
            "Epoch 130/150  AvgLoss:0.000002\n",
            "Saved checkpoint: /content/model_epoch_130.pth\n",
            "Epoch 131/150  AvgLoss:0.000002\n",
            "Epoch 132/150  AvgLoss:0.000002\n",
            "Epoch 133/150  AvgLoss:0.000002\n",
            "Epoch 134/150  AvgLoss:0.000002\n",
            "Epoch 135/150  AvgLoss:0.000002\n",
            "Epoch 136/150  AvgLoss:0.000002\n",
            "Epoch 137/150  AvgLoss:0.000002\n",
            "Epoch 138/150  AvgLoss:0.000002\n",
            "Epoch 139/150  AvgLoss:0.000002\n",
            "Epoch 140/150  AvgLoss:0.000002\n",
            "Saved checkpoint: /content/model_epoch_140.pth\n",
            "Epoch 141/150  AvgLoss:0.000002\n",
            "Epoch 142/150  AvgLoss:0.000002\n",
            "Epoch 143/150  AvgLoss:0.000002\n",
            "Epoch 144/150  AvgLoss:0.000002\n",
            "Epoch 145/150  AvgLoss:0.000002\n",
            "Epoch 146/150  AvgLoss:0.000002\n",
            "Epoch 147/150  AvgLoss:0.000002\n",
            "Epoch 148/150  AvgLoss:0.000002\n",
            "Epoch 149/150  AvgLoss:0.000002\n",
            "Epoch 150/150  AvgLoss:0.000002\n",
            "Saved checkpoint: /content/model_epoch_150.pth\n",
            "Final model saved: /content/model_epoch_150.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. list checkpoints\n",
        "!ls -lh /content/model_epoch_*.pth\n",
        "\n",
        "# 2. quick forward pass (sanity)\n",
        "import torch, numpy as np\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "ckpt_path = \"/content/model_epoch_150.pth\"\n",
        "model = CrowdCounter().to(device)          # use your CrowdCounter class from notebook\n",
        "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# take one batch from loader\n",
        "imgs, dens = next(iter(loader))\n",
        "imgs = imgs.to(device)\n",
        "with torch.no_grad():\n",
        "    preds = model(imgs)\n",
        "print(\"imgs\", imgs.shape, \"preds\", preds.shape, \"gt\", dens.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8N6NaRyD7t2",
        "outputId": "ace05737-9b11-46da-9e2d-84b12829c3d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 35M Nov 28 10:20 /content/model_epoch_100.pth\n",
            "-rw-r--r-- 1 root root 35M Nov 28 09:40 /content/model_epoch_10.pth\n",
            "-rw-r--r-- 1 root root 35M Nov 28 10:25 /content/model_epoch_110.pth\n",
            "-rw-r--r-- 1 root root 35M Nov 28 10:29 /content/model_epoch_120.pth\n",
            "-rw-r--r-- 1 root root 35M Nov 28 10:34 /content/model_epoch_130.pth\n",
            "-rw-r--r-- 1 root root 35M Nov 28 10:38 /content/model_epoch_140.pth\n",
            "-rw-r--r-- 1 root root 35M Nov 28 10:43 /content/model_epoch_150.pth\n",
            "-rw-r--r-- 1 root root 35M Nov 28 09:44 /content/model_epoch_20.pth\n",
            "-rw-r--r-- 1 root root 35M Nov 28 09:49 /content/model_epoch_30.pth\n",
            "-rw-r--r-- 1 root root 35M Nov 28 09:53 /content/model_epoch_40.pth\n",
            "-rw-r--r-- 1 root root 35M Nov 28 09:58 /content/model_epoch_50.pth\n",
            "-rw-r--r-- 1 root root 35M Nov 28 10:02 /content/model_epoch_60.pth\n",
            "-rw-r--r-- 1 root root 35M Nov 28 10:07 /content/model_epoch_70.pth\n",
            "-rw-r--r-- 1 root root 35M Nov 28 10:11 /content/model_epoch_80.pth\n",
            "-rw-r--r-- 1 root root 35M Nov 28 10:16 /content/model_epoch_90.pth\n",
            "imgs torch.Size([2, 3, 512, 512]) preds torch.Size([2, 1, 64, 64]) gt torch.Size([2, 1, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== TEST DATASET ======\n",
        "\n",
        "TEST_IMG_DIR = \"/content/DEEPVISION/DATA/ShanghaiTech/part_A/test_data/images\"\n",
        "TEST_DEN_DIR = \"/content/DEEPVISION/DATA/processed/part_A/density\"\n",
        "\n",
        "test_ds = CrowdDataset(TEST_IMG_DIR, TEST_DEN_DIR)\n",
        "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
        "\n",
        "print(\"Test samples:\", len(test_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTFMzNhkEdop",
        "outputId": "dc8a48da-cfc4-4fa6-a558-c6ada6128bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test samples: 182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mae, rmse = evaluate_mae_rmse(model, test_loader, device=device)\n",
        "print(\"MAE:\", mae, \"RMSE:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c7kgpwKEevy",
        "outputId": "01af072d-833a-449a-d62a-5ee0f3c14fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 4.146615 RMSE: 6.191793782232998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "def evaluate_mae_rmse(model, data_loader, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    mae_sum = 0.0\n",
        "    mse_sum = 0.0\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, dens in data_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            dens = dens.to(device)\n",
        "            if dens.dim()==3: dens = dens.unsqueeze(1)\n",
        "            preds = model(imgs)\n",
        "            # resize gt -> pred size if needed\n",
        "            if preds.shape[2:] != dens.shape[2:]:\n",
        "                dens_res = torch.nn.functional.interpolate(dens, size=preds.shape[2:], mode='bilinear', align_corners=False)\n",
        "            else:\n",
        "                dens_res = dens\n",
        "            # sum over spatial dims\n",
        "            pred_counts = preds.sum(dim=(1,2,3)).cpu().numpy()\n",
        "            gt_counts   = dens_res.sum(dim=(1,2,3)).cpu().numpy()\n",
        "            mae_sum += (abs(pred_counts - gt_counts)).sum()\n",
        "            mse_sum += ((pred_counts - gt_counts)**2).sum()\n",
        "            n += imgs.size(0)\n",
        "    mae = mae_sum / n\n",
        "    rmse = math.sqrt(mse_sum / n)\n",
        "    return mae, rmse\n",
        "\n",
        "# use your saved model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = CrowdCounter().to(device)\n",
        "model.load_state_dict(torch.load(\"/content/model_epoch_150.pth\", map_location=device))\n",
        "\n",
        "# create test_loader  replace with your actual test DataLoader creation:\n",
        "# test_ds = CrowdDataset(TEST_IMG_DIR, TEST_DEN_DIR)\n",
        "# test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
        "mae, rmse = evaluate_mae_rmse(model, test_loader, device=device)\n",
        "print(\"MAE:\", mae, \"RMSE:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y35VujlEwgH",
        "outputId": "a63bcba2-6972-4955-ac46-b3af8c8c05b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 4.146615 RMSE: 6.191793782232998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2, os, numpy as np\n",
        "\n",
        "def downsample_preserve_sum(den, new_shape):\n",
        "    # den: numpy array HxW\n",
        "    # new_shape: (h_out, w_out)\n",
        "    h, w = den.shape\n",
        "    h2, w2 = new_shape\n",
        "    # use cv2.INTER_AREA (gives averaged values), then multiply by factor to preserve sum\n",
        "    den_small = cv2.resize(den, (w2, h2), interpolation=cv2.INTER_AREA)\n",
        "    scale = (h*w) / (h2*w2)\n",
        "    den_small_sum_preserved = den_small * scale\n",
        "    return den_small_sum_preserved\n",
        "\n",
        "# Example: iterate over your density folder and save new files\n",
        "den_dir = \"/content/DEEPVISION/DATA/processed/part_A/density\"\n",
        "out_dir = \"/content/DEEPVISION/DATA/processed/part_A/density_down32\"  # e.g.\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "for f in os.listdir(den_dir):\n",
        "    if not f.endswith(\".npy\"): continue\n",
        "    den = np.load(os.path.join(den_dir, f))\n",
        "    den_small = downsample_preserve_sum(den, (32,32))   # change to target\n",
        "    np.save(os.path.join(out_dir, f), den_small)\n",
        "\n"
      ],
      "metadata": {
        "id": "L1SvdgecE5WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(train_losses, marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "XAttnd0hFsGp",
        "outputId": "8b9db47e-2551-49b8-e495-f1aa6ca4c601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERAL TESTING (MAE + RMSE)\n",
        " **Define Testing Dataset**"
      ],
      "metadata": {
        "id": "NFqvQuoDpKlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, torch, numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import scipy.io as sio\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, img_dir, mat_dir):\n",
        "        self.img_dir = img_dir\n",
        "        self.mat_dir = mat_dir\n",
        "        self.imgs = sorted([f for f in os.listdir(img_dir) if f.endswith(\".jpg\")])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.imgs[idx]\n",
        "\n",
        "        # ---- Load Image ----\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (512,512)).astype(\"float32\")/255.0\n",
        "        img = torch.from_numpy(img).permute(2,0,1)\n",
        "\n",
        "        # ---- Load Ground Truth MAT ----\n",
        "        mat_name = img_name.replace(\".jpg\", \".mat\")\n",
        "        mat_path = os.path.join(self.mat_dir, mat_name)\n",
        "        mat = sio.loadmat(mat_path)\n",
        "        gt_count = mat[\"annPoints\"].shape[0]   # GT number of people\n",
        "\n",
        "        return img, gt_count, img_name\n"
      ],
      "metadata": {
        "id": "nIq3XXOOpDNa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define MAE & RMSE Evaluation**"
      ],
      "metadata": {
        "id": "1FvKoJ1QprZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def evaluate_model(model, loader, device):\n",
        "    model.eval()\n",
        "    mae_sum = 0\n",
        "    rmse_sum = 0\n",
        "    n = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img, gt_count, name in loader:\n",
        "            img = img.to(device)\n",
        "            pred = model(img)\n",
        "\n",
        "            # predicted count = sum of density map\n",
        "            pred_count = pred.sum().item()\n",
        "\n",
        "            # error\n",
        "            mae_sum += abs(pred_count - gt_count.item())\n",
        "            rmse_sum += (pred_count - gt_count.item())**2\n",
        "            n += 1\n",
        "\n",
        "    mae = mae_sum / n\n",
        "    rmse = math.sqrt(rmse_sum / n)\n",
        "    return mae, rmse\n"
      ],
      "metadata": {
        "id": "kjClIj0PpgWx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpfbzAIQ0O27",
        "outputId": "79e4b256-ed0b-48ba-c4a7-68b6475ac397"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/DEEPVISION"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_637hGq0Uki",
        "outputId": "228733bc-10cf-40ea-eee6-7a285ef40177"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEEPVISION\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) See top-level of your Drive\n",
        "!ls /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXG0kJok1mG6",
        "outputId": "53bcaf5b-350c-4ccd-b552-4c5ef6a15410"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "'Colab Notebooks'   DEEPVISION.zip   model_epoch_50.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) make sure cwd and /tmp exist (avoid obscure os errors)\n",
        "import os, sys\n",
        "print(\"cwd before:\", os.getcwd())\n",
        "try:\n",
        "    os.makedirs(os.getcwd(), exist_ok=True)\n",
        "    os.makedirs(\"/tmp\", exist_ok=True)\n",
        "    os.makedirs(\"/content\", exist_ok=True)\n",
        "except Exception as e:\n",
        "    print(\"mkdir error (harmless):\", e)\n",
        "\n",
        "# 2) (Optional but recommended) reinstall matching torch & torchvision\n",
        "#    Colab sometimes has mismatched preinstalled packages; reinstalling helps.\n",
        "#    This will restart the import machinery for torchvision.\n",
        "#    NOTE: this may take ~1-2 minutes.\n",
        "print(\"Upgrading torch & torchvision...\")\n",
        "!pip install -q --upgrade torch torchvision --progress-bar off\n",
        "\n",
        "# 3) Clear python import caches (safe way to ensure new install used)\n",
        "import importlib, pkgutil\n",
        "for m in (\"torch\", \"torchvision\"):\n",
        "    if m in sys.modules:\n",
        "        del sys.modules[m]\n",
        "\n",
        "# 4) Try importing the problematic symbol now\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    from torchvision.models import vgg16\n",
        "    print(\"torch version:\", torch.__version__)\n",
        "    print(\"torchvision version:\", torchvision.__version__)\n",
        "    _ = vgg16(weights=\"DEFAULT\")\n",
        "    print(\"vgg16 import OK\")\n",
        "except Exception as err:\n",
        "    # show a compact traceback so we can diagnose further\n",
        "    import traceback\n",
        "    print(\"Import failed  full traceback below:\\n\")\n",
        "    traceback.print_exc()\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otoh96xT3IBk",
        "outputId": "ef3142aa-2513-45e6-f46e-bf1f154d286e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cwd before: /content\n",
            "Upgrading torch & torchvision...\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mtorch version: 2.9.1+cu128\n",
            "torchvision version: 0.24.1+cu128\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 528M/528M [00:06<00:00, 91.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vgg16 import OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"cwd:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAuwj43d3d_-",
        "outputId": "0f933b1c-07a3-4f57-c4d9-308377dc052d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cwd: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content\n",
        "!ls -lh /content/DEEPVISION\n",
        "!ls -lh /content/DEEPVISION/DATA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKGhn2Zj5Bb_",
        "outputId": "46da8c64-458a-48be-f78a-2141e824a35e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12K\n",
            "drwxr-xr-x 3 root root 4.0K Nov 30 17:31 DEEPVISION\n",
            "drwx------ 5 root root 4.0K Nov 30 16:51 drive\n",
            "drwxr-xr-x 1 root root 4.0K Nov 20 14:30 sample_data\n",
            "total 4.0K\n",
            "drwxrwxrwx 5 root root 4.0K Nov 30 17:31 DEEPVISION\n",
            "ls: cannot access '/content/DEEPVISION/DATA': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content -name \"model_epoch_150.pth\""
      ],
      "metadata": {
        "id": "gEaaJqo35X5G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run as a single code cell (python shell lines starting with !)\n",
        "!echo \"---- find any model_epoch_*.pth under /content ----\"\n",
        "!find /content -maxdepth 4 -type f -name \"model_epoch_*.pth\" -print || true\n",
        "\n",
        "!echo \"\"\n",
        "!echo \"---- list top-level /content and DEEPVISION contents ----\"\n",
        "!ls -lh /content || true\n",
        "!echo \"\"\n",
        "!ls -lh /content/DEEPVISION || true\n",
        "!echo \"\"\n",
        "!echo \"---- recursive list (first 4 levels) of DEEPVISION ----\"\n",
        "!ls -R /content/DEEPVISION | sed -n '1,200p' || true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljxVi8Cg5z32",
        "outputId": "8eecbc27-3640-4ed4-aae1-02f0d92ba9f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- find any model_epoch_*.pth under /content ----\n",
            "/content/drive/MyDrive/model_epoch_50.pth\n",
            "\n",
            "---- list top-level /content and DEEPVISION contents ----\n",
            "total 12K\n",
            "drwxr-xr-x 3 root root 4.0K Nov 30 17:31 DEEPVISION\n",
            "drwx------ 5 root root 4.0K Nov 30 16:51 drive\n",
            "drwxr-xr-x 1 root root 4.0K Nov 20 14:30 sample_data\n",
            "\n",
            "total 4.0K\n",
            "drwxrwxrwx 5 root root 4.0K Nov 30 17:31 DEEPVISION\n",
            "\n",
            "---- recursive list (first 4 levels) of DEEPVISION ----\n",
            "/content/DEEPVISION:\n",
            "DEEPVISION\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION:\n",
            "DATA\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA:\n",
            "processed\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/processed:\n",
            "part_A\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/processed/part_A:\n",
            "density\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/processed/part_A/density:\n",
            "IMG_100.npy\n",
            "IMG_101.npy\n",
            "IMG_102.npy\n",
            "IMG_103.npy\n",
            "IMG_104.npy\n",
            "IMG_105.npy\n",
            "IMG_106.npy\n",
            "IMG_107.npy\n",
            "IMG_108.npy\n",
            "IMG_109.npy\n",
            "IMG_10.npy\n",
            "IMG_110.npy\n",
            "IMG_111.npy\n",
            "IMG_112.npy\n",
            "IMG_113.npy\n",
            "IMG_114.npy\n",
            "IMG_115.npy\n",
            "IMG_116.npy\n",
            "IMG_117.npy\n",
            "IMG_118.npy\n",
            "IMG_119.npy\n",
            "IMG_11.npy\n",
            "IMG_120.npy\n",
            "IMG_121.npy\n",
            "IMG_122.npy\n",
            "IMG_123.npy\n",
            "IMG_124.npy\n",
            "IMG_125.npy\n",
            "IMG_126.npy\n",
            "IMG_127.npy\n",
            "IMG_128.npy\n",
            "IMG_129.npy\n",
            "IMG_12.npy\n",
            "IMG_130.npy\n",
            "IMG_131.npy\n",
            "IMG_132.npy\n",
            "IMG_133.npy\n",
            "IMG_134.npy\n",
            "IMG_135.npy\n",
            "IMG_136.npy\n",
            "IMG_137.npy\n",
            "IMG_138.npy\n",
            "IMG_139.npy\n",
            "IMG_13.npy\n",
            "IMG_140.npy\n",
            "IMG_141.npy\n",
            "IMG_142.npy\n",
            "IMG_143.npy\n",
            "IMG_144.npy\n",
            "IMG_145.npy\n",
            "IMG_146.npy\n",
            "IMG_147.npy\n",
            "IMG_148.npy\n",
            "IMG_149.npy\n",
            "IMG_14.npy\n",
            "IMG_150.npy\n",
            "IMG_151.npy\n",
            "IMG_152.npy\n",
            "IMG_153.npy\n",
            "IMG_154.npy\n",
            "IMG_155.npy\n",
            "IMG_156.npy\n",
            "IMG_157.npy\n",
            "IMG_158.npy\n",
            "IMG_159.npy\n",
            "IMG_15.npy\n",
            "IMG_160.npy\n",
            "IMG_161.npy\n",
            "IMG_162.npy\n",
            "IMG_163.npy\n",
            "IMG_164.npy\n",
            "IMG_165.npy\n",
            "IMG_166.npy\n",
            "IMG_167.npy\n",
            "IMG_168.npy\n",
            "IMG_169.npy\n",
            "IMG_16.npy\n",
            "IMG_170.npy\n",
            "IMG_171.npy\n",
            "IMG_172.npy\n",
            "IMG_173.npy\n",
            "IMG_174.npy\n",
            "IMG_175.npy\n",
            "IMG_176.npy\n",
            "IMG_177.npy\n",
            "IMG_178.npy\n",
            "IMG_179.npy\n",
            "IMG_17.npy\n",
            "IMG_180.npy\n",
            "IMG_181.npy\n",
            "IMG_182.npy\n",
            "IMG_183.npy\n",
            "IMG_184.npy\n",
            "IMG_185.npy\n",
            "IMG_186.npy\n",
            "IMG_187.npy\n",
            "IMG_188.npy\n",
            "IMG_189.npy\n",
            "IMG_18.npy\n",
            "IMG_190.npy\n",
            "IMG_191.npy\n",
            "IMG_192.npy\n",
            "IMG_193.npy\n",
            "IMG_194.npy\n",
            "IMG_195.npy\n",
            "IMG_196.npy\n",
            "IMG_197.npy\n",
            "IMG_198.npy\n",
            "IMG_199.npy\n",
            "IMG_19.npy\n",
            "IMG_1.npy\n",
            "IMG_200.npy\n",
            "IMG_201.npy\n",
            "IMG_202.npy\n",
            "IMG_203.npy\n",
            "IMG_204.npy\n",
            "IMG_205.npy\n",
            "IMG_206.npy\n",
            "IMG_207.npy\n",
            "IMG_208.npy\n",
            "IMG_209.npy\n",
            "IMG_20.npy\n",
            "IMG_210.npy\n",
            "IMG_211.npy\n",
            "IMG_212.npy\n",
            "IMG_213.npy\n",
            "IMG_214.npy\n",
            "IMG_215.npy\n",
            "IMG_216.npy\n",
            "IMG_217.npy\n",
            "IMG_218.npy\n",
            "IMG_219.npy\n",
            "IMG_21.npy\n",
            "IMG_220.npy\n",
            "IMG_221.npy\n",
            "IMG_222.npy\n",
            "IMG_223.npy\n",
            "IMG_224.npy\n",
            "IMG_225.npy\n",
            "IMG_226.npy\n",
            "IMG_227.npy\n",
            "IMG_228.npy\n",
            "IMG_229.npy\n",
            "IMG_22.npy\n",
            "IMG_230.npy\n",
            "IMG_231.npy\n",
            "IMG_232.npy\n",
            "IMG_233.npy\n",
            "IMG_234.npy\n",
            "IMG_235.npy\n",
            "IMG_236.npy\n",
            "IMG_237.npy\n",
            "IMG_238.npy\n",
            "IMG_239.npy\n",
            "IMG_23.npy\n",
            "IMG_240.npy\n",
            "IMG_241.npy\n",
            "IMG_242.npy\n",
            "IMG_243.npy\n",
            "IMG_244.npy\n",
            "IMG_245.npy\n",
            "IMG_24.npy\n",
            "IMG_2.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!find /content/drive -maxdepth 4 -type f -iname \"DEEPVISION*.zip\" -print || true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dseo0a486Wht",
        "outputId": "50815b68-d008-4c89-cf00-0a7c503f5344"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DEEPVISION.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -R /content/DEEPVISION | head -50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLy9k_M68TFh",
        "outputId": "aa819e1b-0bdc-40ae-9ad0-804b525a85ab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DEEPVISION:\n",
            "\u001b[0m\u001b[34;42mDEEPVISION\u001b[0m/\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION:\n",
            "\u001b[34;42mDATA\u001b[0m/\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA:\n",
            "\u001b[34;42mprocessed\u001b[0m/\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/processed:\n",
            "\u001b[34;42mpart_A\u001b[0m/\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/processed/part_A:\n",
            "\u001b[34;42mdensity\u001b[0m/\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/processed/part_A/density:\n",
            "IMG_100.npy\n",
            "IMG_101.npy\n",
            "IMG_102.npy\n",
            "IMG_103.npy\n",
            "IMG_104.npy\n",
            "IMG_105.npy\n",
            "IMG_106.npy\n",
            "IMG_107.npy\n",
            "IMG_108.npy\n",
            "IMG_109.npy\n",
            "IMG_10.npy\n",
            "IMG_110.npy\n",
            "IMG_111.npy\n",
            "IMG_112.npy\n",
            "IMG_113.npy\n",
            "IMG_114.npy\n",
            "IMG_115.npy\n",
            "IMG_116.npy\n",
            "IMG_117.npy\n",
            "IMG_118.npy\n",
            "IMG_119.npy\n",
            "IMG_11.npy\n",
            "IMG_120.npy\n",
            "IMG_121.npy\n",
            "IMG_122.npy\n",
            "IMG_123.npy\n",
            "IMG_124.npy\n",
            "IMG_125.npy\n",
            "IMG_126.npy\n",
            "IMG_127.npy\n",
            "IMG_128.npy\n",
            "IMG_129.npy\n",
            "IMG_12.npy\n",
            "IMG_130.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- single Python cell: run evaluation -----\n",
        "import os, math, numpy as np, torch, cv2\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import vgg16\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- update these to match your unzipped structure (I used what I saw in your ls) ---\n",
        "TEST_IMG_DIR = \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A/test_data/images\"\n",
        "TEST_DEN_DIR = \"/content/DEEPVISION/DEEPVISION/DATA/processed/part_A/density\"\n",
        "RESULTS_TXT = \"/content/test_results.txt\"\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# --- helper: try to find a .pth checkpoint in several likely places ---\n",
        "def find_checkpoint(names_to_try=None):\n",
        "    if names_to_try is None:\n",
        "        names_to_try = [\n",
        "            \"/content/model_epoch_150.pth\",\n",
        "            \"/content/model_epoch_100.pth\",\n",
        "            \"/content/model_epoch_50.pth\",\n",
        "            \"/content/DEEPVISION/model_epoch_150.pth\",\n",
        "            \"/content/DEEPVISION/model_epoch_100.pth\",\n",
        "            \"/content/DEEPVISION/DEEPVISION/model_epoch_150.pth\",\n",
        "            \"/content/drive/MyDrive/model_epoch_150.pth\",\n",
        "            \"/content/drive/MyDrive/DEEPVISION/model_epoch_150.pth\",\n",
        "        ]\n",
        "    # also search for any .pth under /content (fast)\n",
        "    for path in names_to_try:\n",
        "        if os.path.exists(path):\n",
        "            return path\n",
        "    # fallback: quick search for any .pth under /content (depth limited)\n",
        "    for root, dirs, files in os.walk(\"/content\", topdown=True):\n",
        "        # limit depth to avoid long search; break when depth > 4\n",
        "        if root.count(os.sep) - \"/content\".count(os.sep) > 5:\n",
        "            continue\n",
        "        for f in files:\n",
        "            if f.endswith(\".pth\"):\n",
        "                return os.path.join(root, f)\n",
        "    return None\n",
        "\n",
        "# ---- Dataset (same preprocessing as used for training) ----\n",
        "class CrowdDataset(Dataset):\n",
        "    def __init__(self, img_dir, den_dir, img_size=(512,512), den_down_to=(32,32)):\n",
        "        self.img_dir = img_dir\n",
        "        self.den_dir = den_dir\n",
        "        imgs = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(\".jpg\")]) if os.path.isdir(img_dir) else []\n",
        "        dens = sorted([f for f in os.listdir(den_dir) if f.lower().endswith(\".npy\")]) if os.path.isdir(den_dir) else []\n",
        "        den_names = set([os.path.splitext(x)[0] for x in dens])\n",
        "        self.pairs = []\n",
        "        for im in imgs:\n",
        "            stem = os.path.splitext(im)[0]\n",
        "            if stem in den_names:\n",
        "                self.pairs.append((os.path.join(img_dir, im), os.path.join(den_dir, stem + \".npy\")))\n",
        "        self.img_size = img_size\n",
        "        self.den_down_to = den_down_to\n",
        "\n",
        "    def __len__(self): return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        imgp, denp = self.pairs[idx]\n",
        "        img = cv2.imread(imgp)\n",
        "        if img is None:\n",
        "            raise FileNotFoundError(f\"Image not found: {imgp}\")\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, self.img_size).astype(np.float32) / 255.0\n",
        "        img = torch.from_numpy(img).permute(2,0,1).contiguous()\n",
        "        den = np.load(denp).astype(np.float32)\n",
        "        den = cv2.resize(den, self.den_down_to)\n",
        "        den = torch.from_numpy(den).unsqueeze(0).contiguous()\n",
        "        return img, den, imgp\n",
        "\n",
        "# ---- model (same arch you used) ----\n",
        "class CrowdCounter(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = vgg16(weights=\"DEFAULT\")\n",
        "        self.backbone = nn.Sequential(*list(vgg.features.children())[:23])\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Conv2d(512,256,3,padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256,128,3,padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128,1,1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.regressor(x)\n",
        "        return x\n",
        "\n",
        "# ---- evaluation function ----\n",
        "def evaluate_mae_rmse(model, data_loader, device):\n",
        "    model.eval()\n",
        "    mae_sum = 0.0\n",
        "    mse_sum = 0.0\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, dens, paths in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            imgs = imgs.to(device)\n",
        "            dens = dens.to(device)\n",
        "            preds = model(imgs)\n",
        "            if preds.shape[-2:] != dens.shape[-2:]:\n",
        "                dens_interp = torch.nn.functional.interpolate(dens, size=preds.shape[-2:], mode='bilinear', align_corners=False)\n",
        "            else:\n",
        "                dens_interp = dens\n",
        "            pred_count = preds.sum(dim=(1,2,3)).cpu().numpy()\n",
        "            gt_count = dens_interp.sum(dim=(1,2,3)).cpu().numpy()\n",
        "            mae_sum += np.abs(pred_count - gt_count).sum()\n",
        "            mse_sum += ((pred_count - gt_count)**2).sum()\n",
        "            n += imgs.size(0)\n",
        "    mae = mae_sum / n if n>0 else float('nan')\n",
        "    rmse = math.sqrt(mse_sum / n) if n>0 else float('nan')\n",
        "    return float(mae), float(rmse)\n",
        "\n",
        "# ---- main flow ----\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# confirm test & density folders exist\n",
        "print(\"Test image folder exists:\", os.path.isdir(TEST_IMG_DIR))\n",
        "print(\"Test density folder exists:\", os.path.isdir(TEST_DEN_DIR))\n",
        "\n",
        "# quick list counts (helpful)\n",
        "try:\n",
        "    print(\"Test images:\", len([f for f in os.listdir(TEST_IMG_DIR) if f.lower().endswith('.jpg')]))\n",
        "    print(\"Density maps:\", len([f for f in os.listdir(TEST_DEN_DIR) if f.lower().endswith('.npy')]))\n",
        "except Exception as e:\n",
        "    print(\"Could not list files:\", e)\n",
        "\n",
        "checkpoint = find_checkpoint()\n",
        "if checkpoint is None:\n",
        "    raise FileNotFoundError(\"No .pth checkpoint found under /content. Please point CHECKPOINT_PATH to your saved model file.\")\n",
        "print(\"Found checkpoint:\", checkpoint)\n",
        "\n",
        "# build and load model\n",
        "model = CrowdCounter().to(device)\n",
        "state = torch.load(checkpoint, map_location=device)\n",
        "# handle common save formats\n",
        "if isinstance(state, dict) and ('state_dict' in state or 'model_state_dict' in state):\n",
        "    if 'state_dict' in state:\n",
        "        sd = state['state_dict']\n",
        "    else:\n",
        "        sd = state['model_state_dict']\n",
        "    # remove 'module.' if present\n",
        "    sd = {k.replace('module.',''):v for k,v in sd.items()}\n",
        "    model.load_state_dict(sd)\n",
        "else:\n",
        "    # assume it's a state_dict\n",
        "    try:\n",
        "        model.load_state_dict(state)\n",
        "    except Exception:\n",
        "        model.load_state_dict({k.replace('module.',''):v for k,v in state.items()})\n",
        "\n",
        "print(\"Checkpoint loaded.\")\n",
        "\n",
        "# create test loader and run eval\n",
        "test_ds = CrowdDataset(TEST_IMG_DIR, TEST_DEN_DIR)\n",
        "if len(test_ds)==0:\n",
        "    raise RuntimeError(f\"No test pairs found. Check TEST_IMG_DIR and TEST_DEN_DIR paths.\\nIMG_DIR={TEST_IMG_DIR}\\nDEN_DIR={TEST_DEN_DIR}\")\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "print(\"Test samples:\", len(test_ds))\n",
        "\n",
        "mae, rmse = evaluate_mae_rmse(model, test_loader, device=device)\n",
        "print(\"\\nFINAL TEST RESULTS:\")\n",
        "print(\"MAE:\", mae)\n",
        "print(\"RMSE:\", rmse)\n",
        "\n",
        "with open(RESULTS_TXT, \"w\") as f:\n",
        "    f.write(f\"checkpoint: {checkpoint}\\n\")\n",
        "    f.write(f\"test_samples: {len(test_ds)}\\n\")\n",
        "    f.write(f\"MAE: {mae}\\n\")\n",
        "    f.write(f\"RMSE: {rmse}\\n\")\n",
        "print(\"Saved results to:\", RESULTS_TXT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "ABugpxX5-LXc",
        "outputId": "204032f4-33cb-44ed-9328-b47952fa5110"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Test image folder exists: True\n",
            "Test density folder exists: True\n",
            "Test images: 182\n",
            "Density maps: 300\n",
            "Found checkpoint: /content/DEEPVISION/DEEPVISION/venv/Lib/site-packages/distutils-precedence.pth\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\nPlease file an issue with the following so that we can make `weights_only=True` compatible with your use case: WeightsUnpickler error: \n\nUnsupported operand 105\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1388660576.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;31m# build and load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrowdCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;31m# handle common save formats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'state_dict'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'model_state_dict'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1551\u001b[0m                 )\n\u001b[1;32m   1552\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         return _legacy_load(\n\u001b[1;32m   1555\u001b[0m             \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\nPlease file an issue with the following so that we can make `weights_only=True` compatible with your use case: WeightsUnpickler error: \n\nUnsupported operand 105\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/DEEPVISION -type f -iname \"*.pth\" -print"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeHGFQpf_lxt",
        "outputId": "4717fdf1-5015-4014-89e7-9ed734f48508"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DEEPVISION/DEEPVISION/venv/Lib/site-packages/distutils-precedence.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# search common checkpoint extensions under /content (where Colab keeps things)\n",
        "!find /content -type f \\( -iname \"*.pth\" -o -iname \"*.pt\" -o -iname \"*.ckpt\" -o -iname \"*.pth.tar\" \\) -print\n",
        "\n",
        "# search your mounted Google Drive (this may be large, but necessary)\n",
        "!find /content/drive -type f \\( -iname \"*.pth\" -o -iname \"*.pt\" -o -iname \"*.ckpt\" -o -iname \"*.pth.tar\" \\) -print\n",
        "\n",
        "# list top-level of extracted DEEPVISION to be 100% sure where content lives\n",
        "!ls -R /content/DEEPVISION | sed -n '1,200p'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-QP5YugAne0",
        "outputId": "1c4223ac-ad7a-4d45-daec-6b5770f42c62"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DEEPVISION/DEEPVISION/venv/Lib/site-packages/distutils-precedence.pth\n",
            "/content/drive/MyDrive/model_epoch_50.pth\n",
            "/content/drive/MyDrive/model_epoch_50.pth\n",
            "/content/DEEPVISION:\n",
            "DEEPVISION\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION:\n",
            "DATA\n",
            "explore_dataset.ipynb\n",
            "modeltraining.ipynb\n",
            "preprocessing.ipynb\n",
            "processed\n",
            "training\n",
            "venv\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA:\n",
            "processed\n",
            "ShanghaiTech\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/processed:\n",
            "part_A\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/processed/part_A:\n",
            "density\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/processed/part_A/density:\n",
            "IMG_100.npy\n",
            "IMG_101.npy\n",
            "IMG_102.npy\n",
            "IMG_103.npy\n",
            "IMG_104.npy\n",
            "IMG_105.npy\n",
            "IMG_106.npy\n",
            "IMG_107.npy\n",
            "IMG_108.npy\n",
            "IMG_109.npy\n",
            "IMG_10.npy\n",
            "IMG_110.npy\n",
            "IMG_111.npy\n",
            "IMG_112.npy\n",
            "IMG_113.npy\n",
            "IMG_114.npy\n",
            "IMG_115.npy\n",
            "IMG_116.npy\n",
            "IMG_117.npy\n",
            "IMG_118.npy\n",
            "IMG_119.npy\n",
            "IMG_11.npy\n",
            "IMG_120.npy\n",
            "IMG_121.npy\n",
            "IMG_122.npy\n",
            "IMG_123.npy\n",
            "IMG_124.npy\n",
            "IMG_125.npy\n",
            "IMG_126.npy\n",
            "IMG_127.npy\n",
            "IMG_128.npy\n",
            "IMG_129.npy\n",
            "IMG_12.npy\n",
            "IMG_130.npy\n",
            "IMG_131.npy\n",
            "IMG_132.npy\n",
            "IMG_133.npy\n",
            "IMG_134.npy\n",
            "IMG_135.npy\n",
            "IMG_136.npy\n",
            "IMG_137.npy\n",
            "IMG_138.npy\n",
            "IMG_139.npy\n",
            "IMG_13.npy\n",
            "IMG_140.npy\n",
            "IMG_141.npy\n",
            "IMG_142.npy\n",
            "IMG_143.npy\n",
            "IMG_144.npy\n",
            "IMG_145.npy\n",
            "IMG_146.npy\n",
            "IMG_147.npy\n",
            "IMG_148.npy\n",
            "IMG_149.npy\n",
            "IMG_14.npy\n",
            "IMG_150.npy\n",
            "IMG_151.npy\n",
            "IMG_152.npy\n",
            "IMG_153.npy\n",
            "IMG_154.npy\n",
            "IMG_155.npy\n",
            "IMG_156.npy\n",
            "IMG_157.npy\n",
            "IMG_158.npy\n",
            "IMG_159.npy\n",
            "IMG_15.npy\n",
            "IMG_160.npy\n",
            "IMG_161.npy\n",
            "IMG_162.npy\n",
            "IMG_163.npy\n",
            "IMG_164.npy\n",
            "IMG_165.npy\n",
            "IMG_166.npy\n",
            "IMG_167.npy\n",
            "IMG_168.npy\n",
            "IMG_169.npy\n",
            "IMG_16.npy\n",
            "IMG_170.npy\n",
            "IMG_171.npy\n",
            "IMG_172.npy\n",
            "IMG_173.npy\n",
            "IMG_174.npy\n",
            "IMG_175.npy\n",
            "IMG_176.npy\n",
            "IMG_177.npy\n",
            "IMG_178.npy\n",
            "IMG_179.npy\n",
            "IMG_17.npy\n",
            "IMG_180.npy\n",
            "IMG_181.npy\n",
            "IMG_182.npy\n",
            "IMG_183.npy\n",
            "IMG_184.npy\n",
            "IMG_185.npy\n",
            "IMG_186.npy\n",
            "IMG_187.npy\n",
            "IMG_188.npy\n",
            "IMG_189.npy\n",
            "IMG_18.npy\n",
            "IMG_190.npy\n",
            "IMG_191.npy\n",
            "IMG_192.npy\n",
            "IMG_193.npy\n",
            "IMG_194.npy\n",
            "IMG_195.npy\n",
            "IMG_196.npy\n",
            "IMG_197.npy\n",
            "IMG_198.npy\n",
            "IMG_199.npy\n",
            "IMG_19.npy\n",
            "IMG_1.npy\n",
            "IMG_200.npy\n",
            "IMG_201.npy\n",
            "IMG_202.npy\n",
            "IMG_203.npy\n",
            "IMG_204.npy\n",
            "IMG_205.npy\n",
            "IMG_206.npy\n",
            "IMG_207.npy\n",
            "IMG_208.npy\n",
            "IMG_209.npy\n",
            "IMG_20.npy\n",
            "IMG_210.npy\n",
            "IMG_211.npy\n",
            "IMG_212.npy\n",
            "IMG_213.npy\n",
            "IMG_214.npy\n",
            "IMG_215.npy\n",
            "IMG_216.npy\n",
            "IMG_217.npy\n",
            "IMG_218.npy\n",
            "IMG_219.npy\n",
            "IMG_21.npy\n",
            "IMG_220.npy\n",
            "IMG_221.npy\n",
            "IMG_222.npy\n",
            "IMG_223.npy\n",
            "IMG_224.npy\n",
            "IMG_225.npy\n",
            "IMG_226.npy\n",
            "IMG_227.npy\n",
            "IMG_228.npy\n",
            "IMG_229.npy\n",
            "IMG_22.npy\n",
            "IMG_230.npy\n",
            "IMG_231.npy\n",
            "IMG_232.npy\n",
            "IMG_233.npy\n",
            "IMG_234.npy\n",
            "IMG_235.npy\n",
            "IMG_236.npy\n",
            "IMG_237.npy\n",
            "IMG_238.npy\n",
            "IMG_239.npy\n",
            "IMG_23.npy\n",
            "IMG_240.npy\n",
            "IMG_241.npy\n",
            "IMG_242.npy\n",
            "IMG_243.npy\n",
            "IMG_244.npy\n",
            "IMG_245.npy\n",
            "IMG_246.npy\n",
            "IMG_247.npy\n",
            "IMG_248.npy\n",
            "IMG_249.npy\n",
            "IMG_24.npy\n",
            "IMG_250.npy\n",
            "IMG_251.npy\n",
            "IMG_252.npy\n",
            "IMG_253.npy\n",
            "IMG_254.npy\n",
            "IMG_255.npy\n",
            "IMG_256.npy\n",
            "IMG_257.npy\n",
            "IMG_258.npy\n",
            "IMG_259.npy\n",
            "IMG_25.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/DEEPVISION_checkpoints"
      ],
      "metadata": {
        "id": "q6XazHMtBNR3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),\n",
        "           \"/content/drive/MyDrive/DEEPVISION_checkpoints/model_epoch_150_state_dict.pth\")\n",
        "\n",
        "print(\"Saved:\", \"/content/drive/MyDrive/DEEPVISION_checkpoints/model_epoch_150_state_dict.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrZS9L6qBSeF",
        "outputId": "67d6745b-78eb-443c-f9bc-be1f9ff56f26"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/DEEPVISION_checkpoints/model_epoch_150_state_dict.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TESTING**"
      ],
      "metadata": {
        "id": "bGtjphmEFCQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Robust evaluation: auto-match image/density pairs, skip bad .npy files, compute MAE/RMSE\n",
        "import os, math, cv2, numpy as np, torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vgg16\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Update checkpoint path if needed ---\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/DEEPVISION_checkpoints/model_epoch_150_state_dict.pth\"\n",
        "RESULTS_TXT = \"/content/test_results_epoch150.txt\"\n",
        "BATCH_SIZE = 1\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "print(\"Checkpoint path:\", CHECKPOINT_PATH)\n",
        "\n",
        "# --- candidate roots we found earlier (you can edit these if you want) ---\n",
        "IMAGE_CANDIDATES = [\n",
        "    \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A/test_data/images\",\n",
        "    \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A/train_data/images\",\n",
        "    \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images\",\n",
        "    \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/train_data/images\",\n",
        "]\n",
        "DENS_CANDIDATES = [\n",
        "    \"/content/DEEPVISION/DEEPVISION/DATA/processed/part_A/density\",\n",
        "    # add other density folders if present\n",
        "]\n",
        "\n",
        "# --- helper to list stems and counts ---\n",
        "def stems_in(dirpath, ext):\n",
        "    if not os.path.isdir(dirpath):\n",
        "        return set()\n",
        "    return set(os.path.splitext(f)[0] for f in os.listdir(dirpath) if f.lower().endswith(ext))\n",
        "\n",
        "# find best image folder matching the chosen density folder (first density candidate)\n",
        "DEN_DIR = None\n",
        "for d in DENS_CANDIDATES:\n",
        "    if os.path.isdir(d):\n",
        "        DEN_DIR = d\n",
        "        break\n",
        "if DEN_DIR is None:\n",
        "    raise FileNotFoundError(\"No density folder found. Check DENS_CANDIDATES or unzipping.\")\n",
        "\n",
        "print(\"Chosen density folder:\", DEN_DIR, \" (files:\", len([f for f in os.listdir(DEN_DIR) if f.lower().endswith('.npy')]), \")\")\n",
        "\n",
        "# choose image folder with largest intersection of stems\n",
        "best_img = None\n",
        "best_common = 0\n",
        "den_stems = stems_in(DEN_DIR, \".npy\")\n",
        "for img_dir in IMAGE_CANDIDATES:\n",
        "    if not os.path.isdir(img_dir): continue\n",
        "    img_stems = stems_in(img_dir, \".jpg\") | stems_in(img_dir, \".jpeg\") | stems_in(img_dir, \".png\")\n",
        "    common = len(den_stems & img_stems)\n",
        "    print(f\"Candidate {img_dir} -> {len(img_stems)} images, {common} matching density stems\")\n",
        "    if common > best_common:\n",
        "        best_common = common\n",
        "        best_img = img_dir\n",
        "\n",
        "if best_img is None or best_common == 0:\n",
        "    raise FileNotFoundError(\"No matching image/density pairs found. Check your dataset folders.\")\n",
        "\n",
        "TEST_IMG_DIR = best_img\n",
        "TEST_DEN_DIR = DEN_DIR\n",
        "print(\"Using test image dir:\", TEST_IMG_DIR)\n",
        "print(\"Using test density dir:\", TEST_DEN_DIR)\n",
        "print(\"Matching pairs:\", best_common)\n",
        "\n",
        "# --- Dataset that only keeps valid pairs and checks .npy shape on load (skips bad ones) ---\n",
        "class CrowdDatasetSafe(Dataset):\n",
        "    def __init__(self, img_dir, den_dir, img_size=(512,512), den_down_to=(32,32), verbose=True):\n",
        "        self.img_dir = img_dir\n",
        "        self.den_dir = den_dir\n",
        "        imgs = sorted([f for f in os.listdir(img_dir) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\"))])\n",
        "        dens = sorted([f for f in os.listdir(den_dir) if f.lower().endswith(\".npy\")])\n",
        "        den_stems = set(os.path.splitext(x)[0] for x in dens)\n",
        "        pairs = []\n",
        "        bad = []\n",
        "        for im in imgs:\n",
        "            stem = os.path.splitext(im)[0]\n",
        "            if stem not in den_stems:\n",
        "                continue\n",
        "            imgp = os.path.join(img_dir, im)\n",
        "            denp = os.path.join(den_dir, stem + \".npy\")\n",
        "            # quick validation: can we load the npy and is it 2D?\n",
        "            try:\n",
        "                arr = np.load(denp, allow_pickle=False)\n",
        "                if arr.ndim != 2:\n",
        "                    bad.append((imgp, denp, \"npy_not_2d\"))\n",
        "                    continue\n",
        "            except Exception as e:\n",
        "                bad.append((imgp, denp, f\"load_err:{e}\"))\n",
        "                continue\n",
        "            pairs.append((imgp, denp))\n",
        "        self.pairs = pairs\n",
        "        self.img_size = img_size\n",
        "        self.den_down_to = den_down_to\n",
        "        if verbose:\n",
        "            print(\"Valid pairs:\", len(self.pairs), \"Skipped pairs:\", len(bad))\n",
        "            if bad:\n",
        "                print(\"Examples skipped (first 10):\")\n",
        "                for i,b in enumerate(bad[:10]):\n",
        "                    print(\" \", i, b[0], \"|\", b[1], \"| reason:\", b[2])\n",
        "    def __len__(self): return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        imgp, denp = self.pairs[idx]\n",
        "        img = cv2.imread(imgp)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, self.img_size).astype(np.float32)/255.0\n",
        "        img = torch.from_numpy(img).permute(2,0,1).contiguous()\n",
        "        den = np.load(denp).astype(np.float32)\n",
        "        den = cv2.resize(den, self.den_down_to)\n",
        "        den = torch.from_numpy(den).unsqueeze(0).contiguous()\n",
        "        return img, den, imgp\n",
        "\n",
        "# --- model (same architecture as training) ---\n",
        "class CrowdCounter(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = vgg16(weights=\"DEFAULT\")\n",
        "        self.backbone = nn.Sequential(*list(vgg.features.children())[:23])\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Conv2d(512,256,3,padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256,128,3,padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128,1,1)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.regressor(x)\n",
        "        return x\n",
        "\n",
        "# --- evaluation function ---\n",
        "def evaluate_mae_rmse(model, data_loader, device):\n",
        "    model.eval()\n",
        "    mae_sum, mse_sum, n = 0.0, 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, dens, img_paths in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            imgs = imgs.to(device); dens = dens.to(device)\n",
        "            preds = model(imgs)\n",
        "            if preds.shape[-2:] != dens.shape[-2:]:\n",
        "                dens = torch.nn.functional.interpolate(dens, size=preds.shape[-2:], mode='bilinear', align_corners=False)\n",
        "            pred_count = preds.sum(dim=(1,2,3)).cpu().numpy()\n",
        "            gt_count = dens.sum(dim=(1,2,3)).cpu().numpy()\n",
        "            mae_sum += np.abs(pred_count - gt_count).sum()\n",
        "            mse_sum += ((pred_count - gt_count)**2).sum()\n",
        "            n += imgs.size(0)\n",
        "    if n==0:\n",
        "        raise RuntimeError(\"No valid test samples found after filtering.\")\n",
        "    mae = mae_sum / n\n",
        "    rmse = math.sqrt(mse_sum / n)\n",
        "    return float(mae), float(rmse)\n",
        "\n",
        "# --- load model + checkpoint robustly ---\n",
        "model = CrowdCounter().to(device)\n",
        "if not os.path.exists(CHECKPOINT_PATH):\n",
        "    raise FileNotFoundError(\"Checkpoint not found: \" + CHECKPOINT_PATH)\n",
        "\n",
        "state = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "# several possible formats\n",
        "if isinstance(state, dict) and ('state_dict' in state or 'model_state_dict' in state):\n",
        "    sd = state.get('state_dict', state.get('model_state_dict', state['state_dict']))\n",
        "    # strip 'module.' if present\n",
        "    sd2 = {k.replace('module.',''):v for k,v in sd.items()}\n",
        "    model.load_state_dict(sd2)\n",
        "else:\n",
        "    # assume state is a plain state_dict\n",
        "    try:\n",
        "        model.load_state_dict(state)\n",
        "    except Exception as e:\n",
        "        # last fallback: if state itself is nested, try to find first dict-like value\n",
        "        found = None\n",
        "        if isinstance(state, dict):\n",
        "            for v in state.values():\n",
        "                if isinstance(v, dict):\n",
        "                    found = v; break\n",
        "        if found is not None:\n",
        "            model.load_state_dict({k.replace('module.',''):v for k,v in found.items()})\n",
        "        else:\n",
        "            raise e\n",
        "\n",
        "print(\"Loaded checkpoint.\")\n",
        "\n",
        "# --- prepare test loader ---\n",
        "test_ds = CrowdDatasetSafe(TEST_IMG_DIR, TEST_DEN_DIR, img_size=(512,512), den_down_to=(32,32))\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "print(\"Test samples:\", len(test_ds))\n",
        "\n",
        "# --- run evaluation ---\n",
        "mae, rmse = evaluate_mae_rmse(model, test_loader, device=device)\n",
        "print(\"\\nFINAL TEST RESULTS:\")\n",
        "print(\"MAE:\", mae)\n",
        "print(\"RMSE:\", rmse)\n",
        "\n",
        "with open(RESULTS_TXT, \"w\") as f:\n",
        "    f.write(f\"checkpoint: {CHECKPOINT_PATH}\\n\")\n",
        "    f.write(f\"test_samples: {len(test_ds)}\\n\")\n",
        "    f.write(f\"MAE: {mae}\\nRMSE: {rmse}\\n\")\n",
        "print(\"Saved results to:\", RESULTS_TXT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhVT9jNEDM0_",
        "outputId": "a73babfc-204b-4c87-a9f9-df72f29f6ea8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Checkpoint path: /content/drive/MyDrive/DEEPVISION_checkpoints/model_epoch_150_state_dict.pth\n",
            "Chosen density folder: /content/DEEPVISION/DEEPVISION/DATA/processed/part_A/density  (files: 300 )\n",
            "Candidate /content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A/test_data/images -> 182 images, 182 matching density stems\n",
            "Candidate /content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A/train_data/images -> 300 images, 300 matching density stems\n",
            "Candidate /content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images -> 316 images, 300 matching density stems\n",
            "Candidate /content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/train_data/images -> 400 images, 300 matching density stems\n",
            "Using test image dir: /content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A/train_data/images\n",
            "Using test density dir: /content/DEEPVISION/DEEPVISION/DATA/processed/part_A/density\n",
            "Matching pairs: 300\n",
            "Loaded checkpoint.\n",
            "Valid pairs: 299 Skipped pairs: 1\n",
            "Examples skipped (first 10):\n",
            "  0 /content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A/train_data/images/IMG_245.jpg | /content/DEEPVISION/DEEPVISION/DATA/processed/part_A/density/IMG_245.npy | reason: load_err:cannot reshape array of size 49120 into shape (825,1024)\n",
            "Test samples: 299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 299/299 [00:10<00:00, 28.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FINAL TEST RESULTS:\n",
            "MAE: 240.72251892089844\n",
            "RMSE: 255.83201428134439\n",
            "Saved results to: /content/test_results_epoch150.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Daigonostic-find bad npy files**"
      ],
      "metadata": {
        "id": "qrGsE9F2Gc45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, numpy as np\n",
        "\n",
        "IMG_DIR = \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A/train_data/images\"\n",
        "DEN_DIR = \"/content/DEEPVISION/DEEPVISION/DATA/processed/part_A/density\"\n",
        "\n",
        "imgs = sorted([f for f in os.listdir(IMG_DIR) if f.lower().endswith(\".jpg\")])\n",
        "bad = []\n",
        "for im in imgs:\n",
        "    stem = os.path.splitext(im)[0]\n",
        "    denp = os.path.join(DEN_DIR, stem + \".npy\")\n",
        "    if not os.path.exists(denp):\n",
        "        bad.append((stem, denp, \"missing file\"))\n",
        "        continue\n",
        "    try:\n",
        "        a = np.load(denp, allow_pickle=False)\n",
        "        # basic sanity checks:\n",
        "        if not (isinstance(a, np.ndarray) and a.ndim in (2,)):\n",
        "            bad.append((stem, denp, f\"unexpected array shape/dtype: {getattr(a,'shape',None)} {type(a)}\"))\n",
        "    except Exception as e:\n",
        "        bad.append((stem, denp, repr(e)))\n",
        "\n",
        "print(\"Checked\", len(imgs), \"images.\")\n",
        "print(\"Bad entries:\", len(bad))\n",
        "for b in bad[:50]:\n",
        "    print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSKxZCdrFLEU",
        "outputId": "14bccbcd-bed6-4c86-d365-1de78f7d0d6f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checked 300 images.\n",
            "Bad entries: 1\n",
            "('IMG_245', '/content/DEEPVISION/DEEPVISION/DATA/processed/part_A/density/IMG_245.npy', \"ValueError('cannot reshape array of size 49120 into shape (825,1024)')\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Robust dataset + fine-tuning**"
      ],
      "metadata": {
        "id": "UtMSHoOjGmg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# robust fine-tune cell (skips invalid density files)\n",
        "import os, cv2, numpy as np, torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import vgg16\n",
        "\n",
        "IMG_DIR = \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A/train_data/images\"\n",
        "DEN_DIR = \"/content/DEEPVISION/DEEPVISION/DATA/processed/part_A/density\"\n",
        "CHECKPOINT = \"/content/drive/MyDrive/DEEPVISION_checkpoints/model_epoch_150_state_dict.pth\"\n",
        "\n",
        "# Build validated pairs list (fast check)\n",
        "imgs = sorted([f for f in os.listdir(IMG_DIR) if f.lower().endswith(\".jpg\")])\n",
        "dens = set([os.path.splitext(f)[0] for f in os.listdir(DEN_DIR) if f.lower().endswith(\".npy\")])\n",
        "\n",
        "pairs = []\n",
        "bad = []\n",
        "for im in imgs:\n",
        "    stem = os.path.splitext(im)[0]\n",
        "    if stem not in dens:\n",
        "        bad.append((stem, \"missing density\"))\n",
        "        continue\n",
        "    denp = os.path.join(DEN_DIR, stem + \".npy\")\n",
        "    try:\n",
        "        a = np.load(denp, allow_pickle=False)\n",
        "        if not isinstance(a, np.ndarray) or a.ndim not in (2,):\n",
        "            bad.append((stem, \"bad array shape/dtype\", getattr(a,'shape',None)))\n",
        "            continue\n",
        "        # add pair if ok\n",
        "        pairs.append((os.path.join(IMG_DIR, im), denp))\n",
        "    except Exception as e:\n",
        "        bad.append((stem, repr(e)))\n",
        "\n",
        "print(\"Valid pairs:\", len(pairs), \" Bad:\", len(bad))\n",
        "if len(bad)>0:\n",
        "    print(\"examples:\", bad[:8])\n",
        "\n",
        "# Custom dataset using validated pairs\n",
        "class CleanCrowdDataset(Dataset):\n",
        "    def __init__(self, pairs, img_size=(512,512), den_size=(32,32)):\n",
        "        self.pairs = pairs\n",
        "        self.img_size = img_size\n",
        "        self.den_size = den_size\n",
        "    def __len__(self): return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        imgp, denp = self.pairs[idx]\n",
        "        img = cv2.cvtColor(cv2.imread(imgp), cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, self.img_size).astype(\"float32\")/255.0\n",
        "        img = torch.from_numpy(img).permute(2,0,1).contiguous()\n",
        "        den = np.load(denp, allow_pickle=False).astype(\"float32\")\n",
        "        den = cv2.resize(den, self.den_size)\n",
        "        den = torch.from_numpy(den).unsqueeze(0).contiguous()\n",
        "        return img, den\n",
        "\n",
        "ds = CleanCrowdDataset(pairs)\n",
        "loader = DataLoader(ds, batch_size=2, shuffle=True, num_workers=0)\n",
        "print(\"Loader ready with\", len(ds), \"samples.\")\n",
        "\n",
        "# Model as before\n",
        "class CrowdCounter(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = vgg16(weights=\"DEFAULT\")\n",
        "        self.backbone = nn.Sequential(*list(vgg.features.children())[:23])\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Conv2d(512,256,3,padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256,128,3,padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128,1,1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x); x = self.regressor(x); return x\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = CrowdCounter().to(device)\n",
        "state = torch.load(CHECKPOINT, map_location=device)\n",
        "model.load_state_dict(state)\n",
        "print(\"Loaded checkpoint.\")\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
        "FINE_EPOCHS = 5   # safe short run  change to 10/50 later\n",
        "\n",
        "for epoch in range(1, FINE_EPOCHS+1):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for imgs, dens in loader:\n",
        "        imgs = imgs.to(device); dens = dens.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(imgs)\n",
        "        if preds.shape[-2:] != dens.shape[-2:]:\n",
        "            dens = torch.nn.functional.interpolate(dens, size=preds.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        loss = criterion(preds, dens)\n",
        "        loss.backward(); optimizer.step()\n",
        "        total += loss.item()\n",
        "    print(f\"Epoch {epoch}/{FINE_EPOCHS}  AvgLoss:{total/len(loader):.6f}\")\n",
        "\n",
        "# save\n",
        "out = \"/content/drive/MyDrive/DEEPVISION_checkpoints/model_finetuned_quick.pth\"\n",
        "os.makedirs(os.path.dirname(out), exist_ok=True)\n",
        "torch.save(model.state_dict(), out)\n",
        "print(\"Saved fine-tuned:\", out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuxO-AYIGGHY",
        "outputId": "a9966e3f-0a04-4b5f-b6d8-651731693aec"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid pairs: 299  Bad: 1\n",
            "examples: [('IMG_245', \"ValueError('cannot reshape array of size 49120 into shape (825,1024)')\")]\n",
            "Loader ready with 299 samples.\n",
            "Loaded checkpoint.\n",
            "Epoch 1/5  AvgLoss:0.003180\n",
            "Epoch 2/5  AvgLoss:0.001430\n",
            "Epoch 3/5  AvgLoss:0.000898\n",
            "Epoch 4/5  AvgLoss:0.000641\n",
            "Epoch 5/5  AvgLoss:0.000494\n",
            "Saved fine-tuned: /content/drive/MyDrive/DEEPVISION_checkpoints/model_finetuned_quick.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the fine-tuning model"
      ],
      "metadata": {
        "id": "KUaG2NrxHnUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# robust fine-tune cell (skips invalid density files)\n",
        "import os, cv2, numpy as np, torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import vgg16\n",
        "\n",
        "IMG_DIR = \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A/train_data/images\"\n",
        "DEN_DIR = \"/content/DEEPVISION/DEEPVISION/DATA/processed/part_A/density\"\n",
        "CHECKPOINT = \"/content/drive/MyDrive/DEEPVISION_checkpoints/model_finetuned_quick.pth\"\n",
        "\n",
        "# Build validated pairs list (fast check)\n",
        "imgs = sorted([f for f in os.listdir(IMG_DIR) if f.lower().endswith(\".jpg\")])\n",
        "dens = set([os.path.splitext(f)[0] for f in os.listdir(DEN_DIR) if f.lower().endswith(\".npy\")])\n",
        "\n",
        "pairs = []\n",
        "bad = []\n",
        "for im in imgs:\n",
        "    stem = os.path.splitext(im)[0]\n",
        "    if stem not in dens:\n",
        "        bad.append((stem, \"missing density\"))\n",
        "        continue\n",
        "    denp = os.path.join(DEN_DIR, stem + \".npy\")\n",
        "    try:\n",
        "        a = np.load(denp, allow_pickle=False)\n",
        "        if not isinstance(a, np.ndarray) or a.ndim not in (2,):\n",
        "            bad.append((stem, \"bad array shape/dtype\", getattr(a,'shape',None)))\n",
        "            continue\n",
        "        # add pair if ok\n",
        "        pairs.append((os.path.join(IMG_DIR, im), denp))\n",
        "    except Exception as e:\n",
        "        bad.append((stem, repr(e)))\n",
        "\n",
        "print(\"Valid pairs:\", len(pairs), \" Bad:\", len(bad))\n",
        "if len(bad)>0:\n",
        "    print(\"examples:\", bad[:8])\n",
        "\n",
        "# Custom dataset using validated pairs\n",
        "class CleanCrowdDataset(Dataset):\n",
        "    def __init__(self, pairs, img_size=(512,512), den_size=(32,32)):\n",
        "        self.pairs = pairs\n",
        "        self.img_size = img_size\n",
        "        self.den_size = den_size\n",
        "    def __len__(self): return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        imgp, denp = self.pairs[idx]\n",
        "        img = cv2.cvtColor(cv2.imread(imgp), cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, self.img_size).astype(\"float32\")/255.0\n",
        "        img = torch.from_numpy(img).permute(2,0,1).contiguous()\n",
        "        den = np.load(denp, allow_pickle=False).astype(\"float32\")\n",
        "        den = cv2.resize(den, self.den_size)\n",
        "        den = torch.from_numpy(den).unsqueeze(0).contiguous()\n",
        "        return img, den\n",
        "\n",
        "ds = CleanCrowdDataset(pairs)\n",
        "loader = DataLoader(ds, batch_size=2, shuffle=True, num_workers=0)\n",
        "print(\"Loader ready with\", len(ds), \"samples.\")\n",
        "\n",
        "# Model as before\n",
        "class CrowdCounter(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = vgg16(weights=\"DEFAULT\")\n",
        "        self.backbone = nn.Sequential(*list(vgg.features.children())[:23])\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Conv2d(512,256,3,padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256,128,3,padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128,1,1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x); x = self.regressor(x); return x\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = CrowdCounter().to(device)\n",
        "state = torch.load(CHECKPOINT, map_location=device)\n",
        "model.load_state_dict(state)\n",
        "print(\"Loaded checkpoint.\")\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
        "FINE_EPOCHS = 5   # safe short run  change to 10/50 later\n",
        "\n",
        "for epoch in range(1, FINE_EPOCHS+1):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for imgs, dens in loader:\n",
        "        imgs = imgs.to(device); dens = dens.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(imgs)\n",
        "        if preds.shape[-2:] != dens.shape[-2:]:\n",
        "            dens = torch.nn.functional.interpolate(dens, size=preds.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        loss = criterion(preds, dens)\n",
        "        loss.backward(); optimizer.step()\n",
        "        total += loss.item()\n",
        "    print(f\"Epoch {epoch}/{FINE_EPOCHS}  AvgLoss:{total/len(loader):.6f}\")\n",
        "\n",
        "# save\n",
        "out = \"/content/drive/MyDrive/DEEPVISION_checkpoints/model_finetuned_quick.pth\"\n",
        "os.makedirs(os.path.dirname(out), exist_ok=True)\n",
        "torch.save(model.state_dict(), out)\n",
        "print(\"Saved fine-tuned:\", out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvoxhSYGG8Ls",
        "outputId": "3c68fd90-e794-4368-8ae3-93947373d9fd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid pairs: 299  Bad: 1\n",
            "examples: [('IMG_245', \"ValueError('cannot reshape array of size 49120 into shape (825,1024)')\")]\n",
            "Loader ready with 299 samples.\n",
            "Loaded checkpoint.\n",
            "Epoch 1/5  AvgLoss:0.000334\n",
            "Epoch 2/5  AvgLoss:0.000227\n",
            "Epoch 3/5  AvgLoss:0.000173\n",
            "Epoch 4/5  AvgLoss:0.000139\n",
            "Epoch 5/5  AvgLoss:0.000116\n",
            "Saved fine-tuned: /content/drive/MyDrive/DEEPVISION_checkpoints/model_finetuned_quick.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART-B**"
      ],
      "metadata": {
        "id": "M__Ra3Z2LaMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R /content/DEEPVISION/DEEPVISION/DATA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aptgjlqJTuE",
        "outputId": "ebc0ed4a-1759-4d25-faef-52edbe286f8f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DEEPVISION/DEEPVISION/DATA:\n",
            "processed  ShanghaiTech\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/processed:\n",
            "part_A\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/processed/part_A:\n",
            "density\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/processed/part_A/density:\n",
            "IMG_100.npy  IMG_146.npy  IMG_191.npy  IMG_236.npy  IMG_281.npy  IMG_55.npy\n",
            "IMG_101.npy  IMG_147.npy  IMG_192.npy  IMG_237.npy  IMG_282.npy  IMG_56.npy\n",
            "IMG_102.npy  IMG_148.npy  IMG_193.npy  IMG_238.npy  IMG_283.npy  IMG_57.npy\n",
            "IMG_103.npy  IMG_149.npy  IMG_194.npy  IMG_239.npy  IMG_284.npy  IMG_58.npy\n",
            "IMG_104.npy  IMG_14.npy   IMG_195.npy  IMG_23.npy   IMG_285.npy  IMG_59.npy\n",
            "IMG_105.npy  IMG_150.npy  IMG_196.npy  IMG_240.npy  IMG_286.npy  IMG_5.npy\n",
            "IMG_106.npy  IMG_151.npy  IMG_197.npy  IMG_241.npy  IMG_287.npy  IMG_60.npy\n",
            "IMG_107.npy  IMG_152.npy  IMG_198.npy  IMG_242.npy  IMG_288.npy  IMG_61.npy\n",
            "IMG_108.npy  IMG_153.npy  IMG_199.npy  IMG_243.npy  IMG_289.npy  IMG_62.npy\n",
            "IMG_109.npy  IMG_154.npy  IMG_19.npy   IMG_244.npy  IMG_28.npy\t IMG_63.npy\n",
            "IMG_10.npy   IMG_155.npy  IMG_1.npy    IMG_245.npy  IMG_290.npy  IMG_64.npy\n",
            "IMG_110.npy  IMG_156.npy  IMG_200.npy  IMG_246.npy  IMG_291.npy  IMG_65.npy\n",
            "IMG_111.npy  IMG_157.npy  IMG_201.npy  IMG_247.npy  IMG_292.npy  IMG_66.npy\n",
            "IMG_112.npy  IMG_158.npy  IMG_202.npy  IMG_248.npy  IMG_293.npy  IMG_67.npy\n",
            "IMG_113.npy  IMG_159.npy  IMG_203.npy  IMG_249.npy  IMG_294.npy  IMG_68.npy\n",
            "IMG_114.npy  IMG_15.npy   IMG_204.npy  IMG_24.npy   IMG_295.npy  IMG_69.npy\n",
            "IMG_115.npy  IMG_160.npy  IMG_205.npy  IMG_250.npy  IMG_296.npy  IMG_6.npy\n",
            "IMG_116.npy  IMG_161.npy  IMG_206.npy  IMG_251.npy  IMG_297.npy  IMG_70.npy\n",
            "IMG_117.npy  IMG_162.npy  IMG_207.npy  IMG_252.npy  IMG_298.npy  IMG_71.npy\n",
            "IMG_118.npy  IMG_163.npy  IMG_208.npy  IMG_253.npy  IMG_299.npy  IMG_72.npy\n",
            "IMG_119.npy  IMG_164.npy  IMG_209.npy  IMG_254.npy  IMG_29.npy\t IMG_73.npy\n",
            "IMG_11.npy   IMG_165.npy  IMG_20.npy   IMG_255.npy  IMG_2.npy\t IMG_74.npy\n",
            "IMG_120.npy  IMG_166.npy  IMG_210.npy  IMG_256.npy  IMG_300.npy  IMG_75.npy\n",
            "IMG_121.npy  IMG_167.npy  IMG_211.npy  IMG_257.npy  IMG_30.npy\t IMG_76.npy\n",
            "IMG_122.npy  IMG_168.npy  IMG_212.npy  IMG_258.npy  IMG_31.npy\t IMG_77.npy\n",
            "IMG_123.npy  IMG_169.npy  IMG_213.npy  IMG_259.npy  IMG_32.npy\t IMG_78.npy\n",
            "IMG_124.npy  IMG_16.npy   IMG_214.npy  IMG_25.npy   IMG_33.npy\t IMG_79.npy\n",
            "IMG_125.npy  IMG_170.npy  IMG_215.npy  IMG_260.npy  IMG_34.npy\t IMG_7.npy\n",
            "IMG_126.npy  IMG_171.npy  IMG_216.npy  IMG_261.npy  IMG_35.npy\t IMG_80.npy\n",
            "IMG_127.npy  IMG_172.npy  IMG_217.npy  IMG_262.npy  IMG_36.npy\t IMG_81.npy\n",
            "IMG_128.npy  IMG_173.npy  IMG_218.npy  IMG_263.npy  IMG_37.npy\t IMG_82.npy\n",
            "IMG_129.npy  IMG_174.npy  IMG_219.npy  IMG_264.npy  IMG_38.npy\t IMG_83.npy\n",
            "IMG_12.npy   IMG_175.npy  IMG_21.npy   IMG_265.npy  IMG_39.npy\t IMG_84.npy\n",
            "IMG_130.npy  IMG_176.npy  IMG_220.npy  IMG_266.npy  IMG_3.npy\t IMG_85.npy\n",
            "IMG_131.npy  IMG_177.npy  IMG_221.npy  IMG_267.npy  IMG_40.npy\t IMG_86.npy\n",
            "IMG_132.npy  IMG_178.npy  IMG_222.npy  IMG_268.npy  IMG_41.npy\t IMG_87.npy\n",
            "IMG_133.npy  IMG_179.npy  IMG_223.npy  IMG_269.npy  IMG_42.npy\t IMG_88.npy\n",
            "IMG_134.npy  IMG_17.npy   IMG_224.npy  IMG_26.npy   IMG_43.npy\t IMG_89.npy\n",
            "IMG_135.npy  IMG_180.npy  IMG_225.npy  IMG_270.npy  IMG_44.npy\t IMG_8.npy\n",
            "IMG_136.npy  IMG_181.npy  IMG_226.npy  IMG_271.npy  IMG_45.npy\t IMG_90.npy\n",
            "IMG_137.npy  IMG_182.npy  IMG_227.npy  IMG_272.npy  IMG_46.npy\t IMG_91.npy\n",
            "IMG_138.npy  IMG_183.npy  IMG_228.npy  IMG_273.npy  IMG_47.npy\t IMG_92.npy\n",
            "IMG_139.npy  IMG_184.npy  IMG_229.npy  IMG_274.npy  IMG_48.npy\t IMG_93.npy\n",
            "IMG_13.npy   IMG_185.npy  IMG_22.npy   IMG_275.npy  IMG_49.npy\t IMG_94.npy\n",
            "IMG_140.npy  IMG_186.npy  IMG_230.npy  IMG_276.npy  IMG_4.npy\t IMG_95.npy\n",
            "IMG_141.npy  IMG_187.npy  IMG_231.npy  IMG_277.npy  IMG_50.npy\t IMG_96.npy\n",
            "IMG_142.npy  IMG_188.npy  IMG_232.npy  IMG_278.npy  IMG_51.npy\t IMG_97.npy\n",
            "IMG_143.npy  IMG_189.npy  IMG_233.npy  IMG_279.npy  IMG_52.npy\t IMG_98.npy\n",
            "IMG_144.npy  IMG_18.npy   IMG_234.npy  IMG_27.npy   IMG_53.npy\t IMG_99.npy\n",
            "IMG_145.npy  IMG_190.npy  IMG_235.npy  IMG_280.npy  IMG_54.npy\t IMG_9.npy\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech:\n",
            "part_A\tpart_B\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A:\n",
            "test_data  train_data\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A/test_data:\n",
            "ground-truth  images\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A/test_data/ground-truth:\n",
            "GT_IMG_100.mat\tGT_IMG_134.mat\tGT_IMG_168.mat\tGT_IMG_36.mat  GT_IMG_6.mat\n",
            "GT_IMG_101.mat\tGT_IMG_135.mat\tGT_IMG_169.mat\tGT_IMG_37.mat  GT_IMG_70.mat\n",
            "GT_IMG_102.mat\tGT_IMG_136.mat\tGT_IMG_16.mat\tGT_IMG_38.mat  GT_IMG_71.mat\n",
            "GT_IMG_103.mat\tGT_IMG_137.mat\tGT_IMG_170.mat\tGT_IMG_39.mat  GT_IMG_72.mat\n",
            "GT_IMG_104.mat\tGT_IMG_138.mat\tGT_IMG_171.mat\tGT_IMG_3.mat   GT_IMG_73.mat\n",
            "GT_IMG_105.mat\tGT_IMG_139.mat\tGT_IMG_172.mat\tGT_IMG_40.mat  GT_IMG_74.mat\n",
            "GT_IMG_106.mat\tGT_IMG_13.mat\tGT_IMG_173.mat\tGT_IMG_41.mat  GT_IMG_75.mat\n",
            "GT_IMG_107.mat\tGT_IMG_140.mat\tGT_IMG_174.mat\tGT_IMG_42.mat  GT_IMG_76.mat\n",
            "GT_IMG_108.mat\tGT_IMG_141.mat\tGT_IMG_175.mat\tGT_IMG_43.mat  GT_IMG_77.mat\n",
            "GT_IMG_109.mat\tGT_IMG_142.mat\tGT_IMG_176.mat\tGT_IMG_44.mat  GT_IMG_78.mat\n",
            "GT_IMG_10.mat\tGT_IMG_143.mat\tGT_IMG_177.mat\tGT_IMG_45.mat  GT_IMG_79.mat\n",
            "GT_IMG_110.mat\tGT_IMG_144.mat\tGT_IMG_178.mat\tGT_IMG_46.mat  GT_IMG_7.mat\n",
            "GT_IMG_111.mat\tGT_IMG_145.mat\tGT_IMG_179.mat\tGT_IMG_47.mat  GT_IMG_80.mat\n",
            "GT_IMG_112.mat\tGT_IMG_146.mat\tGT_IMG_17.mat\tGT_IMG_48.mat  GT_IMG_81.mat\n",
            "GT_IMG_113.mat\tGT_IMG_147.mat\tGT_IMG_180.mat\tGT_IMG_49.mat  GT_IMG_82.mat\n",
            "GT_IMG_114.mat\tGT_IMG_148.mat\tGT_IMG_181.mat\tGT_IMG_4.mat   GT_IMG_83.mat\n",
            "GT_IMG_115.mat\tGT_IMG_149.mat\tGT_IMG_182.mat\tGT_IMG_50.mat  GT_IMG_84.mat\n",
            "GT_IMG_116.mat\tGT_IMG_14.mat\tGT_IMG_18.mat\tGT_IMG_51.mat  GT_IMG_85.mat\n",
            "GT_IMG_117.mat\tGT_IMG_150.mat\tGT_IMG_19.mat\tGT_IMG_52.mat  GT_IMG_86.mat\n",
            "GT_IMG_118.mat\tGT_IMG_151.mat\tGT_IMG_1.mat\tGT_IMG_53.mat  GT_IMG_87.mat\n",
            "GT_IMG_119.mat\tGT_IMG_152.mat\tGT_IMG_20.mat\tGT_IMG_54.mat  GT_IMG_88.mat\n",
            "GT_IMG_11.mat\tGT_IMG_153.mat\tGT_IMG_21.mat\tGT_IMG_55.mat  GT_IMG_89.mat\n",
            "GT_IMG_120.mat\tGT_IMG_154.mat\tGT_IMG_22.mat\tGT_IMG_56.mat  GT_IMG_8.mat\n",
            "GT_IMG_121.mat\tGT_IMG_155.mat\tGT_IMG_23.mat\tGT_IMG_57.mat  GT_IMG_90.mat\n",
            "GT_IMG_122.mat\tGT_IMG_156.mat\tGT_IMG_24.mat\tGT_IMG_58.mat  GT_IMG_91.mat\n",
            "GT_IMG_123.mat\tGT_IMG_157.mat\tGT_IMG_25.mat\tGT_IMG_59.mat  GT_IMG_92.mat\n",
            "GT_IMG_124.mat\tGT_IMG_158.mat\tGT_IMG_26.mat\tGT_IMG_5.mat   GT_IMG_93.mat\n",
            "GT_IMG_125.mat\tGT_IMG_159.mat\tGT_IMG_27.mat\tGT_IMG_60.mat  GT_IMG_94.mat\n",
            "GT_IMG_126.mat\tGT_IMG_15.mat\tGT_IMG_28.mat\tGT_IMG_61.mat  GT_IMG_95.mat\n",
            "GT_IMG_127.mat\tGT_IMG_160.mat\tGT_IMG_29.mat\tGT_IMG_62.mat  GT_IMG_96.mat\n",
            "GT_IMG_128.mat\tGT_IMG_161.mat\tGT_IMG_2.mat\tGT_IMG_63.mat  GT_IMG_97.mat\n",
            "GT_IMG_129.mat\tGT_IMG_162.mat\tGT_IMG_30.mat\tGT_IMG_64.mat  GT_IMG_98.mat\n",
            "GT_IMG_12.mat\tGT_IMG_163.mat\tGT_IMG_31.mat\tGT_IMG_65.mat  GT_IMG_99.mat\n",
            "GT_IMG_130.mat\tGT_IMG_164.mat\tGT_IMG_32.mat\tGT_IMG_66.mat  GT_IMG_9.mat\n",
            "GT_IMG_131.mat\tGT_IMG_165.mat\tGT_IMG_33.mat\tGT_IMG_67.mat\n",
            "GT_IMG_132.mat\tGT_IMG_166.mat\tGT_IMG_34.mat\tGT_IMG_68.mat\n",
            "GT_IMG_133.mat\tGT_IMG_167.mat\tGT_IMG_35.mat\tGT_IMG_69.mat\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A/test_data/images:\n",
            "IMG_100.jpg  IMG_129.jpg  IMG_157.jpg  IMG_1.jpg   IMG_48.jpg  IMG_76.jpg\n",
            "IMG_101.jpg  IMG_12.jpg   IMG_158.jpg  IMG_20.jpg  IMG_49.jpg  IMG_77.jpg\n",
            "IMG_102.jpg  IMG_130.jpg  IMG_159.jpg  IMG_21.jpg  IMG_4.jpg   IMG_78.jpg\n",
            "IMG_103.jpg  IMG_131.jpg  IMG_15.jpg   IMG_22.jpg  IMG_50.jpg  IMG_79.jpg\n",
            "IMG_104.jpg  IMG_132.jpg  IMG_160.jpg  IMG_23.jpg  IMG_51.jpg  IMG_7.jpg\n",
            "IMG_105.jpg  IMG_133.jpg  IMG_161.jpg  IMG_24.jpg  IMG_52.jpg  IMG_80.jpg\n",
            "IMG_106.jpg  IMG_134.jpg  IMG_162.jpg  IMG_25.jpg  IMG_53.jpg  IMG_81.jpg\n",
            "IMG_107.jpg  IMG_135.jpg  IMG_163.jpg  IMG_26.jpg  IMG_54.jpg  IMG_82.jpg\n",
            "IMG_108.jpg  IMG_136.jpg  IMG_164.jpg  IMG_27.jpg  IMG_55.jpg  IMG_83.jpg\n",
            "IMG_109.jpg  IMG_137.jpg  IMG_165.jpg  IMG_28.jpg  IMG_56.jpg  IMG_84.jpg\n",
            "IMG_10.jpg   IMG_138.jpg  IMG_166.jpg  IMG_29.jpg  IMG_57.jpg  IMG_85.jpg\n",
            "IMG_110.jpg  IMG_139.jpg  IMG_167.jpg  IMG_2.jpg   IMG_58.jpg  IMG_86.jpg\n",
            "IMG_111.jpg  IMG_13.jpg   IMG_168.jpg  IMG_30.jpg  IMG_59.jpg  IMG_87.jpg\n",
            "IMG_112.jpg  IMG_140.jpg  IMG_169.jpg  IMG_31.jpg  IMG_5.jpg   IMG_88.jpg\n",
            "IMG_113.jpg  IMG_141.jpg  IMG_16.jpg   IMG_32.jpg  IMG_60.jpg  IMG_89.jpg\n",
            "IMG_114.jpg  IMG_142.jpg  IMG_170.jpg  IMG_33.jpg  IMG_61.jpg  IMG_8.jpg\n",
            "IMG_115.jpg  IMG_143.jpg  IMG_171.jpg  IMG_34.jpg  IMG_62.jpg  IMG_90.jpg\n",
            "IMG_116.jpg  IMG_144.jpg  IMG_172.jpg  IMG_35.jpg  IMG_63.jpg  IMG_91.jpg\n",
            "IMG_117.jpg  IMG_145.jpg  IMG_173.jpg  IMG_36.jpg  IMG_64.jpg  IMG_92.jpg\n",
            "IMG_118.jpg  IMG_146.jpg  IMG_174.jpg  IMG_37.jpg  IMG_65.jpg  IMG_93.jpg\n",
            "IMG_119.jpg  IMG_147.jpg  IMG_175.jpg  IMG_38.jpg  IMG_66.jpg  IMG_94.jpg\n",
            "IMG_11.jpg   IMG_148.jpg  IMG_176.jpg  IMG_39.jpg  IMG_67.jpg  IMG_95.jpg\n",
            "IMG_120.jpg  IMG_149.jpg  IMG_177.jpg  IMG_3.jpg   IMG_68.jpg  IMG_96.jpg\n",
            "IMG_121.jpg  IMG_14.jpg   IMG_178.jpg  IMG_40.jpg  IMG_69.jpg  IMG_97.jpg\n",
            "IMG_122.jpg  IMG_150.jpg  IMG_179.jpg  IMG_41.jpg  IMG_6.jpg   IMG_98.jpg\n",
            "IMG_123.jpg  IMG_151.jpg  IMG_17.jpg   IMG_42.jpg  IMG_70.jpg  IMG_99.jpg\n",
            "IMG_124.jpg  IMG_152.jpg  IMG_180.jpg  IMG_43.jpg  IMG_71.jpg  IMG_9.jpg\n",
            "IMG_125.jpg  IMG_153.jpg  IMG_181.jpg  IMG_44.jpg  IMG_72.jpg\n",
            "IMG_126.jpg  IMG_154.jpg  IMG_182.jpg  IMG_45.jpg  IMG_73.jpg\n",
            "IMG_127.jpg  IMG_155.jpg  IMG_18.jpg   IMG_46.jpg  IMG_74.jpg\n",
            "IMG_128.jpg  IMG_156.jpg  IMG_19.jpg   IMG_47.jpg  IMG_75.jpg\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A/train_data:\n",
            "ground-truth  images\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A/train_data/ground-truth:\n",
            "GT_IMG_100.mat\tGT_IMG_155.mat\tGT_IMG_209.mat\tGT_IMG_263.mat\tGT_IMG_46.mat\n",
            "GT_IMG_101.mat\tGT_IMG_156.mat\tGT_IMG_20.mat\tGT_IMG_264.mat\tGT_IMG_47.mat\n",
            "GT_IMG_102.mat\tGT_IMG_157.mat\tGT_IMG_210.mat\tGT_IMG_265.mat\tGT_IMG_48.mat\n",
            "GT_IMG_103.mat\tGT_IMG_158.mat\tGT_IMG_211.mat\tGT_IMG_266.mat\tGT_IMG_49.mat\n",
            "GT_IMG_104.mat\tGT_IMG_159.mat\tGT_IMG_212.mat\tGT_IMG_267.mat\tGT_IMG_4.mat\n",
            "GT_IMG_105.mat\tGT_IMG_15.mat\tGT_IMG_213.mat\tGT_IMG_268.mat\tGT_IMG_50.mat\n",
            "GT_IMG_106.mat\tGT_IMG_160.mat\tGT_IMG_214.mat\tGT_IMG_269.mat\tGT_IMG_51.mat\n",
            "GT_IMG_107.mat\tGT_IMG_161.mat\tGT_IMG_215.mat\tGT_IMG_26.mat\tGT_IMG_52.mat\n",
            "GT_IMG_108.mat\tGT_IMG_162.mat\tGT_IMG_216.mat\tGT_IMG_270.mat\tGT_IMG_53.mat\n",
            "GT_IMG_109.mat\tGT_IMG_163.mat\tGT_IMG_217.mat\tGT_IMG_271.mat\tGT_IMG_54.mat\n",
            "GT_IMG_10.mat\tGT_IMG_164.mat\tGT_IMG_218.mat\tGT_IMG_272.mat\tGT_IMG_55.mat\n",
            "GT_IMG_110.mat\tGT_IMG_165.mat\tGT_IMG_219.mat\tGT_IMG_273.mat\tGT_IMG_56.mat\n",
            "GT_IMG_111.mat\tGT_IMG_166.mat\tGT_IMG_21.mat\tGT_IMG_274.mat\tGT_IMG_57.mat\n",
            "GT_IMG_112.mat\tGT_IMG_167.mat\tGT_IMG_220.mat\tGT_IMG_275.mat\tGT_IMG_58.mat\n",
            "GT_IMG_113.mat\tGT_IMG_168.mat\tGT_IMG_221.mat\tGT_IMG_276.mat\tGT_IMG_59.mat\n",
            "GT_IMG_114.mat\tGT_IMG_169.mat\tGT_IMG_222.mat\tGT_IMG_277.mat\tGT_IMG_5.mat\n",
            "GT_IMG_115.mat\tGT_IMG_16.mat\tGT_IMG_223.mat\tGT_IMG_278.mat\tGT_IMG_60.mat\n",
            "GT_IMG_116.mat\tGT_IMG_170.mat\tGT_IMG_224.mat\tGT_IMG_279.mat\tGT_IMG_61.mat\n",
            "GT_IMG_117.mat\tGT_IMG_171.mat\tGT_IMG_225.mat\tGT_IMG_27.mat\tGT_IMG_62.mat\n",
            "GT_IMG_118.mat\tGT_IMG_172.mat\tGT_IMG_226.mat\tGT_IMG_280.mat\tGT_IMG_63.mat\n",
            "GT_IMG_119.mat\tGT_IMG_173.mat\tGT_IMG_227.mat\tGT_IMG_281.mat\tGT_IMG_64.mat\n",
            "GT_IMG_11.mat\tGT_IMG_174.mat\tGT_IMG_228.mat\tGT_IMG_282.mat\tGT_IMG_65.mat\n",
            "GT_IMG_120.mat\tGT_IMG_175.mat\tGT_IMG_229.mat\tGT_IMG_283.mat\tGT_IMG_66.mat\n",
            "GT_IMG_121.mat\tGT_IMG_176.mat\tGT_IMG_22.mat\tGT_IMG_284.mat\tGT_IMG_67.mat\n",
            "GT_IMG_122.mat\tGT_IMG_177.mat\tGT_IMG_230.mat\tGT_IMG_285.mat\tGT_IMG_68.mat\n",
            "GT_IMG_123.mat\tGT_IMG_178.mat\tGT_IMG_231.mat\tGT_IMG_286.mat\tGT_IMG_69.mat\n",
            "GT_IMG_124.mat\tGT_IMG_179.mat\tGT_IMG_232.mat\tGT_IMG_287.mat\tGT_IMG_6.mat\n",
            "GT_IMG_125.mat\tGT_IMG_17.mat\tGT_IMG_233.mat\tGT_IMG_288.mat\tGT_IMG_70.mat\n",
            "GT_IMG_126.mat\tGT_IMG_180.mat\tGT_IMG_234.mat\tGT_IMG_289.mat\tGT_IMG_71.mat\n",
            "GT_IMG_127.mat\tGT_IMG_181.mat\tGT_IMG_235.mat\tGT_IMG_28.mat\tGT_IMG_72.mat\n",
            "GT_IMG_128.mat\tGT_IMG_182.mat\tGT_IMG_236.mat\tGT_IMG_290.mat\tGT_IMG_73.mat\n",
            "GT_IMG_129.mat\tGT_IMG_183.mat\tGT_IMG_237.mat\tGT_IMG_291.mat\tGT_IMG_74.mat\n",
            "GT_IMG_12.mat\tGT_IMG_184.mat\tGT_IMG_238.mat\tGT_IMG_292.mat\tGT_IMG_75.mat\n",
            "GT_IMG_130.mat\tGT_IMG_185.mat\tGT_IMG_239.mat\tGT_IMG_293.mat\tGT_IMG_76.mat\n",
            "GT_IMG_131.mat\tGT_IMG_186.mat\tGT_IMG_23.mat\tGT_IMG_294.mat\tGT_IMG_77.mat\n",
            "GT_IMG_132.mat\tGT_IMG_187.mat\tGT_IMG_240.mat\tGT_IMG_295.mat\tGT_IMG_78.mat\n",
            "GT_IMG_133.mat\tGT_IMG_188.mat\tGT_IMG_241.mat\tGT_IMG_296.mat\tGT_IMG_79.mat\n",
            "GT_IMG_134.mat\tGT_IMG_189.mat\tGT_IMG_242.mat\tGT_IMG_297.mat\tGT_IMG_7.mat\n",
            "GT_IMG_135.mat\tGT_IMG_18.mat\tGT_IMG_243.mat\tGT_IMG_298.mat\tGT_IMG_80.mat\n",
            "GT_IMG_136.mat\tGT_IMG_190.mat\tGT_IMG_244.mat\tGT_IMG_299.mat\tGT_IMG_81.mat\n",
            "GT_IMG_137.mat\tGT_IMG_191.mat\tGT_IMG_245.mat\tGT_IMG_29.mat\tGT_IMG_82.mat\n",
            "GT_IMG_138.mat\tGT_IMG_192.mat\tGT_IMG_246.mat\tGT_IMG_2.mat\tGT_IMG_83.mat\n",
            "GT_IMG_139.mat\tGT_IMG_193.mat\tGT_IMG_247.mat\tGT_IMG_300.mat\tGT_IMG_84.mat\n",
            "GT_IMG_13.mat\tGT_IMG_194.mat\tGT_IMG_248.mat\tGT_IMG_30.mat\tGT_IMG_85.mat\n",
            "GT_IMG_140.mat\tGT_IMG_195.mat\tGT_IMG_249.mat\tGT_IMG_31.mat\tGT_IMG_86.mat\n",
            "GT_IMG_141.mat\tGT_IMG_196.mat\tGT_IMG_24.mat\tGT_IMG_32.mat\tGT_IMG_87.mat\n",
            "GT_IMG_142.mat\tGT_IMG_197.mat\tGT_IMG_250.mat\tGT_IMG_33.mat\tGT_IMG_88.mat\n",
            "GT_IMG_143.mat\tGT_IMG_198.mat\tGT_IMG_251.mat\tGT_IMG_34.mat\tGT_IMG_89.mat\n",
            "GT_IMG_144.mat\tGT_IMG_199.mat\tGT_IMG_252.mat\tGT_IMG_35.mat\tGT_IMG_8.mat\n",
            "GT_IMG_145.mat\tGT_IMG_19.mat\tGT_IMG_253.mat\tGT_IMG_36.mat\tGT_IMG_90.mat\n",
            "GT_IMG_146.mat\tGT_IMG_1.mat\tGT_IMG_254.mat\tGT_IMG_37.mat\tGT_IMG_91.mat\n",
            "GT_IMG_147.mat\tGT_IMG_200.mat\tGT_IMG_255.mat\tGT_IMG_38.mat\tGT_IMG_92.mat\n",
            "GT_IMG_148.mat\tGT_IMG_201.mat\tGT_IMG_256.mat\tGT_IMG_39.mat\tGT_IMG_93.mat\n",
            "GT_IMG_149.mat\tGT_IMG_202.mat\tGT_IMG_257.mat\tGT_IMG_3.mat\tGT_IMG_94.mat\n",
            "GT_IMG_14.mat\tGT_IMG_203.mat\tGT_IMG_258.mat\tGT_IMG_40.mat\tGT_IMG_95.mat\n",
            "GT_IMG_150.mat\tGT_IMG_204.mat\tGT_IMG_259.mat\tGT_IMG_41.mat\tGT_IMG_96.mat\n",
            "GT_IMG_151.mat\tGT_IMG_205.mat\tGT_IMG_25.mat\tGT_IMG_42.mat\tGT_IMG_97.mat\n",
            "GT_IMG_152.mat\tGT_IMG_206.mat\tGT_IMG_260.mat\tGT_IMG_43.mat\tGT_IMG_98.mat\n",
            "GT_IMG_153.mat\tGT_IMG_207.mat\tGT_IMG_261.mat\tGT_IMG_44.mat\tGT_IMG_99.mat\n",
            "GT_IMG_154.mat\tGT_IMG_208.mat\tGT_IMG_262.mat\tGT_IMG_45.mat\tGT_IMG_9.mat\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_A/train_data/images:\n",
            "IMG_100.jpg  IMG_146.jpg  IMG_191.jpg  IMG_236.jpg  IMG_281.jpg  IMG_55.jpg\n",
            "IMG_101.jpg  IMG_147.jpg  IMG_192.jpg  IMG_237.jpg  IMG_282.jpg  IMG_56.jpg\n",
            "IMG_102.jpg  IMG_148.jpg  IMG_193.jpg  IMG_238.jpg  IMG_283.jpg  IMG_57.jpg\n",
            "IMG_103.jpg  IMG_149.jpg  IMG_194.jpg  IMG_239.jpg  IMG_284.jpg  IMG_58.jpg\n",
            "IMG_104.jpg  IMG_14.jpg   IMG_195.jpg  IMG_23.jpg   IMG_285.jpg  IMG_59.jpg\n",
            "IMG_105.jpg  IMG_150.jpg  IMG_196.jpg  IMG_240.jpg  IMG_286.jpg  IMG_5.jpg\n",
            "IMG_106.jpg  IMG_151.jpg  IMG_197.jpg  IMG_241.jpg  IMG_287.jpg  IMG_60.jpg\n",
            "IMG_107.jpg  IMG_152.jpg  IMG_198.jpg  IMG_242.jpg  IMG_288.jpg  IMG_61.jpg\n",
            "IMG_108.jpg  IMG_153.jpg  IMG_199.jpg  IMG_243.jpg  IMG_289.jpg  IMG_62.jpg\n",
            "IMG_109.jpg  IMG_154.jpg  IMG_19.jpg   IMG_244.jpg  IMG_28.jpg\t IMG_63.jpg\n",
            "IMG_10.jpg   IMG_155.jpg  IMG_1.jpg    IMG_245.jpg  IMG_290.jpg  IMG_64.jpg\n",
            "IMG_110.jpg  IMG_156.jpg  IMG_200.jpg  IMG_246.jpg  IMG_291.jpg  IMG_65.jpg\n",
            "IMG_111.jpg  IMG_157.jpg  IMG_201.jpg  IMG_247.jpg  IMG_292.jpg  IMG_66.jpg\n",
            "IMG_112.jpg  IMG_158.jpg  IMG_202.jpg  IMG_248.jpg  IMG_293.jpg  IMG_67.jpg\n",
            "IMG_113.jpg  IMG_159.jpg  IMG_203.jpg  IMG_249.jpg  IMG_294.jpg  IMG_68.jpg\n",
            "IMG_114.jpg  IMG_15.jpg   IMG_204.jpg  IMG_24.jpg   IMG_295.jpg  IMG_69.jpg\n",
            "IMG_115.jpg  IMG_160.jpg  IMG_205.jpg  IMG_250.jpg  IMG_296.jpg  IMG_6.jpg\n",
            "IMG_116.jpg  IMG_161.jpg  IMG_206.jpg  IMG_251.jpg  IMG_297.jpg  IMG_70.jpg\n",
            "IMG_117.jpg  IMG_162.jpg  IMG_207.jpg  IMG_252.jpg  IMG_298.jpg  IMG_71.jpg\n",
            "IMG_118.jpg  IMG_163.jpg  IMG_208.jpg  IMG_253.jpg  IMG_299.jpg  IMG_72.jpg\n",
            "IMG_119.jpg  IMG_164.jpg  IMG_209.jpg  IMG_254.jpg  IMG_29.jpg\t IMG_73.jpg\n",
            "IMG_11.jpg   IMG_165.jpg  IMG_20.jpg   IMG_255.jpg  IMG_2.jpg\t IMG_74.jpg\n",
            "IMG_120.jpg  IMG_166.jpg  IMG_210.jpg  IMG_256.jpg  IMG_300.jpg  IMG_75.jpg\n",
            "IMG_121.jpg  IMG_167.jpg  IMG_211.jpg  IMG_257.jpg  IMG_30.jpg\t IMG_76.jpg\n",
            "IMG_122.jpg  IMG_168.jpg  IMG_212.jpg  IMG_258.jpg  IMG_31.jpg\t IMG_77.jpg\n",
            "IMG_123.jpg  IMG_169.jpg  IMG_213.jpg  IMG_259.jpg  IMG_32.jpg\t IMG_78.jpg\n",
            "IMG_124.jpg  IMG_16.jpg   IMG_214.jpg  IMG_25.jpg   IMG_33.jpg\t IMG_79.jpg\n",
            "IMG_125.jpg  IMG_170.jpg  IMG_215.jpg  IMG_260.jpg  IMG_34.jpg\t IMG_7.jpg\n",
            "IMG_126.jpg  IMG_171.jpg  IMG_216.jpg  IMG_261.jpg  IMG_35.jpg\t IMG_80.jpg\n",
            "IMG_127.jpg  IMG_172.jpg  IMG_217.jpg  IMG_262.jpg  IMG_36.jpg\t IMG_81.jpg\n",
            "IMG_128.jpg  IMG_173.jpg  IMG_218.jpg  IMG_263.jpg  IMG_37.jpg\t IMG_82.jpg\n",
            "IMG_129.jpg  IMG_174.jpg  IMG_219.jpg  IMG_264.jpg  IMG_38.jpg\t IMG_83.jpg\n",
            "IMG_12.jpg   IMG_175.jpg  IMG_21.jpg   IMG_265.jpg  IMG_39.jpg\t IMG_84.jpg\n",
            "IMG_130.jpg  IMG_176.jpg  IMG_220.jpg  IMG_266.jpg  IMG_3.jpg\t IMG_85.jpg\n",
            "IMG_131.jpg  IMG_177.jpg  IMG_221.jpg  IMG_267.jpg  IMG_40.jpg\t IMG_86.jpg\n",
            "IMG_132.jpg  IMG_178.jpg  IMG_222.jpg  IMG_268.jpg  IMG_41.jpg\t IMG_87.jpg\n",
            "IMG_133.jpg  IMG_179.jpg  IMG_223.jpg  IMG_269.jpg  IMG_42.jpg\t IMG_88.jpg\n",
            "IMG_134.jpg  IMG_17.jpg   IMG_224.jpg  IMG_26.jpg   IMG_43.jpg\t IMG_89.jpg\n",
            "IMG_135.jpg  IMG_180.jpg  IMG_225.jpg  IMG_270.jpg  IMG_44.jpg\t IMG_8.jpg\n",
            "IMG_136.jpg  IMG_181.jpg  IMG_226.jpg  IMG_271.jpg  IMG_45.jpg\t IMG_90.jpg\n",
            "IMG_137.jpg  IMG_182.jpg  IMG_227.jpg  IMG_272.jpg  IMG_46.jpg\t IMG_91.jpg\n",
            "IMG_138.jpg  IMG_183.jpg  IMG_228.jpg  IMG_273.jpg  IMG_47.jpg\t IMG_92.jpg\n",
            "IMG_139.jpg  IMG_184.jpg  IMG_229.jpg  IMG_274.jpg  IMG_48.jpg\t IMG_93.jpg\n",
            "IMG_13.jpg   IMG_185.jpg  IMG_22.jpg   IMG_275.jpg  IMG_49.jpg\t IMG_94.jpg\n",
            "IMG_140.jpg  IMG_186.jpg  IMG_230.jpg  IMG_276.jpg  IMG_4.jpg\t IMG_95.jpg\n",
            "IMG_141.jpg  IMG_187.jpg  IMG_231.jpg  IMG_277.jpg  IMG_50.jpg\t IMG_96.jpg\n",
            "IMG_142.jpg  IMG_188.jpg  IMG_232.jpg  IMG_278.jpg  IMG_51.jpg\t IMG_97.jpg\n",
            "IMG_143.jpg  IMG_189.jpg  IMG_233.jpg  IMG_279.jpg  IMG_52.jpg\t IMG_98.jpg\n",
            "IMG_144.jpg  IMG_18.jpg   IMG_234.jpg  IMG_27.jpg   IMG_53.jpg\t IMG_99.jpg\n",
            "IMG_145.jpg  IMG_190.jpg  IMG_235.jpg  IMG_280.jpg  IMG_54.jpg\t IMG_9.jpg\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B:\n",
            "test_data  train_data\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/test_data:\n",
            "ground-truth  images\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/ground-truth:\n",
            "GT_IMG_100.mat\tGT_IMG_159.mat\tGT_IMG_216.mat\tGT_IMG_274.mat\tGT_IMG_46.mat\n",
            "GT_IMG_101.mat\tGT_IMG_15.mat\tGT_IMG_217.mat\tGT_IMG_275.mat\tGT_IMG_47.mat\n",
            "GT_IMG_102.mat\tGT_IMG_160.mat\tGT_IMG_218.mat\tGT_IMG_276.mat\tGT_IMG_48.mat\n",
            "GT_IMG_103.mat\tGT_IMG_161.mat\tGT_IMG_219.mat\tGT_IMG_277.mat\tGT_IMG_49.mat\n",
            "GT_IMG_104.mat\tGT_IMG_162.mat\tGT_IMG_21.mat\tGT_IMG_278.mat\tGT_IMG_4.mat\n",
            "GT_IMG_105.mat\tGT_IMG_163.mat\tGT_IMG_220.mat\tGT_IMG_279.mat\tGT_IMG_50.mat\n",
            "GT_IMG_106.mat\tGT_IMG_164.mat\tGT_IMG_221.mat\tGT_IMG_27.mat\tGT_IMG_51.mat\n",
            "GT_IMG_107.mat\tGT_IMG_165.mat\tGT_IMG_222.mat\tGT_IMG_280.mat\tGT_IMG_52.mat\n",
            "GT_IMG_108.mat\tGT_IMG_166.mat\tGT_IMG_223.mat\tGT_IMG_281.mat\tGT_IMG_53.mat\n",
            "GT_IMG_109.mat\tGT_IMG_167.mat\tGT_IMG_224.mat\tGT_IMG_282.mat\tGT_IMG_54.mat\n",
            "GT_IMG_10.mat\tGT_IMG_168.mat\tGT_IMG_225.mat\tGT_IMG_283.mat\tGT_IMG_55.mat\n",
            "GT_IMG_110.mat\tGT_IMG_169.mat\tGT_IMG_226.mat\tGT_IMG_284.mat\tGT_IMG_56.mat\n",
            "GT_IMG_111.mat\tGT_IMG_16.mat\tGT_IMG_227.mat\tGT_IMG_285.mat\tGT_IMG_57.mat\n",
            "GT_IMG_112.mat\tGT_IMG_170.mat\tGT_IMG_228.mat\tGT_IMG_286.mat\tGT_IMG_58.mat\n",
            "GT_IMG_113.mat\tGT_IMG_171.mat\tGT_IMG_229.mat\tGT_IMG_287.mat\tGT_IMG_59.mat\n",
            "GT_IMG_114.mat\tGT_IMG_172.mat\tGT_IMG_22.mat\tGT_IMG_288.mat\tGT_IMG_5.mat\n",
            "GT_IMG_115.mat\tGT_IMG_173.mat\tGT_IMG_230.mat\tGT_IMG_289.mat\tGT_IMG_60.mat\n",
            "GT_IMG_116.mat\tGT_IMG_174.mat\tGT_IMG_231.mat\tGT_IMG_28.mat\tGT_IMG_61.mat\n",
            "GT_IMG_117.mat\tGT_IMG_175.mat\tGT_IMG_232.mat\tGT_IMG_290.mat\tGT_IMG_62.mat\n",
            "GT_IMG_118.mat\tGT_IMG_176.mat\tGT_IMG_233.mat\tGT_IMG_291.mat\tGT_IMG_63.mat\n",
            "GT_IMG_119.mat\tGT_IMG_177.mat\tGT_IMG_234.mat\tGT_IMG_292.mat\tGT_IMG_64.mat\n",
            "GT_IMG_11.mat\tGT_IMG_178.mat\tGT_IMG_235.mat\tGT_IMG_293.mat\tGT_IMG_65.mat\n",
            "GT_IMG_120.mat\tGT_IMG_179.mat\tGT_IMG_236.mat\tGT_IMG_294.mat\tGT_IMG_66.mat\n",
            "GT_IMG_121.mat\tGT_IMG_17.mat\tGT_IMG_237.mat\tGT_IMG_295.mat\tGT_IMG_67.mat\n",
            "GT_IMG_122.mat\tGT_IMG_180.mat\tGT_IMG_238.mat\tGT_IMG_296.mat\tGT_IMG_68.mat\n",
            "GT_IMG_123.mat\tGT_IMG_181.mat\tGT_IMG_239.mat\tGT_IMG_297.mat\tGT_IMG_69.mat\n",
            "GT_IMG_124.mat\tGT_IMG_182.mat\tGT_IMG_23.mat\tGT_IMG_298.mat\tGT_IMG_6.mat\n",
            "GT_IMG_125.mat\tGT_IMG_183.mat\tGT_IMG_240.mat\tGT_IMG_299.mat\tGT_IMG_70.mat\n",
            "GT_IMG_126.mat\tGT_IMG_184.mat\tGT_IMG_241.mat\tGT_IMG_29.mat\tGT_IMG_71.mat\n",
            "GT_IMG_127.mat\tGT_IMG_185.mat\tGT_IMG_242.mat\tGT_IMG_2.mat\tGT_IMG_72.mat\n",
            "GT_IMG_128.mat\tGT_IMG_186.mat\tGT_IMG_243.mat\tGT_IMG_300.mat\tGT_IMG_73.mat\n",
            "GT_IMG_129.mat\tGT_IMG_187.mat\tGT_IMG_244.mat\tGT_IMG_301.mat\tGT_IMG_74.mat\n",
            "GT_IMG_12.mat\tGT_IMG_188.mat\tGT_IMG_245.mat\tGT_IMG_302.mat\tGT_IMG_75.mat\n",
            "GT_IMG_130.mat\tGT_IMG_189.mat\tGT_IMG_246.mat\tGT_IMG_303.mat\tGT_IMG_76.mat\n",
            "GT_IMG_131.mat\tGT_IMG_18.mat\tGT_IMG_247.mat\tGT_IMG_304.mat\tGT_IMG_77.mat\n",
            "GT_IMG_132.mat\tGT_IMG_190.mat\tGT_IMG_248.mat\tGT_IMG_305.mat\tGT_IMG_78.mat\n",
            "GT_IMG_133.mat\tGT_IMG_191.mat\tGT_IMG_249.mat\tGT_IMG_306.mat\tGT_IMG_79.mat\n",
            "GT_IMG_134.mat\tGT_IMG_192.mat\tGT_IMG_24.mat\tGT_IMG_307.mat\tGT_IMG_7.mat\n",
            "GT_IMG_135.mat\tGT_IMG_193.mat\tGT_IMG_250.mat\tGT_IMG_308.mat\tGT_IMG_80.mat\n",
            "GT_IMG_136.mat\tGT_IMG_194.mat\tGT_IMG_251.mat\tGT_IMG_309.mat\tGT_IMG_81.mat\n",
            "GT_IMG_137.mat\tGT_IMG_195.mat\tGT_IMG_252.mat\tGT_IMG_30.mat\tGT_IMG_82.mat\n",
            "GT_IMG_138.mat\tGT_IMG_196.mat\tGT_IMG_253.mat\tGT_IMG_310.mat\tGT_IMG_83.mat\n",
            "GT_IMG_139.mat\tGT_IMG_197.mat\tGT_IMG_254.mat\tGT_IMG_311.mat\tGT_IMG_84.mat\n",
            "GT_IMG_13.mat\tGT_IMG_198.mat\tGT_IMG_255.mat\tGT_IMG_312.mat\tGT_IMG_85.mat\n",
            "GT_IMG_140.mat\tGT_IMG_199.mat\tGT_IMG_256.mat\tGT_IMG_313.mat\tGT_IMG_86.mat\n",
            "GT_IMG_141.mat\tGT_IMG_19.mat\tGT_IMG_257.mat\tGT_IMG_314.mat\tGT_IMG_87.mat\n",
            "GT_IMG_142.mat\tGT_IMG_1.mat\tGT_IMG_258.mat\tGT_IMG_315.mat\tGT_IMG_88.mat\n",
            "GT_IMG_143.mat\tGT_IMG_200.mat\tGT_IMG_259.mat\tGT_IMG_316.mat\tGT_IMG_89.mat\n",
            "GT_IMG_144.mat\tGT_IMG_201.mat\tGT_IMG_25.mat\tGT_IMG_31.mat\tGT_IMG_8.mat\n",
            "GT_IMG_145.mat\tGT_IMG_202.mat\tGT_IMG_260.mat\tGT_IMG_32.mat\tGT_IMG_90.mat\n",
            "GT_IMG_146.mat\tGT_IMG_203.mat\tGT_IMG_261.mat\tGT_IMG_33.mat\tGT_IMG_91.mat\n",
            "GT_IMG_147.mat\tGT_IMG_204.mat\tGT_IMG_262.mat\tGT_IMG_34.mat\tGT_IMG_92.mat\n",
            "GT_IMG_148.mat\tGT_IMG_205.mat\tGT_IMG_263.mat\tGT_IMG_35.mat\tGT_IMG_93.mat\n",
            "GT_IMG_149.mat\tGT_IMG_206.mat\tGT_IMG_264.mat\tGT_IMG_36.mat\tGT_IMG_94.mat\n",
            "GT_IMG_14.mat\tGT_IMG_207.mat\tGT_IMG_265.mat\tGT_IMG_37.mat\tGT_IMG_95.mat\n",
            "GT_IMG_150.mat\tGT_IMG_208.mat\tGT_IMG_266.mat\tGT_IMG_38.mat\tGT_IMG_96.mat\n",
            "GT_IMG_151.mat\tGT_IMG_209.mat\tGT_IMG_267.mat\tGT_IMG_39.mat\tGT_IMG_97.mat\n",
            "GT_IMG_152.mat\tGT_IMG_20.mat\tGT_IMG_268.mat\tGT_IMG_3.mat\tGT_IMG_98.mat\n",
            "GT_IMG_153.mat\tGT_IMG_210.mat\tGT_IMG_269.mat\tGT_IMG_40.mat\tGT_IMG_99.mat\n",
            "GT_IMG_154.mat\tGT_IMG_211.mat\tGT_IMG_26.mat\tGT_IMG_41.mat\tGT_IMG_9.mat\n",
            "GT_IMG_155.mat\tGT_IMG_212.mat\tGT_IMG_270.mat\tGT_IMG_42.mat\n",
            "GT_IMG_156.mat\tGT_IMG_213.mat\tGT_IMG_271.mat\tGT_IMG_43.mat\n",
            "GT_IMG_157.mat\tGT_IMG_214.mat\tGT_IMG_272.mat\tGT_IMG_44.mat\n",
            "GT_IMG_158.mat\tGT_IMG_215.mat\tGT_IMG_273.mat\tGT_IMG_45.mat\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images:\n",
            "IMG_100.jpg  IMG_149.jpg  IMG_197.jpg  IMG_244.jpg  IMG_292.jpg  IMG_54.jpg\n",
            "IMG_101.jpg  IMG_14.jpg   IMG_198.jpg  IMG_245.jpg  IMG_293.jpg  IMG_55.jpg\n",
            "IMG_102.jpg  IMG_150.jpg  IMG_199.jpg  IMG_246.jpg  IMG_294.jpg  IMG_56.jpg\n",
            "IMG_103.jpg  IMG_151.jpg  IMG_19.jpg   IMG_247.jpg  IMG_295.jpg  IMG_57.jpg\n",
            "IMG_104.jpg  IMG_152.jpg  IMG_1.jpg    IMG_248.jpg  IMG_296.jpg  IMG_58.jpg\n",
            "IMG_105.jpg  IMG_153.jpg  IMG_200.jpg  IMG_249.jpg  IMG_297.jpg  IMG_59.jpg\n",
            "IMG_106.jpg  IMG_154.jpg  IMG_201.jpg  IMG_24.jpg   IMG_298.jpg  IMG_5.jpg\n",
            "IMG_107.jpg  IMG_155.jpg  IMG_202.jpg  IMG_250.jpg  IMG_299.jpg  IMG_60.jpg\n",
            "IMG_108.jpg  IMG_156.jpg  IMG_203.jpg  IMG_251.jpg  IMG_29.jpg\t IMG_61.jpg\n",
            "IMG_109.jpg  IMG_157.jpg  IMG_204.jpg  IMG_252.jpg  IMG_2.jpg\t IMG_62.jpg\n",
            "IMG_10.jpg   IMG_158.jpg  IMG_205.jpg  IMG_253.jpg  IMG_300.jpg  IMG_63.jpg\n",
            "IMG_110.jpg  IMG_159.jpg  IMG_206.jpg  IMG_254.jpg  IMG_301.jpg  IMG_64.jpg\n",
            "IMG_111.jpg  IMG_15.jpg   IMG_207.jpg  IMG_255.jpg  IMG_302.jpg  IMG_65.jpg\n",
            "IMG_112.jpg  IMG_160.jpg  IMG_208.jpg  IMG_256.jpg  IMG_303.jpg  IMG_66.jpg\n",
            "IMG_113.jpg  IMG_161.jpg  IMG_209.jpg  IMG_257.jpg  IMG_304.jpg  IMG_67.jpg\n",
            "IMG_114.jpg  IMG_162.jpg  IMG_20.jpg   IMG_258.jpg  IMG_305.jpg  IMG_68.jpg\n",
            "IMG_115.jpg  IMG_163.jpg  IMG_210.jpg  IMG_259.jpg  IMG_306.jpg  IMG_69.jpg\n",
            "IMG_116.jpg  IMG_164.jpg  IMG_211.jpg  IMG_25.jpg   IMG_307.jpg  IMG_6.jpg\n",
            "IMG_117.jpg  IMG_165.jpg  IMG_212.jpg  IMG_260.jpg  IMG_308.jpg  IMG_70.jpg\n",
            "IMG_118.jpg  IMG_166.jpg  IMG_213.jpg  IMG_261.jpg  IMG_309.jpg  IMG_71.jpg\n",
            "IMG_119.jpg  IMG_167.jpg  IMG_214.jpg  IMG_262.jpg  IMG_30.jpg\t IMG_72.jpg\n",
            "IMG_11.jpg   IMG_168.jpg  IMG_215.jpg  IMG_263.jpg  IMG_310.jpg  IMG_73.jpg\n",
            "IMG_120.jpg  IMG_169.jpg  IMG_216.jpg  IMG_264.jpg  IMG_311.jpg  IMG_74.jpg\n",
            "IMG_121.jpg  IMG_16.jpg   IMG_217.jpg  IMG_265.jpg  IMG_312.jpg  IMG_75.jpg\n",
            "IMG_122.jpg  IMG_170.jpg  IMG_218.jpg  IMG_266.jpg  IMG_313.jpg  IMG_76.jpg\n",
            "IMG_123.jpg  IMG_171.jpg  IMG_219.jpg  IMG_267.jpg  IMG_314.jpg  IMG_77.jpg\n",
            "IMG_124.jpg  IMG_172.jpg  IMG_21.jpg   IMG_268.jpg  IMG_315.jpg  IMG_78.jpg\n",
            "IMG_125.jpg  IMG_173.jpg  IMG_220.jpg  IMG_269.jpg  IMG_316.jpg  IMG_79.jpg\n",
            "IMG_126.jpg  IMG_174.jpg  IMG_221.jpg  IMG_26.jpg   IMG_31.jpg\t IMG_7.jpg\n",
            "IMG_127.jpg  IMG_175.jpg  IMG_222.jpg  IMG_270.jpg  IMG_32.jpg\t IMG_80.jpg\n",
            "IMG_128.jpg  IMG_176.jpg  IMG_223.jpg  IMG_271.jpg  IMG_33.jpg\t IMG_81.jpg\n",
            "IMG_129.jpg  IMG_177.jpg  IMG_224.jpg  IMG_272.jpg  IMG_34.jpg\t IMG_82.jpg\n",
            "IMG_12.jpg   IMG_178.jpg  IMG_225.jpg  IMG_273.jpg  IMG_35.jpg\t IMG_83.jpg\n",
            "IMG_130.jpg  IMG_179.jpg  IMG_226.jpg  IMG_274.jpg  IMG_36.jpg\t IMG_84.jpg\n",
            "IMG_131.jpg  IMG_17.jpg   IMG_227.jpg  IMG_275.jpg  IMG_37.jpg\t IMG_85.jpg\n",
            "IMG_132.jpg  IMG_180.jpg  IMG_228.jpg  IMG_276.jpg  IMG_38.jpg\t IMG_86.jpg\n",
            "IMG_133.jpg  IMG_181.jpg  IMG_229.jpg  IMG_277.jpg  IMG_39.jpg\t IMG_87.jpg\n",
            "IMG_134.jpg  IMG_182.jpg  IMG_22.jpg   IMG_278.jpg  IMG_3.jpg\t IMG_88.jpg\n",
            "IMG_135.jpg  IMG_183.jpg  IMG_230.jpg  IMG_279.jpg  IMG_40.jpg\t IMG_89.jpg\n",
            "IMG_136.jpg  IMG_184.jpg  IMG_231.jpg  IMG_27.jpg   IMG_41.jpg\t IMG_8.jpg\n",
            "IMG_137.jpg  IMG_185.jpg  IMG_232.jpg  IMG_280.jpg  IMG_42.jpg\t IMG_90.jpg\n",
            "IMG_138.jpg  IMG_186.jpg  IMG_233.jpg  IMG_281.jpg  IMG_43.jpg\t IMG_91.jpg\n",
            "IMG_139.jpg  IMG_187.jpg  IMG_234.jpg  IMG_282.jpg  IMG_44.jpg\t IMG_92.jpg\n",
            "IMG_13.jpg   IMG_188.jpg  IMG_235.jpg  IMG_283.jpg  IMG_45.jpg\t IMG_93.jpg\n",
            "IMG_140.jpg  IMG_189.jpg  IMG_236.jpg  IMG_284.jpg  IMG_46.jpg\t IMG_94.jpg\n",
            "IMG_141.jpg  IMG_18.jpg   IMG_237.jpg  IMG_285.jpg  IMG_47.jpg\t IMG_95.jpg\n",
            "IMG_142.jpg  IMG_190.jpg  IMG_238.jpg  IMG_286.jpg  IMG_48.jpg\t IMG_96.jpg\n",
            "IMG_143.jpg  IMG_191.jpg  IMG_239.jpg  IMG_287.jpg  IMG_49.jpg\t IMG_97.jpg\n",
            "IMG_144.jpg  IMG_192.jpg  IMG_23.jpg   IMG_288.jpg  IMG_4.jpg\t IMG_98.jpg\n",
            "IMG_145.jpg  IMG_193.jpg  IMG_240.jpg  IMG_289.jpg  IMG_50.jpg\t IMG_99.jpg\n",
            "IMG_146.jpg  IMG_194.jpg  IMG_241.jpg  IMG_28.jpg   IMG_51.jpg\t IMG_9.jpg\n",
            "IMG_147.jpg  IMG_195.jpg  IMG_242.jpg  IMG_290.jpg  IMG_52.jpg\n",
            "IMG_148.jpg  IMG_196.jpg  IMG_243.jpg  IMG_291.jpg  IMG_53.jpg\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/train_data:\n",
            "ground-truth  images\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/train_data/ground-truth:\n",
            "GT_IMG_100.mat\tGT_IMG_173.mat\tGT_IMG_245.mat\tGT_IMG_317.mat\tGT_IMG_38.mat\n",
            "GT_IMG_101.mat\tGT_IMG_174.mat\tGT_IMG_246.mat\tGT_IMG_318.mat\tGT_IMG_390.mat\n",
            "GT_IMG_102.mat\tGT_IMG_175.mat\tGT_IMG_247.mat\tGT_IMG_319.mat\tGT_IMG_391.mat\n",
            "GT_IMG_103.mat\tGT_IMG_176.mat\tGT_IMG_248.mat\tGT_IMG_31.mat\tGT_IMG_392.mat\n",
            "GT_IMG_104.mat\tGT_IMG_177.mat\tGT_IMG_249.mat\tGT_IMG_320.mat\tGT_IMG_393.mat\n",
            "GT_IMG_105.mat\tGT_IMG_178.mat\tGT_IMG_24.mat\tGT_IMG_321.mat\tGT_IMG_394.mat\n",
            "GT_IMG_106.mat\tGT_IMG_179.mat\tGT_IMG_250.mat\tGT_IMG_322.mat\tGT_IMG_395.mat\n",
            "GT_IMG_107.mat\tGT_IMG_17.mat\tGT_IMG_251.mat\tGT_IMG_323.mat\tGT_IMG_396.mat\n",
            "GT_IMG_108.mat\tGT_IMG_180.mat\tGT_IMG_252.mat\tGT_IMG_324.mat\tGT_IMG_397.mat\n",
            "GT_IMG_109.mat\tGT_IMG_181.mat\tGT_IMG_253.mat\tGT_IMG_325.mat\tGT_IMG_398.mat\n",
            "GT_IMG_10.mat\tGT_IMG_182.mat\tGT_IMG_254.mat\tGT_IMG_326.mat\tGT_IMG_399.mat\n",
            "GT_IMG_110.mat\tGT_IMG_183.mat\tGT_IMG_255.mat\tGT_IMG_327.mat\tGT_IMG_39.mat\n",
            "GT_IMG_111.mat\tGT_IMG_184.mat\tGT_IMG_256.mat\tGT_IMG_328.mat\tGT_IMG_3.mat\n",
            "GT_IMG_112.mat\tGT_IMG_185.mat\tGT_IMG_257.mat\tGT_IMG_329.mat\tGT_IMG_400.mat\n",
            "GT_IMG_113.mat\tGT_IMG_186.mat\tGT_IMG_258.mat\tGT_IMG_32.mat\tGT_IMG_40.mat\n",
            "GT_IMG_114.mat\tGT_IMG_187.mat\tGT_IMG_259.mat\tGT_IMG_330.mat\tGT_IMG_41.mat\n",
            "GT_IMG_115.mat\tGT_IMG_188.mat\tGT_IMG_25.mat\tGT_IMG_331.mat\tGT_IMG_42.mat\n",
            "GT_IMG_116.mat\tGT_IMG_189.mat\tGT_IMG_260.mat\tGT_IMG_332.mat\tGT_IMG_43.mat\n",
            "GT_IMG_117.mat\tGT_IMG_18.mat\tGT_IMG_261.mat\tGT_IMG_333.mat\tGT_IMG_44.mat\n",
            "GT_IMG_118.mat\tGT_IMG_190.mat\tGT_IMG_262.mat\tGT_IMG_334.mat\tGT_IMG_45.mat\n",
            "GT_IMG_119.mat\tGT_IMG_191.mat\tGT_IMG_263.mat\tGT_IMG_335.mat\tGT_IMG_46.mat\n",
            "GT_IMG_11.mat\tGT_IMG_192.mat\tGT_IMG_264.mat\tGT_IMG_336.mat\tGT_IMG_47.mat\n",
            "GT_IMG_120.mat\tGT_IMG_193.mat\tGT_IMG_265.mat\tGT_IMG_337.mat\tGT_IMG_48.mat\n",
            "GT_IMG_121.mat\tGT_IMG_194.mat\tGT_IMG_266.mat\tGT_IMG_338.mat\tGT_IMG_49.mat\n",
            "GT_IMG_122.mat\tGT_IMG_195.mat\tGT_IMG_267.mat\tGT_IMG_339.mat\tGT_IMG_4.mat\n",
            "GT_IMG_123.mat\tGT_IMG_196.mat\tGT_IMG_268.mat\tGT_IMG_33.mat\tGT_IMG_50.mat\n",
            "GT_IMG_124.mat\tGT_IMG_197.mat\tGT_IMG_269.mat\tGT_IMG_340.mat\tGT_IMG_51.mat\n",
            "GT_IMG_125.mat\tGT_IMG_198.mat\tGT_IMG_26.mat\tGT_IMG_341.mat\tGT_IMG_52.mat\n",
            "GT_IMG_126.mat\tGT_IMG_199.mat\tGT_IMG_270.mat\tGT_IMG_342.mat\tGT_IMG_53.mat\n",
            "GT_IMG_127.mat\tGT_IMG_19.mat\tGT_IMG_271.mat\tGT_IMG_343.mat\tGT_IMG_54.mat\n",
            "GT_IMG_128.mat\tGT_IMG_1.mat\tGT_IMG_272.mat\tGT_IMG_344.mat\tGT_IMG_55.mat\n",
            "GT_IMG_129.mat\tGT_IMG_200.mat\tGT_IMG_273.mat\tGT_IMG_345.mat\tGT_IMG_56.mat\n",
            "GT_IMG_12.mat\tGT_IMG_201.mat\tGT_IMG_274.mat\tGT_IMG_346.mat\tGT_IMG_57.mat\n",
            "GT_IMG_130.mat\tGT_IMG_202.mat\tGT_IMG_275.mat\tGT_IMG_347.mat\tGT_IMG_58.mat\n",
            "GT_IMG_131.mat\tGT_IMG_203.mat\tGT_IMG_276.mat\tGT_IMG_348.mat\tGT_IMG_59.mat\n",
            "GT_IMG_132.mat\tGT_IMG_204.mat\tGT_IMG_277.mat\tGT_IMG_349.mat\tGT_IMG_5.mat\n",
            "GT_IMG_133.mat\tGT_IMG_205.mat\tGT_IMG_278.mat\tGT_IMG_34.mat\tGT_IMG_60.mat\n",
            "GT_IMG_134.mat\tGT_IMG_206.mat\tGT_IMG_279.mat\tGT_IMG_350.mat\tGT_IMG_61.mat\n",
            "GT_IMG_135.mat\tGT_IMG_207.mat\tGT_IMG_27.mat\tGT_IMG_351.mat\tGT_IMG_62.mat\n",
            "GT_IMG_136.mat\tGT_IMG_208.mat\tGT_IMG_280.mat\tGT_IMG_352.mat\tGT_IMG_63.mat\n",
            "GT_IMG_137.mat\tGT_IMG_209.mat\tGT_IMG_281.mat\tGT_IMG_353.mat\tGT_IMG_64.mat\n",
            "GT_IMG_138.mat\tGT_IMG_20.mat\tGT_IMG_282.mat\tGT_IMG_354.mat\tGT_IMG_65.mat\n",
            "GT_IMG_139.mat\tGT_IMG_210.mat\tGT_IMG_283.mat\tGT_IMG_355.mat\tGT_IMG_66.mat\n",
            "GT_IMG_13.mat\tGT_IMG_211.mat\tGT_IMG_284.mat\tGT_IMG_356.mat\tGT_IMG_67.mat\n",
            "GT_IMG_140.mat\tGT_IMG_212.mat\tGT_IMG_285.mat\tGT_IMG_357.mat\tGT_IMG_68.mat\n",
            "GT_IMG_141.mat\tGT_IMG_213.mat\tGT_IMG_286.mat\tGT_IMG_358.mat\tGT_IMG_69.mat\n",
            "GT_IMG_142.mat\tGT_IMG_214.mat\tGT_IMG_287.mat\tGT_IMG_359.mat\tGT_IMG_6.mat\n",
            "GT_IMG_143.mat\tGT_IMG_215.mat\tGT_IMG_288.mat\tGT_IMG_35.mat\tGT_IMG_70.mat\n",
            "GT_IMG_144.mat\tGT_IMG_216.mat\tGT_IMG_289.mat\tGT_IMG_360.mat\tGT_IMG_71.mat\n",
            "GT_IMG_145.mat\tGT_IMG_217.mat\tGT_IMG_28.mat\tGT_IMG_361.mat\tGT_IMG_72.mat\n",
            "GT_IMG_146.mat\tGT_IMG_218.mat\tGT_IMG_290.mat\tGT_IMG_362.mat\tGT_IMG_73.mat\n",
            "GT_IMG_147.mat\tGT_IMG_219.mat\tGT_IMG_291.mat\tGT_IMG_363.mat\tGT_IMG_74.mat\n",
            "GT_IMG_148.mat\tGT_IMG_21.mat\tGT_IMG_292.mat\tGT_IMG_364.mat\tGT_IMG_75.mat\n",
            "GT_IMG_149.mat\tGT_IMG_220.mat\tGT_IMG_293.mat\tGT_IMG_365.mat\tGT_IMG_76.mat\n",
            "GT_IMG_14.mat\tGT_IMG_221.mat\tGT_IMG_294.mat\tGT_IMG_366.mat\tGT_IMG_77.mat\n",
            "GT_IMG_150.mat\tGT_IMG_222.mat\tGT_IMG_295.mat\tGT_IMG_367.mat\tGT_IMG_78.mat\n",
            "GT_IMG_151.mat\tGT_IMG_223.mat\tGT_IMG_296.mat\tGT_IMG_368.mat\tGT_IMG_79.mat\n",
            "GT_IMG_152.mat\tGT_IMG_224.mat\tGT_IMG_297.mat\tGT_IMG_369.mat\tGT_IMG_7.mat\n",
            "GT_IMG_153.mat\tGT_IMG_225.mat\tGT_IMG_298.mat\tGT_IMG_36.mat\tGT_IMG_80.mat\n",
            "GT_IMG_154.mat\tGT_IMG_226.mat\tGT_IMG_299.mat\tGT_IMG_370.mat\tGT_IMG_81.mat\n",
            "GT_IMG_155.mat\tGT_IMG_227.mat\tGT_IMG_29.mat\tGT_IMG_371.mat\tGT_IMG_82.mat\n",
            "GT_IMG_156.mat\tGT_IMG_228.mat\tGT_IMG_2.mat\tGT_IMG_372.mat\tGT_IMG_83.mat\n",
            "GT_IMG_157.mat\tGT_IMG_229.mat\tGT_IMG_300.mat\tGT_IMG_373.mat\tGT_IMG_84.mat\n",
            "GT_IMG_158.mat\tGT_IMG_22.mat\tGT_IMG_301.mat\tGT_IMG_374.mat\tGT_IMG_85.mat\n",
            "GT_IMG_159.mat\tGT_IMG_230.mat\tGT_IMG_302.mat\tGT_IMG_375.mat\tGT_IMG_86.mat\n",
            "GT_IMG_15.mat\tGT_IMG_231.mat\tGT_IMG_303.mat\tGT_IMG_376.mat\tGT_IMG_87.mat\n",
            "GT_IMG_160.mat\tGT_IMG_232.mat\tGT_IMG_304.mat\tGT_IMG_377.mat\tGT_IMG_88.mat\n",
            "GT_IMG_161.mat\tGT_IMG_233.mat\tGT_IMG_305.mat\tGT_IMG_378.mat\tGT_IMG_89.mat\n",
            "GT_IMG_162.mat\tGT_IMG_234.mat\tGT_IMG_306.mat\tGT_IMG_379.mat\tGT_IMG_8.mat\n",
            "GT_IMG_163.mat\tGT_IMG_235.mat\tGT_IMG_307.mat\tGT_IMG_37.mat\tGT_IMG_90.mat\n",
            "GT_IMG_164.mat\tGT_IMG_236.mat\tGT_IMG_308.mat\tGT_IMG_380.mat\tGT_IMG_91.mat\n",
            "GT_IMG_165.mat\tGT_IMG_237.mat\tGT_IMG_309.mat\tGT_IMG_381.mat\tGT_IMG_92.mat\n",
            "GT_IMG_166.mat\tGT_IMG_238.mat\tGT_IMG_30.mat\tGT_IMG_382.mat\tGT_IMG_93.mat\n",
            "GT_IMG_167.mat\tGT_IMG_239.mat\tGT_IMG_310.mat\tGT_IMG_383.mat\tGT_IMG_94.mat\n",
            "GT_IMG_168.mat\tGT_IMG_23.mat\tGT_IMG_311.mat\tGT_IMG_384.mat\tGT_IMG_95.mat\n",
            "GT_IMG_169.mat\tGT_IMG_240.mat\tGT_IMG_312.mat\tGT_IMG_385.mat\tGT_IMG_96.mat\n",
            "GT_IMG_16.mat\tGT_IMG_241.mat\tGT_IMG_313.mat\tGT_IMG_386.mat\tGT_IMG_97.mat\n",
            "GT_IMG_170.mat\tGT_IMG_242.mat\tGT_IMG_314.mat\tGT_IMG_387.mat\tGT_IMG_98.mat\n",
            "GT_IMG_171.mat\tGT_IMG_243.mat\tGT_IMG_315.mat\tGT_IMG_388.mat\tGT_IMG_99.mat\n",
            "GT_IMG_172.mat\tGT_IMG_244.mat\tGT_IMG_316.mat\tGT_IMG_389.mat\tGT_IMG_9.mat\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/train_data/images:\n",
            "IMG_100.jpg  IMG_161.jpg  IMG_221.jpg  IMG_282.jpg  IMG_342.jpg  IMG_41.jpg\n",
            "IMG_101.jpg  IMG_162.jpg  IMG_222.jpg  IMG_283.jpg  IMG_343.jpg  IMG_42.jpg\n",
            "IMG_102.jpg  IMG_163.jpg  IMG_223.jpg  IMG_284.jpg  IMG_344.jpg  IMG_43.jpg\n",
            "IMG_103.jpg  IMG_164.jpg  IMG_224.jpg  IMG_285.jpg  IMG_345.jpg  IMG_44.jpg\n",
            "IMG_104.jpg  IMG_165.jpg  IMG_225.jpg  IMG_286.jpg  IMG_346.jpg  IMG_45.jpg\n",
            "IMG_105.jpg  IMG_166.jpg  IMG_226.jpg  IMG_287.jpg  IMG_347.jpg  IMG_46.jpg\n",
            "IMG_106.jpg  IMG_167.jpg  IMG_227.jpg  IMG_288.jpg  IMG_348.jpg  IMG_47.jpg\n",
            "IMG_107.jpg  IMG_168.jpg  IMG_228.jpg  IMG_289.jpg  IMG_349.jpg  IMG_48.jpg\n",
            "IMG_108.jpg  IMG_169.jpg  IMG_229.jpg  IMG_28.jpg   IMG_34.jpg\t IMG_49.jpg\n",
            "IMG_109.jpg  IMG_16.jpg   IMG_22.jpg   IMG_290.jpg  IMG_350.jpg  IMG_4.jpg\n",
            "IMG_10.jpg   IMG_170.jpg  IMG_230.jpg  IMG_291.jpg  IMG_351.jpg  IMG_50.jpg\n",
            "IMG_110.jpg  IMG_171.jpg  IMG_231.jpg  IMG_292.jpg  IMG_352.jpg  IMG_51.jpg\n",
            "IMG_111.jpg  IMG_172.jpg  IMG_232.jpg  IMG_293.jpg  IMG_353.jpg  IMG_52.jpg\n",
            "IMG_112.jpg  IMG_173.jpg  IMG_233.jpg  IMG_294.jpg  IMG_354.jpg  IMG_53.jpg\n",
            "IMG_113.jpg  IMG_174.jpg  IMG_234.jpg  IMG_295.jpg  IMG_355.jpg  IMG_54.jpg\n",
            "IMG_114.jpg  IMG_175.jpg  IMG_235.jpg  IMG_296.jpg  IMG_356.jpg  IMG_55.jpg\n",
            "IMG_115.jpg  IMG_176.jpg  IMG_236.jpg  IMG_297.jpg  IMG_357.jpg  IMG_56.jpg\n",
            "IMG_116.jpg  IMG_177.jpg  IMG_237.jpg  IMG_298.jpg  IMG_358.jpg  IMG_57.jpg\n",
            "IMG_117.jpg  IMG_178.jpg  IMG_238.jpg  IMG_299.jpg  IMG_359.jpg  IMG_58.jpg\n",
            "IMG_118.jpg  IMG_179.jpg  IMG_239.jpg  IMG_29.jpg   IMG_35.jpg\t IMG_59.jpg\n",
            "IMG_119.jpg  IMG_17.jpg   IMG_23.jpg   IMG_2.jpg    IMG_360.jpg  IMG_5.jpg\n",
            "IMG_11.jpg   IMG_180.jpg  IMG_240.jpg  IMG_300.jpg  IMG_361.jpg  IMG_60.jpg\n",
            "IMG_120.jpg  IMG_181.jpg  IMG_241.jpg  IMG_301.jpg  IMG_362.jpg  IMG_61.jpg\n",
            "IMG_121.jpg  IMG_182.jpg  IMG_242.jpg  IMG_302.jpg  IMG_363.jpg  IMG_62.jpg\n",
            "IMG_122.jpg  IMG_183.jpg  IMG_243.jpg  IMG_303.jpg  IMG_364.jpg  IMG_63.jpg\n",
            "IMG_123.jpg  IMG_184.jpg  IMG_244.jpg  IMG_304.jpg  IMG_365.jpg  IMG_64.jpg\n",
            "IMG_124.jpg  IMG_185.jpg  IMG_245.jpg  IMG_305.jpg  IMG_366.jpg  IMG_65.jpg\n",
            "IMG_125.jpg  IMG_186.jpg  IMG_246.jpg  IMG_306.jpg  IMG_367.jpg  IMG_66.jpg\n",
            "IMG_126.jpg  IMG_187.jpg  IMG_247.jpg  IMG_307.jpg  IMG_368.jpg  IMG_67.jpg\n",
            "IMG_127.jpg  IMG_188.jpg  IMG_248.jpg  IMG_308.jpg  IMG_369.jpg  IMG_68.jpg\n",
            "IMG_128.jpg  IMG_189.jpg  IMG_249.jpg  IMG_309.jpg  IMG_36.jpg\t IMG_69.jpg\n",
            "IMG_129.jpg  IMG_18.jpg   IMG_24.jpg   IMG_30.jpg   IMG_370.jpg  IMG_6.jpg\n",
            "IMG_12.jpg   IMG_190.jpg  IMG_250.jpg  IMG_310.jpg  IMG_371.jpg  IMG_70.jpg\n",
            "IMG_130.jpg  IMG_191.jpg  IMG_251.jpg  IMG_311.jpg  IMG_372.jpg  IMG_71.jpg\n",
            "IMG_131.jpg  IMG_192.jpg  IMG_252.jpg  IMG_312.jpg  IMG_373.jpg  IMG_72.jpg\n",
            "IMG_132.jpg  IMG_193.jpg  IMG_253.jpg  IMG_313.jpg  IMG_374.jpg  IMG_73.jpg\n",
            "IMG_133.jpg  IMG_194.jpg  IMG_254.jpg  IMG_314.jpg  IMG_375.jpg  IMG_74.jpg\n",
            "IMG_134.jpg  IMG_195.jpg  IMG_255.jpg  IMG_315.jpg  IMG_376.jpg  IMG_75.jpg\n",
            "IMG_135.jpg  IMG_196.jpg  IMG_256.jpg  IMG_316.jpg  IMG_377.jpg  IMG_76.jpg\n",
            "IMG_136.jpg  IMG_197.jpg  IMG_257.jpg  IMG_317.jpg  IMG_378.jpg  IMG_77.jpg\n",
            "IMG_137.jpg  IMG_198.jpg  IMG_258.jpg  IMG_318.jpg  IMG_379.jpg  IMG_78.jpg\n",
            "IMG_138.jpg  IMG_199.jpg  IMG_259.jpg  IMG_319.jpg  IMG_37.jpg\t IMG_79.jpg\n",
            "IMG_139.jpg  IMG_19.jpg   IMG_25.jpg   IMG_31.jpg   IMG_380.jpg  IMG_7.jpg\n",
            "IMG_13.jpg   IMG_1.jpg\t  IMG_260.jpg  IMG_320.jpg  IMG_381.jpg  IMG_80.jpg\n",
            "IMG_140.jpg  IMG_200.jpg  IMG_261.jpg  IMG_321.jpg  IMG_382.jpg  IMG_81.jpg\n",
            "IMG_141.jpg  IMG_201.jpg  IMG_262.jpg  IMG_322.jpg  IMG_383.jpg  IMG_82.jpg\n",
            "IMG_142.jpg  IMG_202.jpg  IMG_263.jpg  IMG_323.jpg  IMG_384.jpg  IMG_83.jpg\n",
            "IMG_143.jpg  IMG_203.jpg  IMG_264.jpg  IMG_324.jpg  IMG_385.jpg  IMG_84.jpg\n",
            "IMG_144.jpg  IMG_204.jpg  IMG_265.jpg  IMG_325.jpg  IMG_386.jpg  IMG_85.jpg\n",
            "IMG_145.jpg  IMG_205.jpg  IMG_266.jpg  IMG_326.jpg  IMG_387.jpg  IMG_86.jpg\n",
            "IMG_146.jpg  IMG_206.jpg  IMG_267.jpg  IMG_327.jpg  IMG_388.jpg  IMG_87.jpg\n",
            "IMG_147.jpg  IMG_207.jpg  IMG_268.jpg  IMG_328.jpg  IMG_389.jpg  IMG_88.jpg\n",
            "IMG_148.jpg  IMG_208.jpg  IMG_269.jpg  IMG_329.jpg  IMG_38.jpg\t IMG_89.jpg\n",
            "IMG_149.jpg  IMG_209.jpg  IMG_26.jpg   IMG_32.jpg   IMG_390.jpg  IMG_8.jpg\n",
            "IMG_14.jpg   IMG_20.jpg   IMG_270.jpg  IMG_330.jpg  IMG_391.jpg  IMG_90.jpg\n",
            "IMG_150.jpg  IMG_210.jpg  IMG_271.jpg  IMG_331.jpg  IMG_392.jpg  IMG_91.jpg\n",
            "IMG_151.jpg  IMG_211.jpg  IMG_272.jpg  IMG_332.jpg  IMG_393.jpg  IMG_92.jpg\n",
            "IMG_152.jpg  IMG_212.jpg  IMG_273.jpg  IMG_333.jpg  IMG_394.jpg  IMG_93.jpg\n",
            "IMG_153.jpg  IMG_213.jpg  IMG_274.jpg  IMG_334.jpg  IMG_395.jpg  IMG_94.jpg\n",
            "IMG_154.jpg  IMG_214.jpg  IMG_275.jpg  IMG_335.jpg  IMG_396.jpg  IMG_95.jpg\n",
            "IMG_155.jpg  IMG_215.jpg  IMG_276.jpg  IMG_336.jpg  IMG_397.jpg  IMG_96.jpg\n",
            "IMG_156.jpg  IMG_216.jpg  IMG_277.jpg  IMG_337.jpg  IMG_398.jpg  IMG_97.jpg\n",
            "IMG_157.jpg  IMG_217.jpg  IMG_278.jpg  IMG_338.jpg  IMG_399.jpg  IMG_98.jpg\n",
            "IMG_158.jpg  IMG_218.jpg  IMG_279.jpg  IMG_339.jpg  IMG_39.jpg\t IMG_99.jpg\n",
            "IMG_159.jpg  IMG_219.jpg  IMG_27.jpg   IMG_33.jpg   IMG_3.jpg\t IMG_9.jpg\n",
            "IMG_15.jpg   IMG_21.jpg   IMG_280.jpg  IMG_340.jpg  IMG_400.jpg\n",
            "IMG_160.jpg  IMG_220.jpg  IMG_281.jpg  IMG_341.jpg  IMG_40.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Create density maps for ShanghaiTech part_B =====\n",
        "# Run this in your Colab where /content/DEEPVISION is mounted (same environment you used before).\n",
        "\n",
        "import os, math, cv2, numpy as np, scipy.io\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "# Paths (already detected in your tree)\n",
        "IMG_DIR_TRAIN = \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/train_data/images\"\n",
        "IMG_DIR_TEST  = \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images\"\n",
        "GT_DIR_TRAIN  = \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/train_data/ground-truth\"\n",
        "GT_DIR_TEST   = \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/ground-truth\"\n",
        "\n",
        "OUT_DENS_DIR = \"/content/DEEPVISION/DEEPVISION/DATA/processed/part_B/density\"\n",
        "\n",
        "os.makedirs(OUT_DENS_DIR, exist_ok=True)\n",
        "print(\"Writing densities to:\", OUT_DENS_DIR)\n",
        "\n",
        "def load_mat_points(matpath):\n",
        "    \"\"\"Try to load point coordinates from a ground-truth .mat file used by ShanghaiTech.\n",
        "       Return an (N,2) float array of (x,y) or None if not found.\"\"\"\n",
        "    m = scipy.io.loadmat(matpath)\n",
        "    # Common keys: 'image_info', 'annPoints', 'dots', 'object_pos', 'position', 'head'\n",
        "    # Try a few heuristics:\n",
        "    if 'annPoints' in m:\n",
        "        pts = np.array(m['annPoints'])\n",
        "        # sometimes annPoints already Nx2 (x,y)\n",
        "        if pts.ndim == 2 and pts.shape[1] == 2:\n",
        "            return pts.astype(np.float32)\n",
        "    if 'image_info' in m:\n",
        "        info = m['image_info']\n",
        "        # image_info may be a struct: info[0,0]['location'] or similar\n",
        "        try:\n",
        "            # navigate nested struct\n",
        "            if isinstance(info, np.ndarray):\n",
        "                # find any subfields that look like points\n",
        "                for el in info.flatten():\n",
        "                    if isinstance(el, np.ndarray):\n",
        "                        for f in el.dtype.names or []:\n",
        "                            arr = el[f]\n",
        "                            if isinstance(arr, np.ndarray) and arr.size>0:\n",
        "                                arr2 = np.array(arr)\n",
        "                                if arr2.ndim == 2 and arr2.shape[1] == 2:\n",
        "                                    return arr2.astype(np.float32)\n",
        "        except Exception:\n",
        "            pass\n",
        "    # fallback: search for any array that looks like Nx2\n",
        "    for v in m.values():\n",
        "        if isinstance(v, np.ndarray) and v.ndim==2 and v.shape[1]==2 and v.size>0 and v.shape[0]<5000:\n",
        "            return v.astype(np.float32)\n",
        "    return None\n",
        "\n",
        "def make_density_map(img_h, img_w, points, sigma=15):\n",
        "    den = np.zeros((img_h, img_w), dtype=np.float32)\n",
        "    if points is None or len(points)==0:\n",
        "        return den\n",
        "    # points are (x,y). We convert to integer pixel positions safely.\n",
        "    for (x,y) in points:\n",
        "        # coords in GT are usually 1-indexed or float - convert\n",
        "        px = int(round(float(x)))  # x: column\n",
        "        py = int(round(float(y)))  # y: row\n",
        "        if px < 0 or py < 0 or px >= img_w or py >= img_h:\n",
        "            # skip out of bounds\n",
        "            continue\n",
        "        den[py, px] += 1.0\n",
        "    # blur with gaussian to spread each head\n",
        "    # sigma in pixels: you can tune (15 is typical for ShanghaiTech)\n",
        "    den = gaussian_filter(den, sigma=sigma, mode='constant')\n",
        "    return den\n",
        "\n",
        "def process_folder(img_dir, gt_dir, out_dir):\n",
        "    img_files = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg','.png'))])\n",
        "    skipped = []\n",
        "    for imgname in tqdm(img_files, desc=f\"processing {os.path.basename(img_dir)}\"):\n",
        "        imgpath = os.path.join(img_dir, imgname)\n",
        "        base = os.path.splitext(imgname)[0]  # e.g. IMG_100\n",
        "        # ground truth usually called GT_IMG_100.mat\n",
        "        gtname1 = \"GT_\" + base + \".mat\"\n",
        "        gtpath = os.path.join(gt_dir, gtname1)\n",
        "        if not os.path.exists(gtpath):\n",
        "            # sometimes naming may be different, try alternative\n",
        "            # try to extract number and search\n",
        "            import re\n",
        "            m = re.search(r'(\\d+)', base)\n",
        "            found = False\n",
        "            if m:\n",
        "                num = m.group(1)\n",
        "                # look for any GT file containing the number\n",
        "                for candidate in os.listdir(gt_dir):\n",
        "                    if num in candidate and candidate.lower().endswith('.mat'):\n",
        "                        gtpath = os.path.join(gt_dir, candidate)\n",
        "                        found = True\n",
        "                        break\n",
        "            if not found and not os.path.exists(gtpath):\n",
        "                skipped.append((imgname, \"gt not found\"))\n",
        "                continue\n",
        "        # load image to get size\n",
        "        img = cv2.imread(imgpath)\n",
        "        if img is None:\n",
        "            skipped.append((imgname, \"image load failed\"))\n",
        "            continue\n",
        "        h,w = img.shape[:2]\n",
        "        pts = load_mat_points(gtpath)\n",
        "        if pts is None:\n",
        "            # no points found - skip\n",
        "            skipped.append((imgname, \"no points in mat\"))\n",
        "            continue\n",
        "        den = make_density_map(h, w, pts, sigma=15)\n",
        "        outpath = os.path.join(out_dir, base + \".npy\")\n",
        "        np.save(outpath, den.astype(np.float32))\n",
        "    return skipped\n",
        "\n",
        "# Run for train and test (will skip files already present)\n",
        "skipped_train = process_folder(IMG_DIR_TRAIN, GT_DIR_TRAIN, OUT_DENS_DIR)\n",
        "skipped_test  = process_folder(IMG_DIR_TEST,  GT_DIR_TEST,  OUT_DENS_DIR)\n",
        "\n",
        "print(\"Done. skipped_train:\", len(skipped_train), \"skipped_test:\", len(skipped_test))\n",
        "if skipped_train or skipped_test:\n",
        "    print(\"Examples skipped (up to 10):\", (skipped_train + skipped_test)[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQEO_g9GJ9mH",
        "outputId": "95d5897b-8308-4c7c-cabf-b56e3fb18c9c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing densities to: /content/DEEPVISION/DEEPVISION/DATA/processed/part_B/density\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "processing images: 100%|| 400/400 [00:03<00:00, 129.11it/s]\n",
            "processing images: 100%|| 316/316 [00:01<00:00, 207.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. skipped_train: 400 skipped_test: 316\n",
            "Examples skipped (up to 10): [('IMG_1.jpg', 'no points in mat'), ('IMG_10.jpg', 'no points in mat'), ('IMG_100.jpg', 'no points in mat'), ('IMG_101.jpg', 'no points in mat'), ('IMG_102.jpg', 'no points in mat'), ('IMG_103.jpg', 'no points in mat'), ('IMG_104.jpg', 'no points in mat'), ('IMG_105.jpg', 'no points in mat'), ('IMG_106.jpg', 'no points in mat'), ('IMG_107.jpg', 'no points in mat')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R \"/content/DEEPVISION/DEEPVISION/DATA/processed/part_B\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW6wGzUZKRiZ",
        "outputId": "432ecb89-dd06-4161-8134-1668045d6e72"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DEEPVISION/DEEPVISION/DATA/processed/part_B:\n",
            "density\n",
            "\n",
            "/content/DEEPVISION/DEEPVISION/DATA/processed/part_B/density:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, scipy.io, numpy as np\n",
        "from tqdm import tqdm\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "# Paths\n",
        "PART_B_TRAIN_IMG = \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/train_data/images\"\n",
        "PART_B_TRAIN_GT  = \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/train_data/ground-truth\"\n",
        "\n",
        "PART_B_TEST_IMG  = \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images\"\n",
        "PART_B_TEST_GT   = \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/ground-truth\"\n",
        "\n",
        "OUT = \"/content/DEEPVISION/DEEPVISION/DATA/processed/part_B/density\"\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "def extract_points(mat_path):\n",
        "    m = scipy.io.loadmat(mat_path)\n",
        "\n",
        "    # Case 1  Some datasets include annPoints\n",
        "    if \"annPoints\" in m:\n",
        "        pts = m[\"annPoints\"]\n",
        "        return pts.astype(float)\n",
        "\n",
        "    # Case 2  ShanghaiTech Part-B default\n",
        "    try:\n",
        "        pts = m[\"image_info\"][0][0][0][0][0]\n",
        "        if pts.shape[1] == 2:\n",
        "            return pts.astype(float)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Case 3  fallback for any Nx2 array in MAT\n",
        "    for v in m.values():\n",
        "        if isinstance(v, np.ndarray) and v.ndim == 2 and v.shape[1] == 2:\n",
        "            return v.astype(float)\n",
        "\n",
        "    return None\n",
        "\n",
        "def generate(img_dir, gt_dir):\n",
        "    errors = []\n",
        "    for img_name in tqdm(sorted(os.listdir(img_dir))):\n",
        "        if not img_name.lower().endswith(\".jpg\"):\n",
        "            continue\n",
        "\n",
        "        img_stem = img_name.split(\".\")[0]  # IMG_XXX\n",
        "        gt_file = f\"GT_{img_stem}.mat\"\n",
        "        gt_path = os.path.join(gt_dir, gt_file)\n",
        "\n",
        "        if not os.path.exists(gt_path):\n",
        "            errors.append((img_name, \"GT not found\"))\n",
        "            continue\n",
        "\n",
        "        img = cv2.imread(os.path.join(img_dir, img_name))\n",
        "        if img is None:\n",
        "            errors.append((img_name, \"Image load failed\"))\n",
        "            continue\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        pts = extract_points(gt_path)\n",
        "        if pts is None:\n",
        "            errors.append((img_name, \"Could not extract points\"))\n",
        "            continue\n",
        "\n",
        "        density = np.zeros((h, w), np.float32)\n",
        "\n",
        "        for x, y in pts:\n",
        "            x = int(round(x))\n",
        "            y = int(round(y))\n",
        "            if 0 <= x < w and 0 <= y < h:\n",
        "                density[y, x] += 1\n",
        "\n",
        "        density = gaussian_filter(density, sigma=15)\n",
        "\n",
        "        np.save(os.path.join(OUT, img_stem + \".npy\"), density)\n",
        "\n",
        "    return errors\n",
        "\n",
        "print(\"Generating Part-B densities\")\n",
        "err1 = generate(PART_B_TRAIN_IMG, PART_B_TRAIN_GT)\n",
        "err2 = generate(PART_B_TEST_IMG, PART_B_TEST_GT)\n",
        "\n",
        "print(\"\\nDone!\")\n",
        "print(\"Train errors:\", err1[:5])\n",
        "print(\"Test errors:\", err2[:5])\n",
        "print(\"Total generated:\", len(os.listdir(OUT)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ol1tvgFK1my",
        "outputId": "cb48b07d-2f04-43bd-9744-77dd10151645"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Part-B densities\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 400/400 [00:49<00:00,  8.05it/s]\n",
            "100%|| 316/316 [00:39<00:00,  8.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Done!\n",
            "Train errors: []\n",
            "Test errors: []\n",
            "Total generated: 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CrowdCounter(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple CrowdCounter skeleton. If you have the original CrowdCounter\n",
        "    implementation, use that instead. This skeleton will let notebook run\n",
        "    and let you load state-dicts with strict=False if shapes differ.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=3, out_ch=1):\n",
        "        super().__init__()\n",
        "        # small, safe conv encoder-decoder\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 16, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.mid = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.enc(x)\n",
        "        x = self.mid(x)\n",
        "        x = self.dec(x)\n",
        "        return x\n",
        "# --- end model def ---"
      ],
      "metadata": {
        "id": "3QX9dM0xNP9P"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch, numpy as np, math, cv2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# UPDATE these paths to match your notebook (they look correct from your screenshots)\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/DEEPVISION_checkpoints/model_epoch_150_state_dict.pth\"\n",
        "IMG_DIR = \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images\"\n",
        "DEN_DIR = \"/content/DEEPVISION/DEEPVISION/DATA/processed/part_B/density\"              # part_B processed densities\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# create model (must be defined already in the notebook)\n",
        "model = CrowdCounter().to(device)\n",
        "\n",
        "# safe load: state_dict may have keys from a different arch; use strict=False so it doesn't crash\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    state = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "    if isinstance(state, dict) and 'state_dict' in state:\n",
        "        state = state['state_dict']\n",
        "    try:\n",
        "        model.load_state_dict(state, strict=False)\n",
        "        print(\"Loaded checkpoint (strict=False):\", CHECKPOINT_PATH)\n",
        "    except Exception as e:\n",
        "        print(\"Warning: loading checkpoint raised:\", e)\n",
        "else:\n",
        "    print(\"Checkpoint not found:\", CHECKPOINT_PATH)\n",
        "\n",
        "# Simple dataset that pairs images and .npy density files by basename\n",
        "class CrowdDataset(Dataset):\n",
        "    def __init__(self, img_dir, den_dir, img_exts=(\".jpg\",\".png\")):\n",
        "        self.img_dir = img_dir\n",
        "        self.den_dir = den_dir\n",
        "        self.imgs = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(img_exts)])\n",
        "        # match numpy names by stripping ext and prefixing IMG_***\n",
        "        self.pairs = []\n",
        "        self.bad = []\n",
        "        for img in self.imgs:\n",
        "            stem = os.path.splitext(img)[0]   # e.g. IMG_100\n",
        "            denp = os.path.join(den_dir, stem + \".npy\")\n",
        "            if os.path.exists(denp):\n",
        "                self.pairs.append((os.path.join(img_dir,img), denp))\n",
        "            else:\n",
        "                self.bad.append((img, denp))\n",
        "        print(\"Valid pairs:\", len(self.pairs), \"Bad:\", len(self.bad))\n",
        "        if len(self.bad)>0:\n",
        "            print(\"Examples bad:\", self.bad[:5])\n",
        "\n",
        "    def __len__(self): return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        imgp, denp = self.pairs[idx]\n",
        "        # read image with cv2, convert to RGB and normalize to [0-1]\n",
        "        im = cv2.imread(imgp)\n",
        "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB).astype(\"float32\")/255.0\n",
        "        h,w,_ = im.shape\n",
        "        # convert to tensor CHW\n",
        "        img = torch.from_numpy(im).permute(2,0,1).contiguous()\n",
        "        # load density - robustly handle bad npy shapes\n",
        "        den = np.load(denp, allow_pickle=False)\n",
        "        # If density is 1D or weird shape, try to reshape to original image shape approximately:\n",
        "        try:\n",
        "            if den.ndim==2:\n",
        "                pass\n",
        "            elif den.size == (h*w):\n",
        "                den = den.reshape((h,w))\n",
        "            else:\n",
        "                # if density doesn't match, try resizing via OpenCV after flattening/guess\n",
        "                den = den.reshape((int(np.sqrt(den.size)), int(np.sqrt(den.size))))\n",
        "        except Exception:\n",
        "            # final fallback: create zero density map\n",
        "            den = np.zeros((h,w), dtype=np.float32)\n",
        "\n",
        "        # resize density to a reasonable model output size if needed (model expects any)\n",
        "        den_resized = cv2.resize(den, (w, h))  # keep same size as image for counting\n",
        "        den_t = torch.from_numpy(den_resized.astype(\"float32\")).unsqueeze(0).contiguous()\n",
        "        return img, den_t\n",
        "\n",
        "# make dataset+loader\n",
        "test_ds = CrowdDataset(IMG_DIR, DEN_DIR)\n",
        "if len(test_ds)==0:\n",
        "    raise RuntimeError(\"No valid pairs found. Check paths and processed densities.\")\n",
        "\n",
        "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
        "\n",
        "# evaluation: MAE / RMSE (skips any failures)\n",
        "def evaluate_mae_rmse(model, loader, device):\n",
        "    model.eval()\n",
        "    mae_sum = 0.0\n",
        "    mse_sum = 0.0\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, dens in loader:\n",
        "            imgs = imgs.to(device); dens = dens.to(device)\n",
        "            preds = model(imgs)\n",
        "            # sum counts\n",
        "            pred_count = preds.detach().cpu().sum()\n",
        "            gt_count = dens.detach().cpu().sum()\n",
        "            diff = (pred_count - gt_count).item()\n",
        "            mae_sum += abs(diff)\n",
        "            mse_sum += diff*diff\n",
        "            n += 1\n",
        "    if n == 0:\n",
        "        return float(\"nan\"), float(\"nan\")\n",
        "    mae = mae_sum / n\n",
        "    rmse = math.sqrt(mse_sum / n)\n",
        "    return mae, rmse\n",
        "\n",
        "mae, rmse = evaluate_mae_rmse(model, test_loader, device)\n",
        "print(\"MAE:\", mae, \"RMSE:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evGZGGK8N6HJ",
        "outputId": "08f14d12-633f-444e-b732-512c88856063"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loaded checkpoint (strict=False): /content/drive/MyDrive/DEEPVISION_checkpoints/model_epoch_150_state_dict.pth\n",
            "Valid pairs: 316 Bad: 0\n",
            "MAE: 88181.58440466772 RMSE: 88182.41461095592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, math, torch, numpy as np, cv2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "# --------- 1) Simple CrowdCounter fallback (replace with your real class if you have it) ----------\n",
        "class CrowdCounter(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=1):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 16, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.mid = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, out_ch, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.enc(x)\n",
        "        x = self.mid(x)\n",
        "        x = self.dec(x)\n",
        "        return x\n",
        "\n",
        "# --------- 2) Paths for Part B (match what I saw in your notebook) ----------\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/DEEPVISION_checkpoints/model_epoch_150_state_dict.pth\"\n",
        "IMG_DIR = \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images\"\n",
        "DEN_DIR = \"/content/DEEPVISION/DEEPVISION/DATA/processed/part_B/density\"\n",
        "\n",
        "# --------- 3) device & model init & safe load ----------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "model = CrowdCounter().to(device)\n",
        "\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    ck = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "    # if checkpoint wrapped in dict under 'state_dict', extract it\n",
        "    if isinstance(ck, dict) and 'state_dict' in ck:\n",
        "        ck = ck['state_dict']\n",
        "    try:\n",
        "        model.load_state_dict(ck, strict=False)\n",
        "        print(\"Loaded checkpoint (strict=False):\", CHECKPOINT_PATH)\n",
        "    except Exception as e:\n",
        "        print(\"Warning while loading checkpoint (continuing):\", e)\n",
        "else:\n",
        "    print(\"Warning: checkpoint not found at:\", CHECKPOINT_PATH)\n",
        "\n",
        "# --------- 4) Dataset robust to bad density shapes ----------\n",
        "class CrowdDatasetPartB(Dataset):\n",
        "    def __init__(self, img_dir, den_dir, exts=(\".jpg\",\".png\")):\n",
        "        self.img_dir = img_dir\n",
        "        self.den_dir = den_dir\n",
        "        self.imgs = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(exts)])\n",
        "        self.pairs = []\n",
        "        self.bad = []\n",
        "        for img in self.imgs:\n",
        "            stem = os.path.splitext(img)[0]   # IMG_100\n",
        "            denp = os.path.join(den_dir, stem + \".npy\")\n",
        "            if os.path.exists(denp):\n",
        "                self.pairs.append((os.path.join(img_dir,img), denp))\n",
        "            else:\n",
        "                self.bad.append((img, denp))\n",
        "        print(\"Found images:\", len(self.imgs))\n",
        "        print(\"Valid pairs:\", len(self.pairs), \"Bad pairs (missing .npy):\", len(self.bad))\n",
        "        if len(self.bad)>0:\n",
        "            print(\"Examples missing density:\", self.bad[:5])\n",
        "\n",
        "    def __len__(self): return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        imgp, denp = self.pairs[idx]\n",
        "        # load image and convert to float32 RGB [0,1]\n",
        "        im = cv2.imread(imgp)\n",
        "        if im is None:\n",
        "            raise RuntimeError(\"Cannot read image: \" + imgp)\n",
        "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB).astype(\"float32\")/255.0\n",
        "        h,w,_ = im.shape\n",
        "        img_t = torch.from_numpy(im).permute(2,0,1).contiguous()\n",
        "\n",
        "        # load density robustly\n",
        "        try:\n",
        "            den = np.load(denp, allow_pickle=False)\n",
        "        except Exception as e:\n",
        "            # if corrupted, fallback to zeros of image size\n",
        "            den = np.zeros((h,w), dtype=np.float32)\n",
        "\n",
        "        # If den is 2D and matches image shape OK -> use it.\n",
        "        # Else attempt some reasonable reshapes/resizes, else zeros.\n",
        "        if den.ndim == 2:\n",
        "            final_den = den\n",
        "        else:\n",
        "            # try common fallbacks\n",
        "            if den.size == h*w:\n",
        "                try:\n",
        "                    final_den = den.reshape((h,w))\n",
        "                except:\n",
        "                    final_den = np.zeros((h,w), dtype=np.float32)\n",
        "            else:\n",
        "                # try square fallback\n",
        "                side = int(round(np.sqrt(den.size)))\n",
        "                if side*side == den.size:\n",
        "                    final_den = den.reshape((side, side))\n",
        "                    final_den = cv2.resize(final_den, (w,h))\n",
        "                else:\n",
        "                    # fallback zero map\n",
        "                    final_den = np.zeros((h,w), dtype=np.float32)\n",
        "\n",
        "        den_resized = cv2.resize(final_den.astype(\"float32\"), (w, h))\n",
        "        den_t = torch.from_numpy(den_resized).unsqueeze(0).contiguous()\n",
        "        return img_t, den_t\n",
        "\n",
        "# --------- 5) Build loader and evaluate function ----------\n",
        "ds = CrowdDatasetPartB(IMG_DIR, DEN_DIR)\n",
        "if len(ds) == 0:\n",
        "    raise RuntimeError(\"No valid image-density pairs found for Part B -> check IMG_DIR and DEN_DIR paths\")\n",
        "\n",
        "loader = DataLoader(ds, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "def evaluate_mae_rmse(model, loader, device):\n",
        "    model.eval()\n",
        "    mae_sum = 0.0\n",
        "    mse_sum = 0.0\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, dens in loader:\n",
        "            imgs = imgs.to(device); dens = dens.to(device)\n",
        "            preds = model(imgs)\n",
        "            # sum counts\n",
        "            pred_count = preds.detach().cpu().sum().item()\n",
        "            gt_count = dens.detach().cpu().sum().item()\n",
        "            diff = pred_count - gt_count\n",
        "            mae_sum += abs(diff)\n",
        "            mse_sum += (diff*diff)\n",
        "            n += 1\n",
        "    if n == 0:\n",
        "        return float(\"nan\"), float(\"nan\")\n",
        "    mae = mae_sum / n\n",
        "    rmse = math.sqrt(mse_sum / n)\n",
        "    return mae, rmse\n",
        "\n",
        "# --------- 6) Run evaluation ----------\n",
        "print(\"Starting evaluation on Part B ({} samples)...\".format(len(ds)))\n",
        "mae, rmse = evaluate_mae_rmse(model, loader, device)\n",
        "print(\"RESULTS -> MAE: {:.4f}, RMSE: {:.4f}\".format(mae, rmse))\n",
        "# =========================================================================================="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGrmGeh_PYeL",
        "outputId": "27eb3b4f-271e-40ab-8d91-8b24b6ffa3bb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loaded checkpoint (strict=False): /content/drive/MyDrive/DEEPVISION_checkpoints/model_epoch_150_state_dict.pth\n",
            "Found images: 316\n",
            "Valid pairs: 316 Bad pairs (missing .npy): 0\n",
            "Starting evaluation on Part B (316 samples)...\n",
            "RESULTS -> MAE: 48649.8651, RMSE: 48650.0435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 1) Setup / Dataset / Utils ----------\n",
        "import os, glob, math, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Update these paths if your repo uses different ones ---\n",
        "PART_B_IMG_DIR  = \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/train_data/images\"\n",
        "PART_B_DEN_DIR  = \"/content/DEEPVISION/DEEPVISION/DATA/processed/part_B/density\"\n",
        "PART_B_TEST_IMG_DIR = \"/content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images\"\n",
        "PART_B_TEST_DEN_DIR = \"/content/DEEPVISION/DEEPVISION/DATA/processed/part_B/density\"  # usually same processed folder\n",
        "\n",
        "CHECKPOINT_IN = \"/content/drive/MyDrive/DEEPVISION_checkpoints/model_epoch_150_state_dict.pth\"  # transfer checkpoint\n",
        "OUT_CHECKPOINT_DIR = \"/content/drive/MyDrive/DEEPVISION_checkpoints/partB\"\n",
        "os.makedirs(OUT_CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Defensive checks\n",
        "assert os.path.isdir(PART_B_IMG_DIR), f\"PartB image dir not found: {PART_B_IMG_DIR}\"\n",
        "assert os.path.isdir(PART_B_DEN_DIR), f\"PartB density dir not found: {PART_B_DEN_DIR}\"\n",
        "assert os.path.isdir(PART_B_TEST_IMG_DIR), f\"PartB test image dir not found: {PART_B_TEST_IMG_DIR}\"\n",
        "assert os.path.isdir(PART_B_TEST_DEN_DIR), f\"PartB test density dir not found: {PART_B_TEST_DEN_DIR}\"\n",
        "\n",
        "def list_image_stems(img_dir):\n",
        "    imgs = sorted([Path(p).stem for p in glob.glob(os.path.join(img_dir,\"*.jpg\")) + glob.glob(os.path.join(img_dir,\"*.png\"))])\n",
        "    return imgs\n",
        "\n",
        "def list_density_stems(den_dir):\n",
        "    dens = sorted([Path(p).stem for p in glob.glob(os.path.join(den_dir,\"*.npy\"))])\n",
        "    return dens\n",
        "\n",
        "class CrowdDatasetSimple(Dataset):\n",
        "    \"\"\"Simple dataset: matches image stems with same density stem (IMG_1.jpg -> IMG_1.npy).\"\"\"\n",
        "    def __init__(self, img_dir, den_dir, img_size=(3, 512, 512), den_size=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.den_dir = den_dir\n",
        "        self.img_paths = sorted([p for p in glob.glob(os.path.join(img_dir,\"*.jpg\")) + glob.glob(os.path.join(img_dir,\"*.png\"))])\n",
        "        # build map of densities\n",
        "        den_map = {Path(p).stem: p for p in glob.glob(os.path.join(den_dir,\"*.npy\"))}\n",
        "        # keep only those with matching density\n",
        "        self.pairs = []\n",
        "        for p in self.img_paths:\n",
        "            stem = Path(p).stem\n",
        "            if stem in den_map:\n",
        "                self.pairs.append((p, den_map[stem]))\n",
        "        print(f\"Found {len(self.img_paths)} images, {len(self.pairs)} image-density pairs in {img_dir}\")\n",
        "\n",
        "        self.img_size = img_size  # (C,H,W)\n",
        "        self.den_size = den_size  # (H,W) or None -> will resize to image H,W\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, den_path = self.pairs[idx]\n",
        "        # read image\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            raise RuntimeError(\"Failed to read image: \"+img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        # resize image to 512x512 to keep things consistent (you can change)\n",
        "        out_h, out_w = 512, 512\n",
        "        img = cv2.resize(img, (out_w, out_h), interpolation=cv2.INTER_LINEAR)\n",
        "        img = img.astype(\"float32\")/255.0\n",
        "        img = torch.from_numpy(img).permute(2,0,1).contiguous()\n",
        "\n",
        "        den = np.load(den_path)\n",
        "        # dens might be at original image resolution; resize to out_h,out_w\n",
        "        den_resized = cv2.resize(den, (out_w, out_h), interpolation=cv2.INTER_LINEAR)\n",
        "        den_resized = den_resized.astype(\"float32\")\n",
        "        den_tensor = torch.from_numpy(den_resized).unsqueeze(0).contiguous()\n",
        "\n",
        "        return img, den_tensor\n",
        "\n",
        "# quick small test\n",
        "train_ds = CrowdDatasetSimple(PART_B_IMG_DIR, PART_B_DEN_DIR)\n",
        "test_ds  = CrowdDatasetSimple(PART_B_TEST_IMG_DIR, PART_B_TEST_DEN_DIR)\n",
        "print(\"Train pairs:\", len(train_ds), \"Test pairs:\", len(test_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZMJ7LlzQfZO",
        "outputId": "1149c42e-4544-45be-cfb8-644a54d5dc4c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Found 400 images, 400 image-density pairs in /content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/train_data/images\n",
            "Found 316 images, 316 image-density pairs in /content/DEEPVISION/DEEPVISION/DATA/ShanghaiTech/part_B/test_data/images\n",
            "Train pairs: 400 Test pairs: 316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Training + evaluation helpers"
      ],
      "metadata": {
        "id": "RX3f1bJERnnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_mae_rmse(model, loader, device):\n",
        "    model.eval()\n",
        "    mae_sum = 0.0\n",
        "    mse_sum = 0.0\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, dens in tqdm(loader, desc=\"Evaluating\"):\n",
        "            imgs = imgs.to(device); dens = dens.to(device)\n",
        "            preds = model(imgs)  # assuming model returns density map same size as dens\n",
        "            # sum counts\n",
        "            pred_count = preds.detach().sum([1,2,3]) if preds.dim()==4 else preds.detach().sum([1,2])\n",
        "            gt_count   = dens.detach().sum([1,2,3]) if dens.dim()==4 else dens.detach().sum([1,2])\n",
        "            # ensure shapes are compatible: convert both to CPU floats\n",
        "            pred_count = pred_count.cpu().float()\n",
        "            gt_count   = gt_count.cpu().float()\n",
        "            abs_err = (pred_count - gt_count).abs().sum().item()\n",
        "            sq_err  = ((pred_count - gt_count)**2).sum().item()\n",
        "            mae_sum += abs_err\n",
        "            mse_sum += sq_err\n",
        "            n += imgs.size(0)\n",
        "    if n == 0:\n",
        "        return float('nan'), float('nan')\n",
        "    mae = mae_sum / n\n",
        "    rmse = math.sqrt(mse_sum / n)\n",
        "    return mae, rmse\n",
        "\n",
        "def train_finetune(model, train_loader, val_loader, device, epochs=10, lr=1e-5, save_prefix=\"partB\"):\n",
        "    model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    best_mae = 1e9\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        it = 0\n",
        "        for imgs, dens in tqdm(train_loader, desc=f\"Train Epoch {epoch}/{epochs}\"):\n",
        "            imgs = imgs.to(device); dens = dens.to(device)\n",
        "            preds = model(imgs)\n",
        "            # if preds and dens shape mismatch, resize preds to dens with interpolate\n",
        "            if preds.shape[-2:] != dens.shape[-2:]:\n",
        "                preds = torch.nn.functional.interpolate(preds, size=dens.shape[-2:], mode='bilinear', align_corners=False)\n",
        "            loss = criterion(preds, dens)\n",
        "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "            total_loss += loss.item(); it += 1\n",
        "        avg_loss = total_loss / (it if it>0 else 1)\n",
        "        print(f\"Epoch {epoch}: AvgLoss: {avg_loss:.6f}\")\n",
        "        # evaluate on val\n",
        "        if val_loader is not None:\n",
        "            mae, rmse = evaluate_mae_rmse(model, val_loader, device)\n",
        "            print(f\"Val MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
        "            # save checkpoint\n",
        "            out = os.path.join(OUT_CHECKPOINT_DIR, f\"{save_prefix}_epoch{epoch}.pth\")\n",
        "            torch.save(model.state_dict(), out)\n",
        "            print(\"Saved:\", out)\n",
        "            if mae < best_mae:\n",
        "                best_mae = mae\n",
        "                best_out = os.path.join(OUT_CHECKPOINT_DIR, f\"{save_prefix}_best.pth\")\n",
        "                torch.save(model.state_dict(), best_out)\n",
        "                print(\"Saved best:\", best_out)\n",
        "    return model"
      ],
      "metadata": {
        "id": "FzMZUVoXQnbO"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load model (CrowdCounter) and training run"
      ],
      "metadata": {
        "id": "0sw2uxysRjIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if 'CrowdCounter' not in globals():\n",
        "    raise RuntimeError(\"Model class 'CrowdCounter' not found in notebook. Please run the cell that defines your model class before running this cell.\")\n",
        "\n",
        "# create model\n",
        "model = CrowdCounter().to(device)\n",
        "\n",
        "# try to load transfer checkpoint (if file exists)\n",
        "if os.path.isfile(CHECKPOINT_IN):\n",
        "    print(\"Loading checkpoint (strict=False):\", CHECKPOINT_IN)\n",
        "    state = torch.load(CHECKPOINT_IN, map_location=device)\n",
        "    try:\n",
        "        model.load_state_dict(state, strict=False)\n",
        "        print(\"Loaded checkpoint into model (strict=False).\")\n",
        "    except Exception as e:\n",
        "        # sometimes checkpoint saved state_dict under key like 'state_dict' or 'model_state_dict'\n",
        "        if isinstance(state, dict) and ('state_dict' in state or 'model_state_dict' in state):\n",
        "            sd = state.get('state_dict', state.get('model_state_dict'))\n",
        "            model.load_state_dict(sd, strict=False)\n",
        "            print(\"Loaded checkpoint nested state into model (strict=False).\")\n",
        "        else:\n",
        "            print(\"Could not load checkpoint directly:\", e)\n",
        "else:\n",
        "    print(\"No checkpoint found at:\", CHECKPOINT_IN, \"- training from scratch (not recommended)\")\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(test_ds,  batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Train / fine-tune\n",
        "fine_epochs = 10   # set to 10 or 20 or 50 as you like\n",
        "finetuned_model = train_finetune(model, train_loader, val_loader, device, epochs=fine_epochs, lr=1e-5, save_prefix=\"partB_finetune\")\n",
        "\n",
        "# final eval\n",
        "mae, rmse = evaluate_mae_rmse(finetuned_model, val_loader, device)\n",
        "print(\"FINAL PART-B EVAL -> MAE: {:.4f}, RMSE: {:.4f}\".format(mae, rmse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lkym14kDQwVY",
        "outputId": "895751c4-c7d8-4ed6-d15b-f14f6ceccec3"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint (strict=False): /content/drive/MyDrive/DEEPVISION_checkpoints/model_epoch_150_state_dict.pth\n",
            "Loaded checkpoint into model (strict=False).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 1/10: 100%|| 50/50 [00:10<00:00,  4.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: AvgLoss: 0.001626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 79/79 [00:04<00:00, 18.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val MAE: 8290.9389, RMSE: 8297.6438\n",
            "Saved: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_epoch1.pth\n",
            "Saved best: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 2/10: 100%|| 50/50 [00:09<00:00,  5.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: AvgLoss: 0.000550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 79/79 [00:04<00:00, 18.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val MAE: 3318.7425, RMSE: 3393.4378\n",
            "Saved: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_epoch2.pth\n",
            "Saved best: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 3/10: 100%|| 50/50 [00:09<00:00,  5.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: AvgLoss: 0.000112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 79/79 [00:04<00:00, 18.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val MAE: 822.0805, RMSE: 970.7674\n",
            "Saved: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_epoch3.pth\n",
            "Saved best: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 4/10: 100%|| 50/50 [00:09<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: AvgLoss: 0.000046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 79/79 [00:04<00:00, 18.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val MAE: 440.3107, RMSE: 531.2221\n",
            "Saved: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_epoch4.pth\n",
            "Saved best: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 5/10: 100%|| 50/50 [00:10<00:00,  4.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: AvgLoss: 0.000020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 79/79 [00:04<00:00, 18.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val MAE: 180.4392, RMSE: 219.9825\n",
            "Saved: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_epoch5.pth\n",
            "Saved best: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 6/10: 100%|| 50/50 [00:10<00:00,  4.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: AvgLoss: 0.000010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 79/79 [00:04<00:00, 18.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val MAE: 88.9544, RMSE: 106.6934\n",
            "Saved: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_epoch6.pth\n",
            "Saved best: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 7/10: 100%|| 50/50 [00:10<00:00,  4.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: AvgLoss: 0.000008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 79/79 [00:06<00:00, 13.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val MAE: 65.2295, RMSE: 81.1255\n",
            "Saved: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_epoch7.pth\n",
            "Saved best: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 8/10: 100%|| 50/50 [00:10<00:00,  4.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: AvgLoss: 0.000007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 79/79 [00:05<00:00, 14.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val MAE: 68.6215, RMSE: 82.9958\n",
            "Saved: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_epoch8.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 9/10: 100%|| 50/50 [00:10<00:00,  4.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: AvgLoss: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 79/79 [00:05<00:00, 13.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val MAE: 60.9323, RMSE: 73.7196\n",
            "Saved: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_epoch9.pth\n",
            "Saved best: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 10/10: 100%|| 50/50 [00:10<00:00,  4.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: AvgLoss: 0.000005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 79/79 [00:05<00:00, 15.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val MAE: 52.1004, RMSE: 63.4590\n",
            "Saved: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_epoch10.pth\n",
            "Saved best: /content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 79/79 [00:04<00:00, 18.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL PART-B EVAL -> MAE: 52.1004, RMSE: 63.4590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}