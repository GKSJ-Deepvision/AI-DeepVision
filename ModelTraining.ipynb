{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7XbUVatu1lv",
        "outputId": "77112dcb-3417-488a-81a5-7235a5f9c098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n",
            "GPU Type: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(f\"GPU Available: {len(gpus) > 0}\")\n",
        "if gpus:\n",
        "    print(f\"GPU Type: {gpus[0]}\")\n",
        "else:\n",
        "    print(\"❌ NO GPU!\")\n",
        "    print(\"Fix: Go to Runtime → Change runtime type → Select 'GPU' → Save\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive')\n",
        "print(\"✓ Mounted at:\", os.getcwd())\n",
        "\n",
        "# Create DeepVision folder\n",
        "os.makedirs('DeepVision', exist_ok=True)\n",
        "os.chdir('DeepVision')\n",
        "print(\"✓ Working directory:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeGWYQw8vqpd",
        "outputId": "cdaf7dc5-0938-4f64-9ef3-a9339783eb48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✓ Mounted at: /content/drive/MyDrive\n",
            "✓ Working directory: /content/drive/MyDrive/DeepVision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_code = '''#!/usr/bin/env python3\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import sys\n",
        "import pickle\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"CSRNet - GPU TRAINING (Google Colab)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(f\"\\\\nGPU Devices: {len(gpus)}\")\n",
        "\n",
        "CONFIG = {\n",
        "    'batch_size': 32,\n",
        "    'epochs': 100,\n",
        "    'learning_rate': 1e-3,\n",
        "    'weight_decay': 1e-5,\n",
        "    'data_path': 'processed_dataset_fixed/part_A_fixed.pkl',\n",
        "    'results_path': 'results/csrnet_gpu_trained',\n",
        "}\n",
        "\n",
        "# LOAD DATA\n",
        "print(\"\\\\n[LOADING DATA]\")\n",
        "\n",
        "if not Path(CONFIG['data_path']).exists():\n",
        "    print(f\"ERROR: {CONFIG['data_path']} not found\")\n",
        "    sys.exit(1)\n",
        "\n",
        "with open(CONFIG['data_path'], 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "X_train = data['X_train'].astype('float32')\n",
        "y_density_train = data['y_density_train'][..., np.newaxis].astype('float32')\n",
        "X_test = data['X_test'].astype('float32')\n",
        "y_count_test = data['y_count_test']\n",
        "\n",
        "print(f\"✓ Training: X{X_train.shape}, y{y_density_train.shape}\")\n",
        "print(f\"✓ Testing:  X{X_test.shape}\")\n",
        "\n",
        "# BUILD MODEL\n",
        "print(\"\\\\n[BUILDING MODEL]\")\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(256, 256, 3)),\n",
        "\n",
        "    # Block 1\n",
        "    layers.Conv2D(64, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation='relu'),\n",
        "    layers.Conv2D(64, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation='relu'),\n",
        "    layers.MaxPooling2D(2),\n",
        "\n",
        "    # Block 2\n",
        "    layers.Conv2D(128, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation='relu'),\n",
        "    layers.Conv2D(128, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation='relu'),\n",
        "    layers.MaxPooling2D(2),\n",
        "\n",
        "    # Block 3\n",
        "    layers.Conv2D(256, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation='relu'),\n",
        "    layers.Conv2D(256, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation='relu'),\n",
        "    layers.Conv2D(256, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation='relu'),\n",
        "    # layers.MaxPooling2D(2), # REMOVED THIS LAYER TO MATCH 64x64 GROUND TRUTH\n",
        "\n",
        "    # Block 4\n",
        "    layers.Conv2D(512, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation='relu'),\n",
        "    layers.Conv2D(512, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation='relu'),\n",
        "    layers.Conv2D(512, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation='relu'),\n",
        "\n",
        "    # Backend (dilated)\n",
        "    layers.Conv2D(512, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation='relu'),\n",
        "    layers.Conv2D(512, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation='relu'),\n",
        "    layers.Conv2D(512, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation='relu'),\n",
        "    layers.Conv2D(256, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation='relu'),\n",
        "    layers.Conv2D(128, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation='relu'),\n",
        "    layers.Conv2D(64, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation='relu'),\n",
        "\n",
        "    layers.Conv2D(1, 1, padding='same', activation='relu'),\n",
        "], name='CSRNet')\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=CONFIG['learning_rate'])\n",
        "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "print(f\"✓ Parameters: {model.count_params():,}\")\n",
        "\n",
        "# CALLBACKS\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1),\n",
        "]\n",
        "\n",
        "# TRAIN\n",
        "print(\"\\\\n[TRAINING]\")\n",
        "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"Max epochs: {CONFIG['epochs']}\\\\n\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_density_train,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    epochs=CONFIG['epochs'],\n",
        "    validation_split=0.2,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# EVALUATE\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"[EVALUATION]\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "predictions = []\n",
        "for i in range(0, len(X_test), 50):\n",
        "    batch = X_test[i:i+50]\n",
        "    pred_batch = model.predict(batch, verbose=0)\n",
        "    predictions.extend([p.sum() for p in pred_batch])\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "\n",
        "mae = np.mean(np.abs(predictions - y_count_test))\n",
        "rmse = np.sqrt(np.mean((predictions - y_count_test) ** 2))\n",
        "\n",
        "print(f\"\\\\nRESULTS:\")\n",
        "print(f\"  MAE:   {mae:.2f} (Target: 70-150)\")\n",
        "print(f\"  RMSE:  {rmse:.2f} (Target: 100-200)\")\n",
        "\n",
        "mae_pass = 70 <= mae <= 150\n",
        "rmse_pass = 100 <= rmse <= 200\n",
        "\n",
        "print(f\"\\\\nTARGET ACHIEVEMENT:\")\n",
        "print(f\"  MAE  [70-150]:    {'PASS' if mae_pass else 'FAIL'}\")\n",
        "print(f\"  RMSE [100-200]:   {'PASS' if rmse_pass else 'FAIL'}\")\n",
        "\n",
        "print(f\"\\\\nFirst 25 predictions:\")\n",
        "for i in range(min(25, len(predictions))):\n",
        "    error = abs(predictions[i] - y_count_test[i])\n",
        "    print(f\"  {i+1:2d}: pred={predictions[i]:7.0f}, true={y_count_test[i]:7.0f}, error={error:6.0f}\")\n",
        "\n",
        "# SAVE\n",
        "results_dir = Path(CONFIG['results_path'])\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(results_dir / 'results.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'mae': float(mae),\n",
        "        'rmse': float(rmse),\n",
        "        'predictions': predictions.tolist(),\n",
        "        'ground_truth': y_count_test.tolist(),\n",
        "        'model_params': model.count_params(),\n",
        "        'mae_pass': bool(mae_pass),\n",
        "        'rmse_pass': bool(rmse_pass),\n",
        "    }, f)\n",
        "\n",
        "model.save(results_dir / 'model.keras')\n",
        "\n",
        "print(f\"\\\\n✓ Results saved to: {results_dir}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if mae_pass and rmse_pass:\n",
        "    print(\"\\\\n✓✓✓ SUCCESS! TARGETS ACHIEVED! ✓✓✓\\\\n\")\n",
        "else:\n",
        "    print(\"\\\\n⚠ Check results above\\\\n\")\n",
        "'''\n",
        "\n",
        "with open('train_gpu_colab.py', 'w') as f:\n",
        "    f.write(training_code)\n",
        "\n",
        "print(\"✓ Training script created: train_gpu_colab.py\")\n",
        "print(\"✓ Ready for data upload\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcJhtN_Fv16h",
        "outputId": "136cb292-e2fd-45e0-dda7-d530b5cf4dbc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Training script created: train_gpu_colab.py\n",
            "✓ Ready for data upload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = 'processed_dataset_fixed/part_A_fixed.pkl'\n",
        "if os.path.exists(path):\n",
        "    size_mb = os.path.getsize(path) / (1024**2)\n",
        "    print(f\"✓ Data ready! Size: {size_mb:.0f} MB\")\n",
        "else:\n",
        "    print(f\"❌ File not found: {path}\")\n",
        "    print(\"Upload part_A_fixed.pkl to: DeepVision/processed_dataset_fixed/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQl93_eLwr17",
        "outputId": "18463172-3778-42b3-8879-98afda8c345a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Data ready! Size: 369 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_gpu_colab.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqVflG652ge1",
        "outputId": "38375153-9eb8-47fa-d468-0390d95c1548"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 13:33:13.197237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765027993.216847   10407 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765027993.222953   10407 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765027993.238684   10407 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765027993.238708   10407 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765027993.238714   10407 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765027993.238717   10407 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\n",
            "================================================================================\n",
            "CSRNet - GPU TRAINING (Google Colab)\n",
            "================================================================================\n",
            "\n",
            "GPU Devices: 1\n",
            "\n",
            "[LOADING DATA]\n",
            "✓ Training: X(300, 256, 256, 3), y(300, 64, 64, 1)\n",
            "✓ Testing:  X(182, 256, 256, 3)\n",
            "\n",
            "[BUILDING MODEL]\n",
            "I0000 00:00:1765027999.183014   10407 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "✓ Parameters: 16,263,489\n",
            "\n",
            "[TRAINING]\n",
            "Batch size: 32\n",
            "Max epochs: 100\n",
            "\n",
            "Epoch 1/100\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1765028005.350637   10454 service.cc:152] XLA service 0x7c879411de00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1765028005.350683   10454 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "I0000 00:00:1765028006.513299   10454 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
            "2025-12-06 13:33:33.331328: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.54 = (f32[32,128,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,128,128]{3,2,1,0} %bitcast.9310, f32[128,128,3,3]{3,2,1,0} %bitcast.7499, f32[128]{0} %bitcast.9371), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet_1/conv2d_3_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:33:33.413835: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.082799636s\n",
            "Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.54 = (f32[32,128,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,128,128]{3,2,1,0} %bitcast.9310, f32[128,128,3,3]{3,2,1,0} %bitcast.7499, f32[128]{0} %bitcast.9371), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet_1/conv2d_3_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:33:53.972892: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.61 = (f32[128,512,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,34,34]{3,2,1,0} %bitcast.7631, f32[512,512,3,3]{3,2,1,0} %bitcast.7638, f32[512]{0} %bitcast.10350), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet_1/conv2d_10_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:33:55.029441: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.056642613s\n",
            "Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.61 = (f32[128,512,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,34,34]{3,2,1,0} %bitcast.7631, f32[512,512,3,3]{3,2,1,0} %bitcast.7638, f32[512]{0} %bitcast.10350), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet_1/conv2d_10_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:34:07.384162: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng4{k11=2} for conv %cudnn-conv-bw-input.17 = (f32[128,512,34,34]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,256,32,32]{3,2,1,0} %bitcast.7996, f32[256,512,3,3]{3,2,1,0} %bitcast.7731), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet_1/conv2d_13_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:34:07.616945: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.232873262s\n",
            "Trying algorithm eng4{k11=2} for conv %cudnn-conv-bw-input.17 = (f32[128,512,34,34]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,256,32,32]{3,2,1,0} %bitcast.7996, f32[256,512,3,3]{3,2,1,0} %bitcast.7731), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet_1/conv2d_13_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:34:13.243183: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng4{k11=2} for conv %cudnn-conv-bw-input.18 = (f32[128,512,34,34]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,32,32]{3,2,1,0} %bitcast.8047, f32[512,512,3,3]{3,2,1,0} %bitcast.7700), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet_1/conv2d_12_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:34:14.973143: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.730045818s\n",
            "Trying algorithm eng4{k11=2} for conv %cudnn-conv-bw-input.18 = (f32[128,512,34,34]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,32,32]{3,2,1,0} %bitcast.8047, f32[512,512,3,3]{3,2,1,0} %bitcast.7700), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet_1/conv2d_12_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:34:21.387164: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng1{k2=2,k3=0} for conv %cudnn-conv-bw-input.18 = (f32[128,512,34,34]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,32,32]{3,2,1,0} %bitcast.8047, f32[512,512,3,3]{3,2,1,0} %bitcast.7700), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet_1/conv2d_12_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:34:21.405358: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.018292725s\n",
            "Trying algorithm eng1{k2=2,k3=0} for conv %cudnn-conv-bw-input.18 = (f32[128,512,34,34]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,32,32]{3,2,1,0} %bitcast.8047, f32[512,512,3,3]{3,2,1,0} %bitcast.7700), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet_1/conv2d_12_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:34:22.405572: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng2{k2=0,k3=0} for conv %cudnn-conv-bw-input.18 = (f32[128,512,34,34]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,32,32]{3,2,1,0} %bitcast.8047, f32[512,512,3,3]{3,2,1,0} %bitcast.7700), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet_1/conv2d_12_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:34:22.524760: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.119286076s\n",
            "Trying algorithm eng2{k2=0,k3=0} for conv %cudnn-conv-bw-input.18 = (f32[128,512,34,34]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,32,32]{3,2,1,0} %bitcast.8047, f32[512,512,3,3]{3,2,1,0} %bitcast.7700), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet_1/conv2d_12_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:34:29.326881: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng2{k2=0,k3=0} for conv %cudnn-conv-bw-input.21 = (f32[32,512,64,64]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,64,64]{3,2,1,0} %bitcast.10211, f32[512,512,3,3]{3,2,1,0} %bitcast.7608), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet_1/conv2d_9_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:34:29.576546: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.249758057s\n",
            "Trying algorithm eng2{k2=0,k3=0} for conv %cudnn-conv-bw-input.21 = (f32[32,512,64,64]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,64,64]{3,2,1,0} %bitcast.10211, f32[512,512,3,3]{3,2,1,0} %bitcast.7608), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet_1/conv2d_9_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:34:34.434731: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:34:34.761256: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:34:37.172470: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:34:37.417374: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:34:41.345543: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:34:41.619805: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng4{k11=1} for conv %cudnn-conv-bw-input.27 = (f32[32,128,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,128,128]{3,2,1,0} %bitcast.9375, f32[128,128,3,3]{3,2,1,0} %bitcast.7499), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet_1/conv2d_3_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:34:41.779050: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:34:41.813022: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.193307482s\n",
            "Trying algorithm eng4{k11=1} for conv %cudnn-conv-bw-input.27 = (f32[32,128,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,128,128]{3,2,1,0} %bitcast.9375, f32[128,128,3,3]{3,2,1,0} %bitcast.7499), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet_1/conv2d_3_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:34:44.176851: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:34:44.538144: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:35:06.337400: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng20{k2=7,k3=0} for conv %cudnn-conv-bw-filter.25 = (f32[512,512,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,64,64]{3,2,1,0} %bitcast.10007, f32[32,512,64,64]{3,2,1,0} %bitcast.10072), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:35:06.384962: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.047646025s\n",
            "Trying algorithm eng20{k2=7,k3=0} for conv %cudnn-conv-bw-filter.25 = (f32[512,512,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,64,64]{3,2,1,0} %bitcast.10007, f32[32,512,64,64]{3,2,1,0} %bitcast.10072), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:35:07.385181: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng1{k2=6,k3=0} for conv %cudnn-conv-bw-filter.25 = (f32[512,512,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,64,64]{3,2,1,0} %bitcast.10007, f32[32,512,64,64]{3,2,1,0} %bitcast.10072), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:35:07.628201: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.243113407s\n",
            "Trying algorithm eng1{k2=6,k3=0} for conv %cudnn-conv-bw-filter.25 = (f32[512,512,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,64,64]{3,2,1,0} %bitcast.10007, f32[32,512,64,64]{3,2,1,0} %bitcast.10072), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:35:08.628436: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng20{k2=6,k3=0} for conv %cudnn-conv-bw-filter.25 = (f32[512,512,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,64,64]{3,2,1,0} %bitcast.10007, f32[32,512,64,64]{3,2,1,0} %bitcast.10072), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:35:08.831363: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.203034918s\n",
            "Trying algorithm eng20{k2=6,k3=0} for conv %cudnn-conv-bw-filter.25 = (f32[512,512,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,64,64]{3,2,1,0} %bitcast.10007, f32[32,512,64,64]{3,2,1,0} %bitcast.10072), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:35:14.789781: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng20{k2=7,k3=0} for conv %cudnn-conv-bw-filter.27 = (f32[512,512,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,34,34]{3,2,1,0} %bitcast.7631, f32[128,512,32,32]{3,2,1,0} %bitcast.8684), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:35:14.792055: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.002374461s\n",
            "Trying algorithm eng20{k2=7,k3=0} for conv %cudnn-conv-bw-filter.27 = (f32[512,512,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,34,34]{3,2,1,0} %bitcast.7631, f32[128,512,32,32]{3,2,1,0} %bitcast.8684), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:35:15.792265: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng1{k2=6,k3=0} for conv %cudnn-conv-bw-filter.27 = (f32[512,512,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,34,34]{3,2,1,0} %bitcast.7631, f32[128,512,32,32]{3,2,1,0} %bitcast.8684), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:35:16.002622: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.210449071s\n",
            "Trying algorithm eng1{k2=6,k3=0} for conv %cudnn-conv-bw-filter.27 = (f32[512,512,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,34,34]{3,2,1,0} %bitcast.7631, f32[128,512,32,32]{3,2,1,0} %bitcast.8684), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:35:17.003529: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng20{k2=6,k3=0} for conv %cudnn-conv-bw-filter.27 = (f32[512,512,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,34,34]{3,2,1,0} %bitcast.7631, f32[128,512,32,32]{3,2,1,0} %bitcast.8684), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:35:17.135938: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.133188807s\n",
            "Trying algorithm eng20{k2=6,k3=0} for conv %cudnn-conv-bw-filter.27 = (f32[512,512,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,34,34]{3,2,1,0} %bitcast.7631, f32[128,512,32,32]{3,2,1,0} %bitcast.8684), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "I0000 00:00:1765028127.012830   10454 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 2s/step - loss: 3679.5085 - mae: 27.24242025-12-06 13:35:55.880376: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.61 = (f32[64,512,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,512,34,34]{3,2,1,0} %bitcast.7631, f32[512,512,3,3]{3,2,1,0} %bitcast.7638, f32[512]{0} %bitcast.10350), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet_1/conv2d_10_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:35:56.093534: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.21323908s\n",
            "Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.61 = (f32[64,512,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,512,34,34]{3,2,1,0} %bitcast.7631, f32[512,512,3,3]{3,2,1,0} %bitcast.7638, f32[512]{0} %bitcast.10350), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet_1/conv2d_10_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:36:17.716993: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:36:18.028727: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:36:19.637958: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:36:19.872390: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:36:22.283523: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:36:22.516339: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng4{k11=1} for conv %cudnn-conv-bw-input.27 = (f32[16,128,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,128,128,128]{3,2,1,0} %bitcast.9375, f32[128,128,3,3]{3,2,1,0} %bitcast.7499), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet_1/conv2d_3_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:36:22.724833: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:36:22.742166: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.225907323s\n",
            "Trying algorithm eng4{k11=1} for conv %cudnn-conv-bw-input.27 = (f32[16,128,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,128,128,128]{3,2,1,0} %bitcast.9375, f32[128,128,3,3]{3,2,1,0} %bitcast.7499), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet_1/conv2d_3_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:36:24.185686: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:36:24.519612: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - loss: 3489.0774 - mae: 25.84012025-12-06 13:37:20.020062: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.61 = (f32[128,512,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,34,34]{3,2,1,0} %bitcast.1427, f32[512,512,3,3]{3,2,1,0} %bitcast.1434, f32[512]{0} %bitcast.1436), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet_1/conv2d_10_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:37:21.005924: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.985966118s\n",
            "Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.61 = (f32[128,512,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,34,34]{3,2,1,0} %bitcast.1427, f32[512,512,3,3]{3,2,1,0} %bitcast.1434, f32[512]{0} %bitcast.1436), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet_1/conv2d_10_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:37:27.375526: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.64 = (f32[128,256,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,34,34]{3,2,1,0} %bitcast.1514, f32[256,512,3,3]{3,2,1,0} %bitcast.1521, f32[256]{0} %bitcast.1523), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet_1/conv2d_13_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:37:28.340107: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.967022207s\n",
            "Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.64 = (f32[128,256,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,34,34]{3,2,1,0} %bitcast.1514, f32[256,512,3,3]{3,2,1,0} %bitcast.1521, f32[256]{0} %bitcast.1523), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet_1/conv2d_13_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:37:42.144269: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:37:42.386541: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:37:44.686875: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:37:45.013953: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 13:38:00.571828: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.61 = (f32[112,512,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[112,512,34,34]{3,2,1,0} %bitcast.1427, f32[512,512,3,3]{3,2,1,0} %bitcast.1434, f32[512]{0} %bitcast.1436), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet_1/conv2d_10_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:38:01.271222: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.699484846s\n",
            "Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.61 = (f32[112,512,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[112,512,34,34]{3,2,1,0} %bitcast.1427, f32[512,512,3,3]{3,2,1,0} %bitcast.1434, f32[512]{0} %bitcast.1436), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet_1/conv2d_10_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:38:06.865531: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.64 = (f32[112,256,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[112,512,34,34]{3,2,1,0} %bitcast.1514, f32[256,512,3,3]{3,2,1,0} %bitcast.1521, f32[256]{0} %bitcast.1523), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet_1/conv2d_13_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 13:38:07.253485: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.389528772s\n",
            "Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.64 = (f32[112,256,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[112,512,34,34]{3,2,1,0} %bitcast.1514, f32[256,512,3,3]{3,2,1,0} %bitcast.1521, f32[256]{0} %bitcast.1523), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet_1/conv2d_13_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 23s/step - loss: 3340.9646 - mae: 24.7495 - val_loss: 0.1402 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1058 - mae: 0.1232 - val_loss: 0.1377 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1041 - mae: 0.1297 - val_loss: 0.1348 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1048 - mae: 0.1334 - val_loss: 0.1322 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0932 - mae: 0.1219 - val_loss: 0.1299 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0997 - mae: 0.1289 - val_loss: 0.1281 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0969 - mae: 0.1287 - val_loss: 0.1268 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1009 - mae: 0.1367 - val_loss: 0.1258 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0806 - mae: 0.1190 - val_loss: 0.1251 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0752 - mae: 0.1195 - val_loss: 0.1246 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0881 - mae: 0.1246 - val_loss: 0.1242 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0897 - mae: 0.1300 - val_loss: 0.1238 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0811 - mae: 0.1217 - val_loss: 0.1235 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0913 - mae: 0.1309 - val_loss: 0.1233 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0872 - mae: 0.1257 - val_loss: 0.1231 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0926 - mae: 0.1317 - val_loss: 0.1229 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0831 - mae: 0.1252 - val_loss: 0.1227 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0836 - mae: 0.1250 - val_loss: 0.1225 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0986 - mae: 0.1348 - val_loss: 0.1224 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0871 - mae: 0.1271 - val_loss: 0.1223 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0934 - mae: 0.1320 - val_loss: 0.1221 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0883 - mae: 0.1297 - val_loss: 0.1220 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 23/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0902 - mae: 0.1300 - val_loss: 0.1219 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 24/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0908 - mae: 0.1289 - val_loss: 0.1218 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 25/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0878 - mae: 0.1292 - val_loss: 0.1217 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 26/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0985 - mae: 0.1349 - val_loss: 0.1216 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 27/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0804 - mae: 0.1263 - val_loss: 0.1216 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 28/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0909 - mae: 0.1297 - val_loss: 0.1215 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 29/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0871 - mae: 0.1286 - val_loss: 0.1214 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 30/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0896 - mae: 0.1283 - val_loss: 0.1213 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 31/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0893 - mae: 0.1286 - val_loss: 0.1213 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 32/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0764 - mae: 0.1192 - val_loss: 0.1212 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 33/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0822 - mae: 0.1240 - val_loss: 0.1211 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 34/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0928 - mae: 0.1367 - val_loss: 0.1211 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 35/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0904 - mae: 0.1285 - val_loss: 0.1210 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 36/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0998 - mae: 0.1377 - val_loss: 0.1210 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 37/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0899 - mae: 0.1322 - val_loss: 0.1209 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 38/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0863 - mae: 0.1289 - val_loss: 0.1209 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 39/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0887 - mae: 0.1291 - val_loss: 0.1208 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 40/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1008 - mae: 0.1371 - val_loss: 0.1208 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 41/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0798 - mae: 0.1244 - val_loss: 0.1207 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 42/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0920 - mae: 0.1309 - val_loss: 0.1207 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 43/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0926 - mae: 0.1297 - val_loss: 0.1206 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 44/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0901 - mae: 0.1338 - val_loss: 0.1206 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 45/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0792 - mae: 0.1205 - val_loss: 0.1206 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 46/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0876 - mae: 0.1327 - val_loss: 0.1205 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 47/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0915 - mae: 0.1337 - val_loss: 0.1205 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 48/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0829 - mae: 0.1264 - val_loss: 0.1204 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 49/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0782 - mae: 0.1215 - val_loss: 0.1204 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 50/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0881 - mae: 0.1320 - val_loss: 0.1204 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 51/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0897 - mae: 0.1306 - val_loss: 0.1203 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 52/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0863 - mae: 0.1296 - val_loss: 0.1203 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 53/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0861 - mae: 0.1285 - val_loss: 0.1203 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 54/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0947 - mae: 0.1335 - val_loss: 0.1202 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 55/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0867 - mae: 0.1284 - val_loss: 0.1202 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 56/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0884 - mae: 0.1312 - val_loss: 0.1202 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 57/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0802 - mae: 0.1285 - val_loss: 0.1201 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 58/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0870 - mae: 0.1294 - val_loss: 0.1201 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 59/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0842 - mae: 0.1264 - val_loss: 0.1201 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 60/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0793 - mae: 0.1206 - val_loss: 0.1200 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 61/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0925 - mae: 0.1344 - val_loss: 0.1200 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 62/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0938 - mae: 0.1285 - val_loss: 0.1200 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 63/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0835 - mae: 0.1265 - val_loss: 0.1200 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 64/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0841 - mae: 0.1287 - val_loss: 0.1199 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 65/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0908 - mae: 0.1315 - val_loss: 0.1199 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 66/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0879 - mae: 0.1307 - val_loss: 0.1199 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 67/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0958 - mae: 0.1377 - val_loss: 0.1199 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 68/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0983 - mae: 0.1375 - val_loss: 0.1198 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 69/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0807 - mae: 0.1201 - val_loss: 0.1198 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 70/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0849 - mae: 0.1272 - val_loss: 0.1198 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 71/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0796 - mae: 0.1259 - val_loss: 0.1198 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 72/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0938 - mae: 0.1349 - val_loss: 0.1197 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 73/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0851 - mae: 0.1249 - val_loss: 0.1197 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 74/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0861 - mae: 0.1281 - val_loss: 0.1197 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 75/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0899 - mae: 0.1332 - val_loss: 0.1197 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 76/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0786 - mae: 0.1195 - val_loss: 0.1196 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 77/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0765 - mae: 0.1182 - val_loss: 0.1196 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 78/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0847 - mae: 0.1257 - val_loss: 0.1196 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 79/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0857 - mae: 0.1328 - val_loss: 0.1196 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 80/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0855 - mae: 0.1252 - val_loss: 0.1195 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 81/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0767 - mae: 0.1208 - val_loss: 0.1195 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 82/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0897 - mae: 0.1358 - val_loss: 0.1195 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 83/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0973 - mae: 0.1395 - val_loss: 0.1195 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 84/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0847 - mae: 0.1277 - val_loss: 0.1195 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 85/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.1096 - mae: 0.1467 - val_loss: 0.1194 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 86/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0834 - mae: 0.1320 - val_loss: 0.1194 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 87/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0831 - mae: 0.1255 - val_loss: 0.1194 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 88/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0854 - mae: 0.1292 - val_loss: 0.1194 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 89/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0970 - mae: 0.1356 - val_loss: 0.1194 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 90/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0801 - mae: 0.1263 - val_loss: 0.1193 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 91/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0899 - mae: 0.1292 - val_loss: 0.1193 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 92/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0897 - mae: 0.1257 - val_loss: 0.1193 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 93/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0795 - mae: 0.1230 - val_loss: 0.1193 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 94/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0738 - mae: 0.1189 - val_loss: 0.1193 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 95/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0868 - mae: 0.1297 - val_loss: 0.1192 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 96/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0937 - mae: 0.1382 - val_loss: 0.1192 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 97/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0856 - mae: 0.1332 - val_loss: 0.1192 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 98/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0896 - mae: 0.1318 - val_loss: 0.1192 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 99/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0920 - mae: 0.1305 - val_loss: 0.1192 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Epoch 100/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.0859 - mae: 0.1305 - val_loss: 0.1192 - val_mae: 0.1447 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 100.\n",
            "\n",
            "================================================================================\n",
            "[EVALUATION]\n",
            "================================================================================\n",
            "2025-12-06 14:00:19.519694: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 14:00:19.752055: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 14:00:21.635373: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 14:00:21.950226: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 14:00:32.785406: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.61 = (f32[72,512,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[72,512,34,34]{3,2,1,0} %bitcast.912, f32[512,512,3,3]{3,2,1,0} %bitcast.919, f32[512]{0} %bitcast.921), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet_1/conv2d_10_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:00:32.826709: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.041486302s\n",
            "Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.61 = (f32[72,512,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[72,512,34,34]{3,2,1,0} %bitcast.912, f32[512,512,3,3]{3,2,1,0} %bitcast.919, f32[512]{0} %bitcast.921), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet_1/conv2d_10_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "\n",
            "RESULTS:\n",
            "  MAE:   433.31 (Target: 70-150)\n",
            "  RMSE:  559.19 (Target: 100-200)\n",
            "\n",
            "TARGET ACHIEVEMENT:\n",
            "  MAE  [70-150]:    FAIL\n",
            "  RMSE [100-200]:   FAIL\n",
            "\n",
            "First 25 predictions:\n",
            "   1: pred=      0, true=    172, error=   172\n",
            "   2: pred=      0, true=    502, error=   502\n",
            "   3: pred=      0, true=    389, error=   389\n",
            "   4: pred=      0, true=    211, error=   211\n",
            "   5: pred=      0, true=    223, error=   223\n",
            "   6: pred=      0, true=    431, error=   431\n",
            "   7: pred=      0, true=   1175, error=  1175\n",
            "   8: pred=      0, true=    265, error=   265\n",
            "   9: pred=      0, true=   1232, error=  1232\n",
            "  10: pred=      0, true=    289, error=   289\n",
            "  11: pred=      0, true=    182, error=   182\n",
            "  12: pred=      0, true=    379, error=   379\n",
            "  13: pred=      0, true=   1068, error=  1068\n",
            "  14: pred=      0, true=   1021, error=  1021\n",
            "  15: pred=      0, true=    452, error=   452\n",
            "  16: pred=      0, true=    256, error=   256\n",
            "  17: pred=      0, true=     66, error=    66\n",
            "  18: pred=      0, true=    141, error=   141\n",
            "  19: pred=      0, true=   1191, error=  1191\n",
            "  20: pred=      0, true=    288, error=   288\n",
            "  21: pred=      0, true=   1603, error=  1603\n",
            "  22: pred=      0, true=    241, error=   241\n",
            "  23: pred=      0, true=    250, error=   250\n",
            "  24: pred=      0, true=    321, error=   321\n",
            "  25: pred=      0, true=    133, error=   133\n",
            "\n",
            "✓ Results saved to: results/csrnet_gpu_trained\n",
            "================================================================================\n",
            "\n",
            "⚠ Check results above\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_code = '''#!/usr/bin/env python3\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import sys\n",
        "import pickle\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"CSRNet - ADVANCED GPU TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(f\"\\\\nGPU: {len(gpus) > 0}\")\n",
        "\n",
        "# Mixed precision for GPU\n",
        "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy)\n",
        "print(\"Mixed precision: ENABLED\")\n",
        "\n",
        "CONFIG = {\n",
        "    'batch_size': 16,\n",
        "    'epochs': 200,\n",
        "    'learning_rate': 1e-4,\n",
        "    'weight_decay': 5e-5,\n",
        "    'data_path': 'processed_dataset_fixed/part_A_fixed.pkl',\n",
        "    'results_path': 'results/csrnet_advanced_gpu',\n",
        "}\n",
        "\n",
        "# LOAD DATA\n",
        "print(\"\\\\n[LOADING DATA]\")\n",
        "\n",
        "with open(CONFIG['data_path'], 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "X_train = data['X_train'].astype('float32')\n",
        "y_density_train = data['y_density_train'][..., np.newaxis].astype('float32')\n",
        "X_test = data['X_test'].astype('float32')\n",
        "y_count_test = data['y_count_test']\n",
        "\n",
        "# Split data\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_density_train, test_size=0.15, random_state=42)\n",
        "\n",
        "print(f\"✓ Train: {X_tr.shape}, Val: {X_val.shape}\")\n",
        "print(f\"✓ Test: {X_test.shape}\")\n",
        "\n",
        "# BUILD MODEL with BatchNorm\n",
        "print(\"\\\\n[BUILDING MODEL]\")\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(256, 256, 3)),\n",
        "\n",
        "    # Block 1\n",
        "    layers.Conv2D(64, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(64, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D(2),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    # Block 2\n",
        "    layers.Conv2D(128, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(128, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D(2),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    # Block 3\n",
        "    layers.Conv2D(256, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(256, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(256, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    # layers.MaxPooling2D(2), # REMOVED THIS LAYER TO MATCH 64x64 GROUND TRUTH\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    # Block 4\n",
        "    layers.Conv2D(512, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(512, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(512, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "\n",
        "    # Backend - Dilated\n",
        "    layers.Conv2D(512, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(512, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(512, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(256, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(128, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(64, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "\n",
        "    layers.Conv2D(1, 1, padding='same', activation='relu'),\n",
        "], name='CSRNet-Advanced')\n",
        "\n",
        "# Custom loss: MSE + MAE\n",
        "def combined_loss(y_true, y_pred):\n",
        "    mse = tf.keras.losses.mse(y_true, y_pred)\n",
        "    mae = tf.keras.losses.mae(y_true, y_pred)\n",
        "    return 0.7 * mse + 0.3 * mae\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=CONFIG['learning_rate'])\n",
        "model.compile(optimizer=optimizer, loss=combined_loss, metrics=['mae', 'mse'])\n",
        "print(f\"✓ Parameters: {model.count_params():,}\")\n",
        "\n",
        "# PHASE 1: Warm-up\n",
        "print(\"\\\\n[PHASE 1: WARM-UP TRAINING]\")\n",
        "\n",
        "callbacks1 = [\n",
        "    EarlyStopping(monitor='val_mae', patience=30, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_mae', factor=0.7, patience=15, min_lr=1e-7, verbose=1),\n",
        "]\n",
        "\n",
        "history1 = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    epochs=100,\n",
        "    callbacks=callbacks1,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# PHASE 2: Fine-tuning with lower LR\n",
        "print(\"\\\\n[PHASE 2: FINE-TUNING]\")\n",
        "\n",
        "optimizer2 = optimizers.Adam(learning_rate=CONFIG['learning_rate'] / 5)\n",
        "model.compile(optimizer=optimizer2, loss=combined_loss, metrics=['mae', 'mse'])\n",
        "\n",
        "callbacks2 = [\n",
        "    EarlyStopping(monitor='val_mae', patience=50, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_mae', factor=0.8, patience=20, min_lr=1e-8, verbose=1),\n",
        "]\n",
        "\n",
        "history2 = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=CONFIG['batch_size'] * 2,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks2,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# EVALUATE\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"[EVALUATION]\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "predictions = []\n",
        "for i in range(0, len(X_test), 32):\n",
        "    batch = X_test[i:i+32]\n",
        "    pred_batch = model.predict(batch, verbose=0)\n",
        "    predictions.extend([max(0, p.sum()) for p in pred_batch])\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "\n",
        "mae = np.mean(np.abs(predictions - y_count_test))\n",
        "rmse = np.sqrt(np.mean((predictions - y_count_test) ** 2))\n",
        "\n",
        "print(f\"\\\\nRESULTS:\")\n",
        "print(f\"  MAE:   {mae:.2f} (Target: 70-150)\")\n",
        "print(f\"  RMSE:  {rmse:.2f} (Target: 100-200)\")\n",
        "\n",
        "mae_pass = 70 <= mae <= 150\n",
        "rmse_pass = 100 <= rmse <= 200\n",
        "\n",
        "print(f\"\\\\nTARGET ACHIEVEMENT:\")\n",
        "print(f\"  MAE  [70-150]:    {'PASS' if mae_pass else 'FAIL'}\")\n",
        "print(f\"  RMSE [100-200]:   {'PASS' if rmse_pass else 'FAIL'}\")\n",
        "\n",
        "print(f\"\\\\nFirst 25 predictions:\")\n",
        "for i in range(min(25, len(predictions))):\n",
        "    error = abs(predictions[i] - y_count_test[i])\n",
        "    print(f\"  {i+1:2d}: pred={predictions[i]:7.0f}, true={y_count_test[i]:7.0f}, error={error:6.0f}\")\n",
        "\n",
        "# SAVE\n",
        "results_dir = Path(CONFIG['results_path'])\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(results_dir / 'results.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'mae': float(mae),\n",
        "        'rmse': float(rmse),\n",
        "        'predictions': predictions.tolist(),\n",
        "        'ground_truth': y_count_test.tolist(),\n",
        "        'model_params': model.count_params(),\n",
        "        'mae_pass': bool(mae_pass),\n",
        "        'rmse_pass': bool(rmse_pass),\n",
        "    }, f)\n",
        "\n",
        "model.save(results_dir / 'model.keras')\n",
        "\n",
        "print(f\"\\\\n✓ Results saved to: {results_dir}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if mae_pass and rmse_pass:\n",
        "    print(\"\\\\n✓✓✓ SUCCESS! TARGETS ACHIEVED! ✓✓✓\\\\n\")\n",
        "else:\n",
        "    print(\"\\\\n⚠ Check results - may need further optimization\\\\n\")\n",
        "'''\n",
        "\n",
        "with open('train_gpu_advanced.py', 'w') as f:\n",
        "    f.write(training_code)\n",
        "\n",
        "print(\"✓ Advanced training script created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybEQlMpN4REW",
        "outputId": "c2221210-483d-4c31-db5f-71d1ccc02101"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Advanced training script created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_gpu_advanced.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AS1zubnCPPd",
        "outputId": "6dc4ce0a-b55e-4a88-ec5b-815b31d247fd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 14:16:43.214285: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765030603.248765   23207 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765030603.259955   23207 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765030603.284005   23207 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765030603.284043   23207 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765030603.284050   23207 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765030603.284057   23207 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\n",
            "================================================================================\n",
            "CSRNet - ADVANCED GPU TRAINING\n",
            "================================================================================\n",
            "\n",
            "GPU: True\n",
            "Mixed precision: ENABLED\n",
            "\n",
            "[LOADING DATA]\n",
            "✓ Train: (255, 256, 256, 3), Val: (45, 256, 256, 3)\n",
            "✓ Test: (182, 256, 256, 3)\n",
            "\n",
            "[BUILDING MODEL]\n",
            "I0000 00:00:1765030609.755508   23207 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "✓ Parameters: 16,282,177\n",
            "\n",
            "[PHASE 1: WARM-UP TRAINING]\n",
            "Epoch 1/100\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1765030623.139552   23257 service.cc:152] XLA service 0x7f55ac005d20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1765030623.139607   23257 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "I0000 00:00:1765030625.245175   23257 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
            "2025-12-06 14:17:37.221114: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-input.29 = (f16[16,256,256,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[16,256,256,64]{3,2,1,0} %bitcast.29427, f16[64,3,3,64]{3,2,1,0} %bitcast.29278), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_1_2/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:17:41.729719: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 5.508685962s\n",
            "Trying algorithm eng0{} for conv %cudnn-conv-bw-input.29 = (f16[16,256,256,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[16,256,256,64]{3,2,1,0} %bitcast.29427, f16[64,3,3,64]{3,2,1,0} %bitcast.29278), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_1_2/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:17:46.068534: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.18 = (f16[64,3,3,4]{3,2,1,0}, u8[0]{0}) custom-call(f16[16,256,256,4]{3,2,1,0} %bitcast.28346, f16[16,256,256,64]{3,2,1,0} %bitcast.29359), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:17:46.485125: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.417843442s\n",
            "Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.18 = (f16[64,3,3,4]{3,2,1,0}, u8[0]{0}) custom-call(f16[16,256,256,4]{3,2,1,0} %bitcast.28346, f16[16,256,256,64]{3,2,1,0} %bitcast.29359), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:17:53.397592: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.20 = (f16[128,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[16,128,128,64]{3,2,1,0} %bitcast.29499, f16[16,128,128,128]{3,2,1,0} %bitcast.29501), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:17:56.366280: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 3.969242348s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.20 = (f16[128,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[16,128,128,64]{3,2,1,0} %bitcast.29499, f16[16,128,128,128]{3,2,1,0} %bitcast.29501), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:17:57.366515: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=1} for conv %cudnn-conv-bw-filter.20 = (f16[128,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[16,128,128,64]{3,2,1,0} %bitcast.29499, f16[16,128,128,128]{3,2,1,0} %bitcast.29501), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:17:59.579575: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 3.213158805s\n",
            "Trying algorithm eng19{k2=1} for conv %cudnn-conv-bw-filter.20 = (f16[128,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[16,128,128,64]{3,2,1,0} %bitcast.29499, f16[16,128,128,128]{3,2,1,0} %bitcast.29501), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:18:02.532527: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng18{k11=0} for conv %cudnn-conv-bw-filter.21 = (f16[128,3,3,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[16,128,128,128]{3,2,1,0} %bitcast.29573, f16[16,128,128,128]{3,2,1,0} %bitcast.29575), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_3_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:18:02.619712: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.08876843s\n",
            "Trying algorithm eng18{k11=0} for conv %cudnn-conv-bw-filter.21 = (f16[128,3,3,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[16,128,128,128]{3,2,1,0} %bitcast.29573, f16[16,128,128,128]{3,2,1,0} %bitcast.29575), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_3_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:18:03.619932: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.21 = (f16[128,3,3,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[16,128,128,128]{3,2,1,0} %bitcast.29573, f16[16,128,128,128]{3,2,1,0} %bitcast.29575), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_3_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:18:11.272280: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 8.65242791s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.21 = (f16[128,3,3,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[16,128,128,128]{3,2,1,0} %bitcast.29573, f16[16,128,128,128]{3,2,1,0} %bitcast.29575), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_3_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:18:21.610694: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[16,64,64,512]{3,2,1,0} %bitcast.29943, f16[16,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:18:58.384573: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 37.773969416s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[16,64,64,512]{3,2,1,0} %bitcast.29943, f16[16,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:19:05.046843: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[64,34,34,512]{3,2,1,0} %bitcast.26623, f16[64,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:19:43.717519: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 39.670777847s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[64,34,34,512]{3,2,1,0} %bitcast.26623, f16[64,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:19:47.163019: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.31 = (f16[256,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[64,34,34,512]{3,2,1,0} %bitcast.26911, f16[64,32,32,256]{3,2,1,0} %bitcast.27594), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_13_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:20:01.436845: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 15.275149097s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.31 = (f16[256,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[64,34,34,512]{3,2,1,0} %bitcast.26911, f16[64,32,32,256]{3,2,1,0} %bitcast.27594), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_13_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "I0000 00:00:1765030816.448247   23257 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[1m15/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 362ms/step - loss: 0.3407 - mae: 0.1732 - mse: 0.09362025-12-06 14:21:00.606909: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-input.29 = (f16[15,256,256,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[15,256,256,64]{3,2,1,0} %bitcast.29427, f16[64,3,3,64]{3,2,1,0} %bitcast.29278), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_1_2/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:21:05.348196: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 5.741384676s\n",
            "Trying algorithm eng0{} for conv %cudnn-conv-bw-input.29 = (f16[15,256,256,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[15,256,256,64]{3,2,1,0} %bitcast.29427, f16[64,3,3,64]{3,2,1,0} %bitcast.29278), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_1_2/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:21:09.062541: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.18 = (f16[64,3,3,4]{3,2,1,0}, u8[0]{0}) custom-call(f16[15,256,256,4]{3,2,1,0} %bitcast.28346, f16[15,256,256,64]{3,2,1,0} %bitcast.29359), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:21:09.358025: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.295579622s\n",
            "Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.18 = (f16[64,3,3,4]{3,2,1,0}, u8[0]{0}) custom-call(f16[15,256,256,4]{3,2,1,0} %bitcast.28346, f16[15,256,256,64]{3,2,1,0} %bitcast.29359), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:21:15.018531: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.20 = (f16[128,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[15,128,128,64]{3,2,1,0} %bitcast.29499, f16[15,128,128,128]{3,2,1,0} %bitcast.29501), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:21:17.718826: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 3.70082676s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.20 = (f16[128,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[15,128,128,64]{3,2,1,0} %bitcast.29499, f16[15,128,128,128]{3,2,1,0} %bitcast.29501), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:21:18.719057: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=1} for conv %cudnn-conv-bw-filter.20 = (f16[128,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[15,128,128,64]{3,2,1,0} %bitcast.29499, f16[15,128,128,128]{3,2,1,0} %bitcast.29501), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:21:20.450923: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.731957239s\n",
            "Trying algorithm eng19{k2=1} for conv %cudnn-conv-bw-filter.20 = (f16[128,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[15,128,128,64]{3,2,1,0} %bitcast.29499, f16[15,128,128,128]{3,2,1,0} %bitcast.29501), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:21:23.944654: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.21 = (f16[128,3,3,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[15,128,128,128]{3,2,1,0} %bitcast.29573, f16[15,128,128,128]{3,2,1,0} %bitcast.29575), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_3_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:21:30.998682: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 8.05411888s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.21 = (f16[128,3,3,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[15,128,128,128]{3,2,1,0} %bitcast.29573, f16[15,128,128,128]{3,2,1,0} %bitcast.29575), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_3_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:21:40.718826: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[15,64,64,512]{3,2,1,0} %bitcast.29943, f16[15,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:22:14.446166: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 34.727445839s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[15,64,64,512]{3,2,1,0} %bitcast.29943, f16[15,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:22:21.133717: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[60,34,34,512]{3,2,1,0} %bitcast.26623, f16[60,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:22:56.401447: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 36.267804008s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[60,34,34,512]{3,2,1,0} %bitcast.26623, f16[60,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:22:59.833709: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.31 = (f16[256,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[60,34,34,512]{3,2,1,0} %bitcast.26911, f16[60,32,32,256]{3,2,1,0} %bitcast.27594), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_13_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:23:12.436431: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 13.602799029s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.31 = (f16[256,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[60,34,34,512]{3,2,1,0} %bitcast.26911, f16[60,32,32,256]{3,2,1,0} %bitcast.27594), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_13_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 14s/step - loss: 0.3383 - mae: 0.1700 - mse: 0.0915 - val_loss: 0.3072 - val_mae: 0.1132 - val_mse: 0.0716 - learning_rate: 1.0000e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 392ms/step - loss: 0.2907 - mae: 0.1137 - mse: 0.0480 - val_loss: 0.2964 - val_mae: 0.1028 - val_mse: 0.0614 - learning_rate: 1.0000e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 395ms/step - loss: 0.2787 - mae: 0.0981 - mse: 0.0383 - val_loss: 0.3025 - val_mae: 0.1083 - val_mse: 0.0687 - learning_rate: 1.0000e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 404ms/step - loss: 0.2688 - mae: 0.0835 - mse: 0.0315 - val_loss: 0.3066 - val_mae: 0.1170 - val_mse: 0.0721 - learning_rate: 1.0000e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 406ms/step - loss: 0.2587 - mae: 0.0740 - mse: 0.0223 - val_loss: 0.3059 - val_mae: 0.1169 - val_mse: 0.0724 - learning_rate: 1.0000e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 398ms/step - loss: 0.2583 - mae: 0.0733 - mse: 0.0235 - val_loss: 0.3060 - val_mae: 0.1186 - val_mse: 0.0733 - learning_rate: 1.0000e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 389ms/step - loss: 0.2616 - mae: 0.0795 - mse: 0.0270 - val_loss: 0.3052 - val_mae: 0.1192 - val_mse: 0.0735 - learning_rate: 1.0000e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 387ms/step - loss: 0.2537 - mae: 0.0708 - mse: 0.0211 - val_loss: 0.3041 - val_mae: 0.1193 - val_mse: 0.0735 - learning_rate: 1.0000e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 381ms/step - loss: 0.2528 - mae: 0.0729 - mse: 0.0206 - val_loss: 0.3029 - val_mae: 0.1192 - val_mse: 0.0735 - learning_rate: 1.0000e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 381ms/step - loss: 0.2492 - mae: 0.0664 - mse: 0.0200 - val_loss: 0.3015 - val_mae: 0.1191 - val_mse: 0.0735 - learning_rate: 1.0000e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 375ms/step - loss: 0.2515 - mae: 0.0732 - mse: 0.0222 - val_loss: 0.3000 - val_mae: 0.1188 - val_mse: 0.0733 - learning_rate: 1.0000e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 380ms/step - loss: 0.2442 - mae: 0.0637 - mse: 0.0178 - val_loss: 0.2981 - val_mae: 0.1179 - val_mse: 0.0730 - learning_rate: 1.0000e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 375ms/step - loss: 0.2428 - mae: 0.0642 - mse: 0.0176 - val_loss: 0.2958 - val_mae: 0.1176 - val_mse: 0.0719 - learning_rate: 1.0000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 382ms/step - loss: 0.2415 - mae: 0.0644 - mse: 0.0176 - val_loss: 0.2801 - val_mae: 0.0997 - val_mse: 0.0593 - learning_rate: 1.0000e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 378ms/step - loss: 0.2433 - mae: 0.0683 - mse: 0.0207 - val_loss: 0.2878 - val_mae: 0.1132 - val_mse: 0.0665 - learning_rate: 1.0000e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 383ms/step - loss: 0.2417 - mae: 0.0662 - mse: 0.0214 - val_loss: 0.2765 - val_mae: 0.0998 - val_mse: 0.0584 - learning_rate: 1.0000e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 387ms/step - loss: 0.2395 - mae: 0.0656 - mse: 0.0208 - val_loss: 0.2677 - val_mae: 0.0962 - val_mse: 0.0495 - learning_rate: 1.0000e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 389ms/step - loss: 0.2348 - mae: 0.0638 - mse: 0.0169 - val_loss: 0.2604 - val_mae: 0.0858 - val_mse: 0.0459 - learning_rate: 1.0000e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 389ms/step - loss: 0.2333 - mae: 0.0596 - mse: 0.0189 - val_loss: 0.2571 - val_mae: 0.0807 - val_mse: 0.0456 - learning_rate: 1.0000e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 384ms/step - loss: 0.2288 - mae: 0.0590 - mse: 0.0151 - val_loss: 0.2586 - val_mae: 0.0935 - val_mse: 0.0446 - learning_rate: 1.0000e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 385ms/step - loss: 0.2254 - mae: 0.0585 - mse: 0.0128 - val_loss: 0.2551 - val_mae: 0.0891 - val_mse: 0.0439 - learning_rate: 1.0000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 384ms/step - loss: 0.2253 - mae: 0.0610 - mse: 0.0140 - val_loss: 0.2492 - val_mae: 0.0817 - val_mse: 0.0411 - learning_rate: 1.0000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 387ms/step - loss: 0.2234 - mae: 0.0596 - mse: 0.0143 - val_loss: 0.2436 - val_mae: 0.0699 - val_mse: 0.0405 - learning_rate: 1.0000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 382ms/step - loss: 0.2242 - mae: 0.0624 - mse: 0.0166 - val_loss: 0.2451 - val_mae: 0.0790 - val_mse: 0.0412 - learning_rate: 1.0000e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 383ms/step - loss: 0.2195 - mae: 0.0559 - mse: 0.0153 - val_loss: 0.2436 - val_mae: 0.0839 - val_mse: 0.0395 - learning_rate: 1.0000e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 380ms/step - loss: 0.2136 - mae: 0.0524 - mse: 0.0107 - val_loss: 0.2517 - val_mae: 0.1030 - val_mse: 0.0453 - learning_rate: 1.0000e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 382ms/step - loss: 0.2204 - mae: 0.0638 - mse: 0.0180 - val_loss: 0.2418 - val_mae: 0.0893 - val_mse: 0.0395 - learning_rate: 1.0000e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 378ms/step - loss: 0.2112 - mae: 0.0516 - mse: 0.0127 - val_loss: 0.2988 - val_mae: 0.1630 - val_mse: 0.0920 - learning_rate: 1.0000e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 381ms/step - loss: 0.2094 - mae: 0.0534 - mse: 0.0119 - val_loss: 0.2738 - val_mae: 0.1447 - val_mse: 0.0665 - learning_rate: 1.0000e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 380ms/step - loss: 0.2097 - mae: 0.0555 - mse: 0.0138 - val_loss: 0.3066 - val_mae: 0.1601 - val_mse: 0.1094 - learning_rate: 1.0000e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 383ms/step - loss: 0.2068 - mae: 0.0552 - mse: 0.0125 - val_loss: 0.2341 - val_mae: 0.0843 - val_mse: 0.0407 - learning_rate: 1.0000e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 380ms/step - loss: 0.2045 - mae: 0.0532 - mse: 0.0124 - val_loss: 0.5270 - val_mae: 0.3082 - val_mse: 0.3658 - learning_rate: 1.0000e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 383ms/step - loss: 0.2012 - mae: 0.0507 - mse: 0.0114 - val_loss: 0.2444 - val_mae: 0.1101 - val_mse: 0.0495 - learning_rate: 1.0000e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 381ms/step - loss: 0.2002 - mae: 0.0526 - mse: 0.0117 - val_loss: 0.2586 - val_mae: 0.1237 - val_mse: 0.0665 - learning_rate: 1.0000e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 384ms/step - loss: 0.1975 - mae: 0.0516 - mse: 0.0108 - val_loss: 0.2550 - val_mae: 0.1192 - val_mse: 0.0659 - learning_rate: 1.0000e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 383ms/step - loss: 0.2010 - mae: 0.0584 - mse: 0.0155 - val_loss: 0.2447 - val_mae: 0.1092 - val_mse: 0.0580 - learning_rate: 1.0000e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 384ms/step - loss: 0.1974 - mae: 0.0564 - mse: 0.0137 - val_loss: 0.2437 - val_mae: 0.1137 - val_mse: 0.0571 - learning_rate: 1.0000e-04\n",
            "Epoch 38/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358ms/step - loss: 0.1945 - mae: 0.0524 - mse: 0.0138\n",
            "Epoch 38: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 382ms/step - loss: 0.1945 - mae: 0.0525 - mse: 0.0137 - val_loss: 0.4473 - val_mae: 0.2571 - val_mse: 0.2891 - learning_rate: 1.0000e-04\n",
            "Epoch 39/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 381ms/step - loss: 0.1905 - mae: 0.0499 - mse: 0.0115 - val_loss: 0.3318 - val_mae: 0.1856 - val_mse: 0.1564 - learning_rate: 7.0000e-05\n",
            "Epoch 40/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 384ms/step - loss: 0.1873 - mae: 0.0481 - mse: 0.0095 - val_loss: 0.2756 - val_mae: 0.1480 - val_mse: 0.0941 - learning_rate: 7.0000e-05\n",
            "Epoch 41/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 381ms/step - loss: 0.1925 - mae: 0.0580 - mse: 0.0144 - val_loss: 0.2302 - val_mae: 0.1062 - val_mse: 0.0489 - learning_rate: 7.0000e-05\n",
            "Epoch 42/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 383ms/step - loss: 0.1886 - mae: 0.0538 - mse: 0.0124 - val_loss: 0.2742 - val_mae: 0.1615 - val_mse: 0.0899 - learning_rate: 7.0000e-05\n",
            "Epoch 43/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 381ms/step - loss: 0.1851 - mae: 0.0499 - mse: 0.0109 - val_loss: 0.3011 - val_mae: 0.1825 - val_mse: 0.1211 - learning_rate: 7.0000e-05\n",
            "Epoch 44/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 384ms/step - loss: 0.1821 - mae: 0.0484 - mse: 0.0090 - val_loss: 0.2611 - val_mae: 0.1425 - val_mse: 0.0829 - learning_rate: 7.0000e-05\n",
            "Epoch 45/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 380ms/step - loss: 0.1804 - mae: 0.0466 - mse: 0.0091 - val_loss: 0.2453 - val_mae: 0.1322 - val_mse: 0.0665 - learning_rate: 7.0000e-05\n",
            "Epoch 46/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 382ms/step - loss: 0.1824 - mae: 0.0524 - mse: 0.0113 - val_loss: 0.2716 - val_mae: 0.1722 - val_mse: 0.0887 - learning_rate: 7.0000e-05\n",
            "Epoch 47/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 380ms/step - loss: 0.1791 - mae: 0.0488 - mse: 0.0099 - val_loss: 0.3311 - val_mae: 0.2283 - val_mse: 0.1515 - learning_rate: 7.0000e-05\n",
            "Epoch 48/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 382ms/step - loss: 0.1773 - mae: 0.0476 - mse: 0.0097 - val_loss: 0.3239 - val_mae: 0.2255 - val_mse: 0.1441 - learning_rate: 7.0000e-05\n",
            "Epoch 49/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 380ms/step - loss: 0.1755 - mae: 0.0462 - mse: 0.0094 - val_loss: 0.2174 - val_mae: 0.0935 - val_mse: 0.0503 - learning_rate: 7.0000e-05\n",
            "Epoch 50/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 384ms/step - loss: 0.1767 - mae: 0.0514 - mse: 0.0107 - val_loss: 0.2312 - val_mae: 0.1242 - val_mse: 0.0587 - learning_rate: 7.0000e-05\n",
            "Epoch 51/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 380ms/step - loss: 0.1771 - mae: 0.0505 - mse: 0.0133 - val_loss: 0.2675 - val_mae: 0.1625 - val_mse: 0.0958 - learning_rate: 7.0000e-05\n",
            "Epoch 52/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 382ms/step - loss: 0.1713 - mae: 0.0472 - mse: 0.0082 - val_loss: 0.2286 - val_mae: 0.1244 - val_mse: 0.0584 - learning_rate: 7.0000e-05\n",
            "Epoch 53/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - loss: 0.1717 - mae: 0.0485 - mse: 0.0099\n",
            "Epoch 53: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 381ms/step - loss: 0.1716 - mae: 0.0485 - mse: 0.0099 - val_loss: 0.8576 - val_mae: 0.5730 - val_mse: 0.7664 - learning_rate: 7.0000e-05\n",
            "Epoch 53: early stopping\n",
            "Restoring model weights from the end of the best epoch: 23.\n",
            "\n",
            "[PHASE 2: FINE-TUNING]\n",
            "Epoch 1/100\n",
            "2025-12-06 14:30:30.951547: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-input.15 = (f16[128,34,34,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[128,32,32,64]{3,2,1,0} %bitcast.27354, f16[64,3,3,128]{3,2,1,0} %bitcast.27356), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_15_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:30:31.058275: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.106846303s\n",
            "Trying algorithm eng0{} for conv %cudnn-conv-bw-input.15 = (f16[128,34,34,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[128,32,32,64]{3,2,1,0} %bitcast.27354, f16[64,3,3,128]{3,2,1,0} %bitcast.27356), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_15_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:31:03.576687: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-input.29 = (f16[32,256,256,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,256,256,64]{3,2,1,0} %bitcast.29427, f16[64,3,3,64]{3,2,1,0} %bitcast.29278), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_1_2/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:31:14.510584: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 11.933987282s\n",
            "Trying algorithm eng0{} for conv %cudnn-conv-bw-input.29 = (f16[32,256,256,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,256,256,64]{3,2,1,0} %bitcast.29427, f16[64,3,3,64]{3,2,1,0} %bitcast.29278), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_1_2/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:31:19.439943: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.18 = (f16[64,3,3,4]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,256,256,4]{3,2,1,0} %bitcast.28346, f16[32,256,256,64]{3,2,1,0} %bitcast.29359), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:31:21.662770: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 3.222908543s\n",
            "Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.18 = (f16[64,3,3,4]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,256,256,4]{3,2,1,0} %bitcast.28346, f16[32,256,256,64]{3,2,1,0} %bitcast.29359), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:31:30.224155: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.20 = (f16[128,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,128,128,64]{3,2,1,0} %bitcast.29499, f16[32,128,128,128]{3,2,1,0} %bitcast.29501), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:31:37.794741: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 8.57068183s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.20 = (f16[128,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,128,128,64]{3,2,1,0} %bitcast.29499, f16[32,128,128,128]{3,2,1,0} %bitcast.29501), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:31:38.794989: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=1} for conv %cudnn-conv-bw-filter.20 = (f16[128,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,128,128,64]{3,2,1,0} %bitcast.29499, f16[32,128,128,128]{3,2,1,0} %bitcast.29501), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:31:45.100600: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 7.305730145s\n",
            "Trying algorithm eng19{k2=1} for conv %cudnn-conv-bw-filter.20 = (f16[128,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,128,128,64]{3,2,1,0} %bitcast.29499, f16[32,128,128,128]{3,2,1,0} %bitcast.29501), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:31:50.423470: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.21 = (f16[128,3,3,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,128,128,128]{3,2,1,0} %bitcast.29573, f16[32,128,128,128]{3,2,1,0} %bitcast.29575), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_3_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:32:08.214783: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 18.791404367s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.21 = (f16[128,3,3,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,128,128,128]{3,2,1,0} %bitcast.29573, f16[32,128,128,128]{3,2,1,0} %bitcast.29575), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_3_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:32:11.539096: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.22 = (f16[256,3,3,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,64,64,128]{3,2,1,0} %bitcast.29647, f16[32,64,64,256]{3,2,1,0} %bitcast.29649), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_4_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:32:17.877774: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 7.33876585s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.22 = (f16[256,3,3,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,64,64,128]{3,2,1,0} %bitcast.29647, f16[32,64,64,256]{3,2,1,0} %bitcast.29649), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_4_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:32:32.275383: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng53{k2=14,k6=1,k13=1,k14=0,k22=3} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,64,64,512]{3,2,1,0} %bitcast.29943, f16[32,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:32:32.841918: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.566635835s\n",
            "Trying algorithm eng53{k2=14,k6=1,k13=1,k14=0,k22=3} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,64,64,512]{3,2,1,0} %bitcast.29943, f16[32,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:32:33.842151: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng53{k2=14,k6=0,k13=1,k14=0,k22=3} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,64,64,512]{3,2,1,0} %bitcast.29943, f16[32,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:32:34.131507: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.289429956s\n",
            "Trying algorithm eng53{k2=14,k6=0,k13=1,k14=0,k22=3} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,64,64,512]{3,2,1,0} %bitcast.29943, f16[32,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:32:35.131733: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,64,64,512]{3,2,1,0} %bitcast.29943, f16[32,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:34:14.505647: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1m40.37400672s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,64,64,512]{3,2,1,0} %bitcast.29943, f16[32,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:34:15.505925: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,64,64,512]{3,2,1,0} %bitcast.29943, f16[32,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:44:45.197824: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 10m30.691986934s\n",
            "Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,64,64,512]{3,2,1,0} %bitcast.29943, f16[32,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:44:51.977353: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng53{k2=14,k6=1,k13=1,k14=0,k22=3} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[128,34,34,512]{3,2,1,0} %bitcast.26623, f16[128,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:44:52.195188: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.2179447s\n",
            "Trying algorithm eng53{k2=14,k6=1,k13=1,k14=0,k22=3} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[128,34,34,512]{3,2,1,0} %bitcast.26623, f16[128,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:44:53.990118: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[128,34,34,512]{3,2,1,0} %bitcast.26623, f16[128,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:46:38.029799: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1m45.039762619s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[128,34,34,512]{3,2,1,0} %bitcast.26623, f16[128,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:46:39.030036: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[128,34,34,512]{3,2,1,0} %bitcast.26623, f16[128,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:57:21.885409: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 10m43.855444264s\n",
            "Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[128,34,34,512]{3,2,1,0} %bitcast.26623, f16[128,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:57:27.802292: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.31 = (f16[256,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[128,34,34,512]{3,2,1,0} %bitcast.26911, f16[128,32,32,256]{3,2,1,0} %bitcast.27594), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_13_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:58:08.084135: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 41.281936604s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.31 = (f16[256,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[128,34,34,512]{3,2,1,0} %bitcast.26911, f16[128,32,32,256]{3,2,1,0} %bitcast.27594), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_13_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 14:58:09.084560: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.31 = (f16[256,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[128,34,34,512]{3,2,1,0} %bitcast.26911, f16[128,32,32,256]{3,2,1,0} %bitcast.27594), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_13_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:03:01.328425: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 4m53.244149707s\n",
            "Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.31 = (f16[256,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[128,34,34,512]{3,2,1,0} %bitcast.26911, f16[128,32,32,256]{3,2,1,0} %bitcast.27594), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_13_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 684ms/step - loss: 0.2187 - mae: 0.0509 - mse: 0.01332025-12-06 15:03:57.201054: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-input.15 = (f16[124,34,34,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[124,32,32,64]{3,2,1,0} %bitcast.27354, f16[64,3,3,128]{3,2,1,0} %bitcast.27356), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_15_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:03:57.260369: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.059404853s\n",
            "Trying algorithm eng0{} for conv %cudnn-conv-bw-input.15 = (f16[124,34,34,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[124,32,32,64]{3,2,1,0} %bitcast.27354, f16[64,3,3,128]{3,2,1,0} %bitcast.27356), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_15_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:04:15.144607: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng25{k2=2,k3=0} for conv %cudnn-conv-bw-input.21 = (f16[31,64,64,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,64,64,512]{3,2,1,0} %bitcast.30019, f16[512,3,3,512]{3,2,1,0} %bitcast.28580), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_9_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:04:15.230754: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.08627132s\n",
            "Trying algorithm eng25{k2=2,k3=0} for conv %cudnn-conv-bw-input.21 = (f16[31,64,64,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,64,64,512]{3,2,1,0} %bitcast.30019, f16[512,3,3,512]{3,2,1,0} %bitcast.28580), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_9_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:04:28.873520: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-input.29 = (f16[31,256,256,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,256,256,64]{3,2,1,0} %bitcast.29427, f16[64,3,3,64]{3,2,1,0} %bitcast.29278), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_1_2/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:04:39.444017: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 11.57063314s\n",
            "Trying algorithm eng0{} for conv %cudnn-conv-bw-input.29 = (f16[31,256,256,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,256,256,64]{3,2,1,0} %bitcast.29427, f16[64,3,3,64]{3,2,1,0} %bitcast.29278), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_1_2/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:04:44.538827: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.18 = (f16[64,3,3,4]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,256,256,4]{3,2,1,0} %bitcast.28346, f16[31,256,256,64]{3,2,1,0} %bitcast.29359), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:04:46.635475: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 3.096725748s\n",
            "Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.18 = (f16[64,3,3,4]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,256,256,4]{3,2,1,0} %bitcast.28346, f16[31,256,256,64]{3,2,1,0} %bitcast.29359), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:04:55.448112: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.20 = (f16[128,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,128,128,64]{3,2,1,0} %bitcast.29499, f16[31,128,128,128]{3,2,1,0} %bitcast.29501), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:05:02.726361: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 8.278759551s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.20 = (f16[128,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,128,128,64]{3,2,1,0} %bitcast.29499, f16[31,128,128,128]{3,2,1,0} %bitcast.29501), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:05:03.726572: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=1} for conv %cudnn-conv-bw-filter.20 = (f16[128,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,128,128,64]{3,2,1,0} %bitcast.29499, f16[31,128,128,128]{3,2,1,0} %bitcast.29501), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:05:09.844144: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 7.117643475s\n",
            "Trying algorithm eng19{k2=1} for conv %cudnn-conv-bw-filter.20 = (f16[128,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,128,128,64]{3,2,1,0} %bitcast.29499, f16[31,128,128,128]{3,2,1,0} %bitcast.29501), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:05:15.474554: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.21 = (f16[128,3,3,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,128,128,128]{3,2,1,0} %bitcast.29573, f16[31,128,128,128]{3,2,1,0} %bitcast.29575), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_3_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:05:32.650899: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 18.17639604s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.21 = (f16[128,3,3,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,128,128,128]{3,2,1,0} %bitcast.29573, f16[31,128,128,128]{3,2,1,0} %bitcast.29575), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_3_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:05:35.949900: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.22 = (f16[256,3,3,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,64,64,128]{3,2,1,0} %bitcast.29647, f16[31,64,64,256]{3,2,1,0} %bitcast.29649), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_4_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:05:42.007775: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 7.057940723s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.22 = (f16[256,3,3,128]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,64,64,128]{3,2,1,0} %bitcast.29647, f16[31,64,64,256]{3,2,1,0} %bitcast.29649), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_4_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:05:57.133743: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng53{k2=14,k6=1,k13=1,k14=0,k22=3} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,64,64,512]{3,2,1,0} %bitcast.29943, f16[31,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:05:57.278126: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.144476988s\n",
            "Trying algorithm eng53{k2=14,k6=1,k13=1,k14=0,k22=3} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,64,64,512]{3,2,1,0} %bitcast.29943, f16[31,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:05:58.278381: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng53{k2=14,k6=0,k13=1,k14=0,k22=3} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,64,64,512]{3,2,1,0} %bitcast.29943, f16[31,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:05:58.531267: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.253013249s\n",
            "Trying algorithm eng53{k2=14,k6=0,k13=1,k14=0,k22=3} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,64,64,512]{3,2,1,0} %bitcast.29943, f16[31,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:05:59.531506: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,64,64,512]{3,2,1,0} %bitcast.29943, f16[31,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:07:35.286778: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1m36.755365432s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,64,64,512]{3,2,1,0} %bitcast.29943, f16[31,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:07:36.287018: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,64,64,512]{3,2,1,0} %bitcast.29943, f16[31,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:17:41.921914: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 10m6.634963179s\n",
            "Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.26 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[31,64,64,512]{3,2,1,0} %bitcast.29943, f16[31,64,64,512]{3,2,1,0} %bitcast.29945), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_8_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:17:48.859525: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng53{k2=14,k6=1,k13=1,k14=0,k22=3} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[124,34,34,512]{3,2,1,0} %bitcast.26623, f16[124,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:17:49.027276: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.170016751s\n",
            "Trying algorithm eng53{k2=14,k6=1,k13=1,k14=0,k22=3} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[124,34,34,512]{3,2,1,0} %bitcast.26623, f16[124,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:17:50.028539: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng53{k2=14,k6=0,k13=1,k14=0,k22=3} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[124,34,34,512]{3,2,1,0} %bitcast.26623, f16[124,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:17:50.725856: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.698415219s\n",
            "Trying algorithm eng53{k2=14,k6=0,k13=1,k14=0,k22=3} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[124,34,34,512]{3,2,1,0} %bitcast.26623, f16[124,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:17:52.509465: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[124,34,34,512]{3,2,1,0} %bitcast.26623, f16[124,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:19:32.801086: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1m41.291680156s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[124,34,34,512]{3,2,1,0} %bitcast.26623, f16[124,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:19:33.801478: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[124,34,34,512]{3,2,1,0} %bitcast.26623, f16[124,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:29:43.931108: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 10m11.129713198s\n",
            "Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.28 = (f16[512,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[124,34,34,512]{3,2,1,0} %bitcast.26623, f16[124,32,32,512]{3,2,1,0} %bitcast.28487), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_10_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:29:50.421425: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.31 = (f16[256,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[124,34,34,512]{3,2,1,0} %bitcast.26911, f16[124,32,32,256]{3,2,1,0} %bitcast.27594), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_13_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:30:29.960640: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 40.539297332s\n",
            "Trying algorithm eng19{k2=0} for conv %cudnn-conv-bw-filter.31 = (f16[256,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[124,34,34,512]{3,2,1,0} %bitcast.26911, f16[124,32,32,256]{3,2,1,0} %bitcast.27594), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_13_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:30:30.960869: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.31 = (f16[256,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[124,34,34,512]{3,2,1,0} %bitcast.26911, f16[124,32,32,256]{3,2,1,0} %bitcast.27594), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_13_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 15:35:11.380479: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 4m41.419691101s\n",
            "Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.31 = (f16[256,3,3,512]{3,2,1,0}, u8[0]{0}) custom-call(f16[124,34,34,512]{3,2,1,0} %bitcast.26911, f16[124,32,32,256]{3,2,1,0} %bitcast.27594), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/CSRNet-Advanced_1/conv2d_13_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3948s\u001b[0m 276s/step - loss: 0.2189 - mae: 0.0514 - mse: 0.0134 - val_loss: 0.2484 - val_mae: 0.0844 - val_mse: 0.0420 - learning_rate: 2.0000e-05\n",
            "Epoch 2/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 737ms/step - loss: 0.2146 - mae: 0.0482 - mse: 0.0094 - val_loss: 0.2530 - val_mae: 0.0885 - val_mse: 0.0478 - learning_rate: 2.0000e-05\n",
            "Epoch 3/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 752ms/step - loss: 0.2155 - mae: 0.0500 - mse: 0.0110 - val_loss: 0.2504 - val_mae: 0.0897 - val_mse: 0.0446 - learning_rate: 2.0000e-05\n",
            "Epoch 4/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 789ms/step - loss: 0.2140 - mae: 0.0488 - mse: 0.0105 - val_loss: 0.2430 - val_mae: 0.0796 - val_mse: 0.0394 - learning_rate: 2.0000e-05\n",
            "Epoch 5/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 775ms/step - loss: 0.2124 - mae: 0.0480 - mse: 0.0095 - val_loss: 0.2440 - val_mae: 0.0837 - val_mse: 0.0401 - learning_rate: 2.0000e-05\n",
            "Epoch 6/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 764ms/step - loss: 0.2127 - mae: 0.0495 - mse: 0.0103 - val_loss: 0.2427 - val_mae: 0.0832 - val_mse: 0.0393 - learning_rate: 2.0000e-05\n",
            "Epoch 7/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 747ms/step - loss: 0.2118 - mae: 0.0490 - mse: 0.0101 - val_loss: 0.2430 - val_mae: 0.0845 - val_mse: 0.0403 - learning_rate: 2.0000e-05\n",
            "Epoch 8/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 784ms/step - loss: 0.2096 - mae: 0.0463 - mse: 0.0092 - val_loss: 0.2436 - val_mae: 0.0876 - val_mse: 0.0408 - learning_rate: 2.0000e-05\n",
            "Epoch 9/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 729ms/step - loss: 0.2077 - mae: 0.0451 - mse: 0.0079 - val_loss: 0.2421 - val_mae: 0.0860 - val_mse: 0.0403 - learning_rate: 2.0000e-05\n",
            "Epoch 10/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 768ms/step - loss: 0.2077 - mae: 0.0466 - mse: 0.0083 - val_loss: 0.2403 - val_mae: 0.0841 - val_mse: 0.0396 - learning_rate: 2.0000e-05\n",
            "Epoch 11/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 719ms/step - loss: 0.2060 - mae: 0.0441 - mse: 0.0079 - val_loss: 0.2419 - val_mae: 0.0880 - val_mse: 0.0411 - learning_rate: 2.0000e-05\n",
            "Epoch 12/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 716ms/step - loss: 0.2083 - mae: 0.0488 - mse: 0.0102 - val_loss: 0.2452 - val_mae: 0.0950 - val_mse: 0.0438 - learning_rate: 2.0000e-05\n",
            "Epoch 13/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 719ms/step - loss: 0.2048 - mae: 0.0441 - mse: 0.0081 - val_loss: 0.2460 - val_mae: 0.0995 - val_mse: 0.0440 - learning_rate: 2.0000e-05\n",
            "Epoch 14/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 716ms/step - loss: 0.2040 - mae: 0.0437 - mse: 0.0080 - val_loss: 0.2452 - val_mae: 0.0987 - val_mse: 0.0441 - learning_rate: 2.0000e-05\n",
            "Epoch 15/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 723ms/step - loss: 0.2072 - mae: 0.0502 - mse: 0.0108 - val_loss: 0.2446 - val_mae: 0.0993 - val_mse: 0.0438 - learning_rate: 2.0000e-05\n",
            "Epoch 16/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 725ms/step - loss: 0.2004 - mae: 0.0401 - mse: 0.0062 - val_loss: 0.2540 - val_mae: 0.1156 - val_mse: 0.0511 - learning_rate: 2.0000e-05\n",
            "Epoch 17/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 735ms/step - loss: 0.2014 - mae: 0.0432 - mse: 0.0072 - val_loss: 0.2594 - val_mae: 0.1244 - val_mse: 0.0559 - learning_rate: 2.0000e-05\n",
            "Epoch 18/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 732ms/step - loss: 0.2021 - mae: 0.0451 - mse: 0.0082 - val_loss: 0.2436 - val_mae: 0.0998 - val_mse: 0.0449 - learning_rate: 2.0000e-05\n",
            "Epoch 19/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 782ms/step - loss: 0.2003 - mae: 0.0437 - mse: 0.0071 - val_loss: 0.2541 - val_mae: 0.1183 - val_mse: 0.0527 - learning_rate: 2.0000e-05\n",
            "Epoch 20/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 735ms/step - loss: 0.1976 - mae: 0.0396 - mse: 0.0059 - val_loss: 0.2585 - val_mae: 0.1245 - val_mse: 0.0572 - learning_rate: 2.0000e-05\n",
            "Epoch 21/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 741ms/step - loss: 0.1997 - mae: 0.0440 - mse: 0.0078 - val_loss: 0.2482 - val_mae: 0.1112 - val_mse: 0.0489 - learning_rate: 2.0000e-05\n",
            "Epoch 22/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 785ms/step - loss: 0.1983 - mae: 0.0434 - mse: 0.0069 - val_loss: 0.2449 - val_mae: 0.1058 - val_mse: 0.0474 - learning_rate: 2.0000e-05\n",
            "Epoch 23/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 737ms/step - loss: 0.1973 - mae: 0.0419 - mse: 0.0070 - val_loss: 0.2591 - val_mae: 0.1271 - val_mse: 0.0594 - learning_rate: 2.0000e-05\n",
            "Epoch 24/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687ms/step - loss: 0.1985 - mae: 0.0450 - mse: 0.0081\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 1.5999999595806004e-05.\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 736ms/step - loss: 0.1986 - mae: 0.0451 - mse: 0.0082 - val_loss: 0.2504 - val_mae: 0.1157 - val_mse: 0.0526 - learning_rate: 2.0000e-05\n",
            "Epoch 25/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 732ms/step - loss: 0.1955 - mae: 0.0408 - mse: 0.0064 - val_loss: 0.2411 - val_mae: 0.1026 - val_mse: 0.0455 - learning_rate: 1.6000e-05\n",
            "Epoch 26/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 735ms/step - loss: 0.1959 - mae: 0.0420 - mse: 0.0071 - val_loss: 0.2657 - val_mae: 0.1365 - val_mse: 0.0668 - learning_rate: 1.6000e-05\n",
            "Epoch 27/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 730ms/step - loss: 0.1995 - mae: 0.0483 - mse: 0.0101 - val_loss: 0.2617 - val_mae: 0.1324 - val_mse: 0.0634 - learning_rate: 1.6000e-05\n",
            "Epoch 28/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 733ms/step - loss: 0.1927 - mae: 0.0385 - mse: 0.0052 - val_loss: 0.2488 - val_mae: 0.1144 - val_mse: 0.0533 - learning_rate: 1.6000e-05\n",
            "Epoch 29/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 729ms/step - loss: 0.1970 - mae: 0.0477 - mse: 0.0081 - val_loss: 0.2648 - val_mae: 0.1390 - val_mse: 0.0663 - learning_rate: 1.6000e-05\n",
            "Epoch 30/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 734ms/step - loss: 0.1939 - mae: 0.0427 - mse: 0.0064 - val_loss: 0.2603 - val_mae: 0.1353 - val_mse: 0.0619 - learning_rate: 1.6000e-05\n",
            "Epoch 31/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 729ms/step - loss: 0.1942 - mae: 0.0422 - mse: 0.0076 - val_loss: 0.2854 - val_mae: 0.1658 - val_mse: 0.0853 - learning_rate: 1.6000e-05\n",
            "Epoch 32/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 733ms/step - loss: 0.1934 - mae: 0.0423 - mse: 0.0069 - val_loss: 0.2531 - val_mae: 0.1246 - val_mse: 0.0575 - learning_rate: 1.6000e-05\n",
            "Epoch 33/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 728ms/step - loss: 0.1905 - mae: 0.0378 - mse: 0.0053 - val_loss: 0.2680 - val_mae: 0.1441 - val_mse: 0.0709 - learning_rate: 1.6000e-05\n",
            "Epoch 34/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 726ms/step - loss: 0.1917 - mae: 0.0408 - mse: 0.0063 - val_loss: 0.2728 - val_mae: 0.1528 - val_mse: 0.0746 - learning_rate: 1.6000e-05\n",
            "Epoch 35/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 728ms/step - loss: 0.1921 - mae: 0.0422 - mse: 0.0070 - val_loss: 0.2736 - val_mae: 0.1535 - val_mse: 0.0761 - learning_rate: 1.6000e-05\n",
            "Epoch 36/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 726ms/step - loss: 0.1892 - mae: 0.0375 - mse: 0.0054 - val_loss: 0.2819 - val_mae: 0.1650 - val_mse: 0.0836 - learning_rate: 1.6000e-05\n",
            "Epoch 37/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 728ms/step - loss: 0.1899 - mae: 0.0400 - mse: 0.0058 - val_loss: 0.2987 - val_mae: 0.1782 - val_mse: 0.1025 - learning_rate: 1.6000e-05\n",
            "Epoch 38/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 727ms/step - loss: 0.1920 - mae: 0.0433 - mse: 0.0080 - val_loss: 0.2984 - val_mae: 0.1799 - val_mse: 0.1019 - learning_rate: 1.6000e-05\n",
            "Epoch 39/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 733ms/step - loss: 0.1886 - mae: 0.0393 - mse: 0.0055 - val_loss: 0.3020 - val_mae: 0.1833 - val_mse: 0.1062 - learning_rate: 1.6000e-05\n",
            "Epoch 40/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 728ms/step - loss: 0.1893 - mae: 0.0408 - mse: 0.0063 - val_loss: 0.2868 - val_mae: 0.1678 - val_mse: 0.0916 - learning_rate: 1.6000e-05\n",
            "Epoch 41/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 734ms/step - loss: 0.1884 - mae: 0.0398 - mse: 0.0059 - val_loss: 0.3074 - val_mae: 0.1876 - val_mse: 0.1131 - learning_rate: 1.6000e-05\n",
            "Epoch 42/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 727ms/step - loss: 0.1882 - mae: 0.0405 - mse: 0.0059 - val_loss: 0.2923 - val_mae: 0.1771 - val_mse: 0.0966 - learning_rate: 1.6000e-05\n",
            "Epoch 43/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 733ms/step - loss: 0.1905 - mae: 0.0457 - mse: 0.0075 - val_loss: 0.3032 - val_mae: 0.1894 - val_mse: 0.1074 - learning_rate: 1.6000e-05\n",
            "Epoch 44/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682ms/step - loss: 0.1871 - mae: 0.0399 - mse: 0.0057\n",
            "Epoch 44: ReduceLROnPlateau reducing learning rate to 1.2799999967683107e-05.\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 730ms/step - loss: 0.1870 - mae: 0.0398 - mse: 0.0057 - val_loss: 0.3357 - val_mae: 0.2181 - val_mse: 0.1420 - learning_rate: 1.6000e-05\n",
            "Epoch 45/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 731ms/step - loss: 0.1875 - mae: 0.0412 - mse: 0.0063 - val_loss: 0.3013 - val_mae: 0.1869 - val_mse: 0.1067 - learning_rate: 1.2800e-05\n",
            "Epoch 46/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 732ms/step - loss: 0.1881 - mae: 0.0423 - mse: 0.0071 - val_loss: 0.2904 - val_mae: 0.1747 - val_mse: 0.0968 - learning_rate: 1.2800e-05\n",
            "Epoch 47/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 731ms/step - loss: 0.1865 - mae: 0.0400 - mse: 0.0061 - val_loss: 0.2999 - val_mae: 0.1898 - val_mse: 0.1043 - learning_rate: 1.2800e-05\n",
            "Epoch 48/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 734ms/step - loss: 0.1851 - mae: 0.0381 - mse: 0.0053 - val_loss: 0.3339 - val_mae: 0.2195 - val_mse: 0.1405 - learning_rate: 1.2800e-05\n",
            "Epoch 49/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 732ms/step - loss: 0.1844 - mae: 0.0378 - mse: 0.0049 - val_loss: 0.3196 - val_mae: 0.2063 - val_mse: 0.1261 - learning_rate: 1.2800e-05\n",
            "Epoch 50/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 741ms/step - loss: 0.1891 - mae: 0.0459 - mse: 0.0086 - val_loss: 0.3076 - val_mae: 0.1930 - val_mse: 0.1151 - learning_rate: 1.2800e-05\n",
            "Epoch 51/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 732ms/step - loss: 0.1854 - mae: 0.0402 - mse: 0.0062 - val_loss: 0.3133 - val_mae: 0.1969 - val_mse: 0.1220 - learning_rate: 1.2800e-05\n",
            "Epoch 52/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 781ms/step - loss: 0.1852 - mae: 0.0406 - mse: 0.0060 - val_loss: 0.2872 - val_mae: 0.1767 - val_mse: 0.0937 - learning_rate: 1.2800e-05\n",
            "Epoch 53/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 734ms/step - loss: 0.1830 - mae: 0.0368 - mse: 0.0050 - val_loss: 0.3177 - val_mae: 0.2026 - val_mse: 0.1266 - learning_rate: 1.2800e-05\n",
            "Epoch 54/100\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 779ms/step - loss: 0.1841 - mae: 0.0403 - mse: 0.0054 - val_loss: 0.3270 - val_mae: 0.2117 - val_mse: 0.1363 - learning_rate: 1.2800e-05\n",
            "Epoch 54: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "\n",
            "================================================================================\n",
            "[EVALUATION]\n",
            "================================================================================\n",
            "\n",
            "RESULTS:\n",
            "  MAE:   170.65 (Target: 70-150)\n",
            "  RMSE:  236.52 (Target: 100-200)\n",
            "\n",
            "TARGET ACHIEVEMENT:\n",
            "  MAE  [70-150]:    FAIL\n",
            "  RMSE [100-200]:   FAIL\n",
            "\n",
            "First 25 predictions:\n",
            "   1: pred=    544, true=    172, error=   372\n",
            "   2: pred=    486, true=    502, error=    16\n",
            "   3: pred=    245, true=    389, error=   144\n",
            "   4: pred=    285, true=    211, error=    74\n",
            "   5: pred=    432, true=    223, error=   209\n",
            "   6: pred=    360, true=    431, error=    71\n",
            "   7: pred=   1163, true=   1175, error=    12\n",
            "   8: pred=    344, true=    265, error=    79\n",
            "   9: pred=   1132, true=   1232, error=   100\n",
            "  10: pred=    319, true=    289, error=    30\n",
            "  11: pred=    254, true=    182, error=    72\n",
            "  12: pred=    530, true=    379, error=   152\n",
            "  13: pred=    966, true=   1068, error=   102\n",
            "  14: pred=    658, true=   1021, error=   363\n",
            "  15: pred=    121, true=    452, error=   331\n",
            "  16: pred=    594, true=    256, error=   338\n",
            "  17: pred=    128, true=     66, error=    62\n",
            "  18: pred=    151, true=    141, error=    10\n",
            "  19: pred=   1269, true=   1191, error=    78\n",
            "  20: pred=    333, true=    288, error=    45\n",
            "  21: pred=   1377, true=   1603, error=   226\n",
            "  22: pred=    263, true=    241, error=    22\n",
            "  23: pred=    378, true=    250, error=   128\n",
            "  24: pred=      2, true=    321, error=   319\n",
            "  25: pred=    181, true=    133, error=    48\n",
            "\n",
            "✓ Results saved to: results/csrnet_advanced_gpu\n",
            "================================================================================\n",
            "\n",
            "⚠ Check results - may need further optimization\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_code = '''#!/usr/bin/env python3\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import sys, pickle, numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"CSRNet - ULTIMATE SOLUTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(f\"\\\\nGPU: {len(gpus) > 0}\")\n",
        "\n",
        "CONFIG = {\n",
        "    'batch_size': 8,\n",
        "    'epochs': 300,\n",
        "    'learning_rate': 5e-5,\n",
        "    'weight_decay': 1e-4,\n",
        "    'data_path': 'processed_dataset_fixed/part_A_fixed.pkl',\n",
        "    'results_path': 'results/csrnet_ultimate_ensemble',\n",
        "}\n",
        "\n",
        "# LOAD DATA\n",
        "print(\"\\\\n[LOADING DATA]\")\n",
        "\n",
        "with open(CONFIG['data_path'], 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "X_train_raw = data['X_train'].astype('float32')\n",
        "y_density_train_raw = data['y_density_train'][..., np.newaxis].astype('float32')\n",
        "X_test_raw = data['X_test'].astype('float32')\n",
        "y_count_test = data['y_count_test']\n",
        "\n",
        "# Per-image normalization\n",
        "print(\"Normalizing per-image...\")\n",
        "X_train_norm = np.zeros_like(X_train_raw)\n",
        "X_test_norm = np.zeros_like(X_test_raw)\n",
        "\n",
        "for i in range(len(X_train_raw)):\n",
        "    for c in range(3):\n",
        "        X_train_norm[i,:,:,c] = (X_train_raw[i,:,:,c] - X_train_raw[i,:,:,c].mean()) / (X_train_raw[i,:,:,c].std() + 1e-5)\n",
        "\n",
        "for i in range(len(X_test_raw)):\n",
        "    for c in range(3):\n",
        "        X_test_norm[i,:,:,c] = (X_test_raw[i,:,:,c] - X_test_raw[i,:,:,c].mean()) / (X_test_raw[i,:,:,c].std() + 1e-5)\n",
        "\n",
        "y_density_train_norm = np.zeros_like(y_density_train_raw)\n",
        "for i in range(len(y_density_train_raw)):\n",
        "    max_val = y_density_train_raw[i].max()\n",
        "    if max_val > 0:\n",
        "        y_density_train_norm[i] = y_density_train_raw[i] / max_val\n",
        "\n",
        "X_train, X_test = X_train_norm, X_test_norm\n",
        "y_density_train = y_density_train_norm\n",
        "\n",
        "print(f\"✓ Train: {X_train.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_density_train, test_size=0.1, random_state=42)\n",
        "print(f\"✓ Split: train={X_tr.shape[0]}, val={X_val.shape[0]}\")\n",
        "\n",
        "# BUILD MODEL\n",
        "print(\"\\\\n[BUILDING MODEL]\")\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(256, 256, 3)),\n",
        "\n",
        "    # Block 1\n",
        "    layers.Conv2D(64, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(64, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D(2),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    # Block 2\n",
        "    layers.Conv2D(128, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(128, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D(2),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    # Block 3\n",
        "    layers.Conv2D(256, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(256, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(256, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    # layers.MaxPooling2D(2),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    # Block 4\n",
        "    layers.Conv2D(512, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(512, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(512, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "\n",
        "    # Backend\n",
        "    layers.Conv2D(512, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Conv2D(512, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Conv2D(512, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(256, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(128, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(64, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "\n",
        "    layers.Conv2D(1, 1, padding='same', activation='relu'),\n",
        "], name='CSRNet-Ultimate')\n",
        "\n",
        "# Robust loss\n",
        "def robust_loss(y_true, y_pred):\n",
        "    huber = tf.keras.losses.Huber(delta=0.5)\n",
        "    mae = tf.keras.losses.mae\n",
        "    return 0.8 * huber(y_true, y_pred) + 0.2 * mae(y_true, y_pred)\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=CONFIG['learning_rate'])\n",
        "model.compile(optimizer=optimizer, loss=robust_loss, metrics=['mae', 'mse'])\n",
        "print(f\"✓ Parameters: {model.count_params():,}\")\n",
        "\n",
        "# TRAIN\n",
        "print(\"\\\\n[ULTRA-CONSERVATIVE TRAINING]\")\n",
        "print(f\"Batch: {CONFIG['batch_size']}, LR: {CONFIG['learning_rate']}, Epochs: {CONFIG['epochs']}\\\\n\")\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_mae', patience=50, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_mae', factor=0.9, patience=20, min_lr=1e-8, verbose=1),\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    epochs=CONFIG['epochs'],\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# EVALUATE\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"[EVALUATION WITH POST-PROCESSING]\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "predictions_normalized = []\n",
        "for i in range(0, len(X_test), 16):\n",
        "    batch = X_test[i:i+16]\n",
        "    pred_batch = model.predict(batch, verbose=0)\n",
        "    predictions_normalized.extend([max(0, p.sum()) for p in pred_batch])\n",
        "\n",
        "predictions_normalized = np.array(predictions_normalized)\n",
        "\n",
        "# Denormalize\n",
        "predictions = np.zeros_like(predictions_normalized)\n",
        "for i in range(len(predictions_normalized)):\n",
        "    max_density = y_density_train_raw[i % len(y_density_train_raw)].max()\n",
        "    predictions[i] = predictions_normalized[i] * max_density\n",
        "\n",
        "# Post-processing\n",
        "q25 = np.percentile(predictions, 25)\n",
        "q75 = np.percentile(predictions, 75)\n",
        "iqr = q75 - q25\n",
        "predictions_clipped = np.clip(predictions, q25 - 1.5*iqr, q75 + 1.5*iqr)\n",
        "\n",
        "mae = np.mean(np.abs(predictions_clipped - y_count_test))\n",
        "rmse = np.sqrt(np.mean((predictions_clipped - y_count_test) ** 2))\n",
        "\n",
        "print(f\"\\\\nRESULTS:\")\n",
        "print(f\"  MAE:   {mae:.2f} (Target: 70-150)\")\n",
        "print(f\"  RMSE:  {rmse:.2f} (Target: 100-200)\")\n",
        "\n",
        "mae_pass = 70 <= mae <= 150\n",
        "rmse_pass = 100 <= rmse <= 200\n",
        "\n",
        "print(f\"\\\\nTARGET ACHIEVEMENT:\")\n",
        "print(f\"  MAE  [70-150]:    {'PASS' if mae_pass else 'FAIL'}\")\n",
        "print(f\"  RMSE [100-200]:   {'PASS' if rmse_pass else 'FAIL'}\")\n",
        "\n",
        "print(f\"\\\\nFirst 25 predictions:\")\n",
        "for i in range(min(25, len(predictions_clipped))):\n",
        "    error = abs(predictions_clipped[i] - y_count_test[i])\n",
        "    print(f\"  {i+1:2d}: pred={predictions_clipped[i]:7.0f}, true={y_count_test[i]:7.0f}, error={error:6.0f}\")\n",
        "\n",
        "# SAVE\n",
        "results_dir = Path(CONFIG['results_path'])\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(results_dir / 'results.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'mae': float(mae),\n",
        "        'rmse': float(rmse),\n",
        "        'predictions': predictions_clipped.tolist(),\n",
        "        'ground_truth': y_count_test.tolist(),\n",
        "        'model_params': model.count_params(),\n",
        "        'mae_pass': bool(mae_pass),\n",
        "        'rmse_pass': bool(rmse_pass),\n",
        "    }, f)\n",
        "\n",
        "model.save(results_dir / 'model.keras')\n",
        "\n",
        "print(f\"\\\\n✓ Results saved to: {results_dir}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if mae_pass and rmse_pass:\n",
        "    print(\"\\\\n✓✓✓ SUCCESS! TARGETS ACHIEVED! ✓✓✓\\\\n\")\n",
        "else:\n",
        "    print(f\"\\\\nResults: MAE {mae:.2f}, RMSE {rmse:.2f}\\\\n\")\n",
        "'''\n",
        "\n",
        "with open('train_gpu_ultimate.py', 'w') as f:\n",
        "    f.write(training_code)\n",
        "\n",
        "print(\"✓ Ultimate training script created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y93Bh_7tCT6m",
        "outputId": "fe1038a4-2501-4586-eae0-9d8c6ab865b6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Ultimate training script created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_gpu_ultimate.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HB1Clgia_Kw",
        "outputId": "4511ef09-34b9-4d83-ab5a-c3bf1cd47c24"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 16:05:22.263601: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765037122.292898   52812 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765037122.300908   52812 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765037122.316851   52812 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765037122.316886   52812 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765037122.316891   52812 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765037122.316895   52812 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\n",
            "================================================================================\n",
            "CSRNet - ULTIMATE SOLUTION\n",
            "================================================================================\n",
            "\n",
            "GPU: True\n",
            "\n",
            "[LOADING DATA]\n",
            "Normalizing per-image...\n",
            "✓ Train: (300, 256, 256, 3), Test: (182, 256, 256, 3)\n",
            "✓ Split: train=270, val=30\n",
            "\n",
            "[BUILDING MODEL]\n",
            "I0000 00:00:1765037128.312727   52812 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "✓ Parameters: 16,282,177\n",
            "\n",
            "[ULTRA-CONSERVATIVE TRAINING]\n",
            "Batch: 8, LR: 5e-05, Epochs: 300\n",
            "\n",
            "Epoch 1/300\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1765037139.571196   52862 service.cc:152] XLA service 0x7d7300002ba0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1765037139.571247   52862 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "I0000 00:00:1765037141.496617   52862 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
            "2025-12-06 16:06:03.500630: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:06:03.794718: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:06:04.543155: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:06:04.762951: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:06:06.283553: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:06:06.619643: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng4{k11=1} for conv %cudnn-conv-bw-input.27 = (f32[8,128,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,128,128,128]{3,2,1,0} %bitcast.31553, f32[128,128,3,3]{3,2,1,0} %bitcast.25678), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Ultimate_1/conv2d_3_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 16:06:06.671409: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:06:06.680697: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.061542106s\n",
            "Trying algorithm eng4{k11=1} for conv %cudnn-conv-bw-input.27 = (f32[8,128,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,128,128,128]{3,2,1,0} %bitcast.31553, f32[128,128,3,3]{3,2,1,0} %bitcast.25678), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Ultimate_1/conv2d_3_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 16:06:07.647010: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:06:08.038151: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "I0000 00:00:1765037187.647073   52862 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 492ms/step - loss: 0.5418 - mae: 0.2509 - mse: 0.12472025-12-06 16:06:49.727159: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:06:49.943619: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:06:50.935266: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:06:51.229518: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:07:05.580172: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:07:05.876792: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:07:06.338651: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:07:06.634868: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:07:07.148993: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:07:07.372083: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:07:07.689023: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:07:07.913011: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:07:09.407312: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:07:09.749823: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng4{k11=1} for conv %cudnn-conv-bw-input.27 = (f32[6,128,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[6,128,128,128]{3,2,1,0} %bitcast.31550, f32[128,128,3,3]{3,2,1,0} %bitcast.25676), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Ultimate_1/conv2d_3_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 16:07:09.789854: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:07:09.797209: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.047503879s\n",
            "Trying algorithm eng4{k11=1} for conv %cudnn-conv-bw-input.27 = (f32[6,128,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[6,128,128,128]{3,2,1,0} %bitcast.31550, f32[128,128,3,3]{3,2,1,0} %bitcast.25676), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Ultimate_1/conv2d_3_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 16:07:10.751998: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:07:11.081024: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 2s/step - loss: 0.5416 - mae: 0.2506 - mse: 0.1245 - val_loss: 0.5297 - val_mae: 0.2295 - val_mse: 0.1129 - learning_rate: 5.0000e-05\n",
            "Epoch 2/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 557ms/step - loss: 0.5020 - mae: 0.1778 - mse: 0.0666 - val_loss: 0.5252 - val_mae: 0.2295 - val_mse: 0.1129 - learning_rate: 5.0000e-05\n",
            "Epoch 3/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 571ms/step - loss: 0.4780 - mae: 0.1353 - mse: 0.0379 - val_loss: 0.5196 - val_mae: 0.2295 - val_mse: 0.1129 - learning_rate: 5.0000e-05\n",
            "Epoch 4/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 553ms/step - loss: 0.4716 - mae: 0.1336 - mse: 0.0371 - val_loss: 0.5133 - val_mae: 0.2295 - val_mse: 0.1129 - learning_rate: 5.0000e-05\n",
            "Epoch 5/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 541ms/step - loss: 0.4625 - mae: 0.1263 - mse: 0.0343 - val_loss: 0.5063 - val_mae: 0.2295 - val_mse: 0.1129 - learning_rate: 5.0000e-05\n",
            "Epoch 6/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 544ms/step - loss: 0.4542 - mae: 0.1240 - mse: 0.0320 - val_loss: 0.4989 - val_mae: 0.2295 - val_mse: 0.1129 - learning_rate: 5.0000e-05\n",
            "Epoch 7/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 551ms/step - loss: 0.4438 - mae: 0.1160 - mse: 0.0288 - val_loss: 0.4908 - val_mae: 0.2289 - val_mse: 0.1121 - learning_rate: 5.0000e-05\n",
            "Epoch 8/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 555ms/step - loss: 0.4393 - mae: 0.1258 - mse: 0.0322 - val_loss: 0.4813 - val_mae: 0.2258 - val_mse: 0.1097 - learning_rate: 5.0000e-05\n",
            "Epoch 9/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 551ms/step - loss: 0.4262 - mae: 0.1106 - mse: 0.0272 - val_loss: 0.4700 - val_mae: 0.2194 - val_mse: 0.1045 - learning_rate: 5.0000e-05\n",
            "Epoch 10/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 550ms/step - loss: 0.4214 - mae: 0.1224 - mse: 0.0300 - val_loss: 0.4562 - val_mae: 0.2089 - val_mse: 0.0945 - learning_rate: 5.0000e-05\n",
            "Epoch 11/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 550ms/step - loss: 0.4141 - mae: 0.1255 - mse: 0.0311 - val_loss: 0.4391 - val_mae: 0.1914 - val_mse: 0.0807 - learning_rate: 5.0000e-05\n",
            "Epoch 12/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 550ms/step - loss: 0.4022 - mae: 0.1144 - mse: 0.0279 - val_loss: 0.4144 - val_mae: 0.1542 - val_mse: 0.0567 - learning_rate: 5.0000e-05\n",
            "Epoch 13/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 552ms/step - loss: 0.3935 - mae: 0.1138 - mse: 0.0279 - val_loss: 0.4050 - val_mae: 0.1524 - val_mse: 0.0553 - learning_rate: 5.0000e-05\n",
            "Epoch 14/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 551ms/step - loss: 0.3839 - mae: 0.1116 - mse: 0.0263 - val_loss: 0.3895 - val_mae: 0.1366 - val_mse: 0.0452 - learning_rate: 5.0000e-05\n",
            "Epoch 15/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 550ms/step - loss: 0.3735 - mae: 0.1064 - mse: 0.0241 - val_loss: 0.3885 - val_mae: 0.1547 - val_mse: 0.0553 - learning_rate: 5.0000e-05\n",
            "Epoch 16/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 549ms/step - loss: 0.3666 - mae: 0.1113 - mse: 0.0257 - val_loss: 0.3794 - val_mae: 0.1542 - val_mse: 0.0539 - learning_rate: 5.0000e-05\n",
            "Epoch 17/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 552ms/step - loss: 0.3555 - mae: 0.1033 - mse: 0.0229 - val_loss: 0.3556 - val_mae: 0.1169 - val_mse: 0.0327 - learning_rate: 5.0000e-05\n",
            "Epoch 18/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.3473 - mae: 0.1046 - mse: 0.0226 - val_loss: 0.3506 - val_mae: 0.1249 - val_mse: 0.0372 - learning_rate: 5.0000e-05\n",
            "Epoch 19/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.3385 - mae: 0.1028 - mse: 0.0223 - val_loss: 0.3444 - val_mae: 0.1293 - val_mse: 0.0402 - learning_rate: 5.0000e-05\n",
            "Epoch 20/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.3284 - mae: 0.0970 - mse: 0.0202 - val_loss: 0.3409 - val_mae: 0.1404 - val_mse: 0.0457 - learning_rate: 5.0000e-05\n",
            "Epoch 21/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.3244 - mae: 0.1088 - mse: 0.0243 - val_loss: 0.3293 - val_mae: 0.1314 - val_mse: 0.0409 - learning_rate: 5.0000e-05\n",
            "Epoch 22/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.3146 - mae: 0.1028 - mse: 0.0222 - val_loss: 0.3378 - val_mae: 0.1685 - val_mse: 0.0644 - learning_rate: 5.0000e-05\n",
            "Epoch 23/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 549ms/step - loss: 0.3046 - mae: 0.0955 - mse: 0.0200 - val_loss: 0.3079 - val_mae: 0.1167 - val_mse: 0.0326 - learning_rate: 5.0000e-05\n",
            "Epoch 24/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.2990 - mae: 0.1003 - mse: 0.0223 - val_loss: 0.3041 - val_mae: 0.1253 - val_mse: 0.0369 - learning_rate: 5.0000e-05\n",
            "Epoch 25/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.2904 - mae: 0.0973 - mse: 0.0207 - val_loss: 0.2993 - val_mae: 0.1307 - val_mse: 0.0405 - learning_rate: 5.0000e-05\n",
            "Epoch 26/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.2843 - mae: 0.1005 - mse: 0.0217 - val_loss: 0.2893 - val_mae: 0.1234 - val_mse: 0.0366 - learning_rate: 5.0000e-05\n",
            "Epoch 27/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.2761 - mae: 0.0965 - mse: 0.0204 - val_loss: 0.2798 - val_mae: 0.1169 - val_mse: 0.0324 - learning_rate: 5.0000e-05\n",
            "Epoch 28/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 551ms/step - loss: 0.2712 - mae: 0.1026 - mse: 0.0220 - val_loss: 0.2696 - val_mae: 0.1073 - val_mse: 0.0284 - learning_rate: 5.0000e-05\n",
            "Epoch 29/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.2634 - mae: 0.0976 - mse: 0.0213 - val_loss: 0.2726 - val_mae: 0.1316 - val_mse: 0.0400 - learning_rate: 5.0000e-05\n",
            "Epoch 30/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 549ms/step - loss: 0.2585 - mae: 0.1020 - mse: 0.0227 - val_loss: 0.2541 - val_mae: 0.1008 - val_mse: 0.0241 - learning_rate: 5.0000e-05\n",
            "Epoch 31/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.2514 - mae: 0.1011 - mse: 0.0208 - val_loss: 0.2530 - val_mae: 0.1140 - val_mse: 0.0301 - learning_rate: 5.0000e-05\n",
            "Epoch 32/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.2423 - mae: 0.0907 - mse: 0.0182 - val_loss: 0.2519 - val_mae: 0.1241 - val_mse: 0.0372 - learning_rate: 5.0000e-05\n",
            "Epoch 33/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.2404 - mae: 0.1032 - mse: 0.0217 - val_loss: 0.2405 - val_mae: 0.1111 - val_mse: 0.0290 - learning_rate: 5.0000e-05\n",
            "Epoch 34/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.2356 - mae: 0.1045 - mse: 0.0233 - val_loss: 0.2383 - val_mae: 0.1199 - val_mse: 0.0329 - learning_rate: 5.0000e-05\n",
            "Epoch 35/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.2263 - mae: 0.0938 - mse: 0.0188 - val_loss: 0.2286 - val_mae: 0.1083 - val_mse: 0.0278 - learning_rate: 5.0000e-05\n",
            "Epoch 36/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - loss: 0.2247 - mae: 0.1037 - mse: 0.0230 - val_loss: 0.2255 - val_mae: 0.1130 - val_mse: 0.0305 - learning_rate: 5.0000e-05\n",
            "Epoch 37/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.2162 - mae: 0.0949 - mse: 0.0189 - val_loss: 0.2215 - val_mae: 0.1154 - val_mse: 0.0315 - learning_rate: 5.0000e-05\n",
            "Epoch 38/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.2137 - mae: 0.1014 - mse: 0.0218 - val_loss: 0.2193 - val_mae: 0.1212 - val_mse: 0.0353 - learning_rate: 5.0000e-05\n",
            "Epoch 39/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.2058 - mae: 0.0914 - mse: 0.0186 - val_loss: 0.2098 - val_mae: 0.1101 - val_mse: 0.0285 - learning_rate: 5.0000e-05\n",
            "Epoch 40/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.2014 - mae: 0.0928 - mse: 0.0185 - val_loss: 0.2043 - val_mae: 0.1065 - val_mse: 0.0274 - learning_rate: 5.0000e-05\n",
            "Epoch 41/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - loss: 0.1989 - mae: 0.0971 - mse: 0.0212 - val_loss: 0.2028 - val_mae: 0.1152 - val_mse: 0.0305 - learning_rate: 5.0000e-05\n",
            "Epoch 42/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.1933 - mae: 0.0944 - mse: 0.0192 - val_loss: 0.1968 - val_mae: 0.1094 - val_mse: 0.0286 - learning_rate: 5.0000e-05\n",
            "Epoch 43/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1870 - mae: 0.0878 - mse: 0.0170 - val_loss: 0.2107 - val_mae: 0.1520 - val_mse: 0.0535 - learning_rate: 5.0000e-05\n",
            "Epoch 44/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1845 - mae: 0.0933 - mse: 0.0181 - val_loss: 0.2010 - val_mae: 0.1379 - val_mse: 0.0453 - learning_rate: 5.0000e-05\n",
            "Epoch 45/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1776 - mae: 0.0828 - mse: 0.0156 - val_loss: 0.1992 - val_mae: 0.1431 - val_mse: 0.0479 - learning_rate: 5.0000e-05\n",
            "Epoch 46/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.1752 - mae: 0.0876 - mse: 0.0164 - val_loss: 0.1827 - val_mae: 0.1121 - val_mse: 0.0305 - learning_rate: 5.0000e-05\n",
            "Epoch 47/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1711 - mae: 0.0855 - mse: 0.0164 - val_loss: 0.1782 - val_mae: 0.1096 - val_mse: 0.0287 - learning_rate: 5.0000e-05\n",
            "Epoch 48/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1678 - mae: 0.0861 - mse: 0.0165 - val_loss: 0.1930 - val_mae: 0.1526 - val_mse: 0.0534 - learning_rate: 5.0000e-05\n",
            "Epoch 49/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1656 - mae: 0.0896 - mse: 0.0174 - val_loss: 0.1741 - val_mae: 0.1166 - val_mse: 0.0317 - learning_rate: 5.0000e-05\n",
            "Epoch 50/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523ms/step - loss: 0.1613 - mae: 0.0857 - mse: 0.0163\n",
            "Epoch 50: ReduceLROnPlateau reducing learning rate to 4.499999886320438e-05.\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1613 - mae: 0.0858 - mse: 0.0163 - val_loss: 0.1714 - val_mae: 0.1167 - val_mse: 0.0327 - learning_rate: 5.0000e-05\n",
            "Epoch 51/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1596 - mae: 0.0903 - mse: 0.0175 - val_loss: 0.1700 - val_mae: 0.1198 - val_mse: 0.0344 - learning_rate: 4.5000e-05\n",
            "Epoch 52/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - loss: 0.1558 - mae: 0.0868 - mse: 0.0165 - val_loss: 0.1651 - val_mae: 0.1150 - val_mse: 0.0309 - learning_rate: 4.5000e-05\n",
            "Epoch 53/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.1572 - mae: 0.0975 - mse: 0.0210 - val_loss: 0.1697 - val_mae: 0.1308 - val_mse: 0.0415 - learning_rate: 4.5000e-05\n",
            "Epoch 54/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - loss: 0.1516 - mae: 0.0892 - mse: 0.0174 - val_loss: 0.1655 - val_mae: 0.1265 - val_mse: 0.0389 - learning_rate: 4.5000e-05\n",
            "Epoch 55/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1470 - mae: 0.0834 - mse: 0.0149 - val_loss: 0.1787 - val_mae: 0.1611 - val_mse: 0.0619 - learning_rate: 4.5000e-05\n",
            "Epoch 56/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - loss: 0.1484 - mae: 0.0937 - mse: 0.0191 - val_loss: 0.1601 - val_mae: 0.1250 - val_mse: 0.0377 - learning_rate: 4.5000e-05\n",
            "Epoch 57/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - loss: 0.1429 - mae: 0.0849 - mse: 0.0154 - val_loss: 0.1639 - val_mae: 0.1388 - val_mse: 0.0465 - learning_rate: 4.5000e-05\n",
            "Epoch 58/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - loss: 0.1405 - mae: 0.0842 - mse: 0.0153 - val_loss: 0.1558 - val_mae: 0.1260 - val_mse: 0.0371 - learning_rate: 4.5000e-05\n",
            "Epoch 59/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1381 - mae: 0.0832 - mse: 0.0149 - val_loss: 0.1540 - val_mae: 0.1254 - val_mse: 0.0382 - learning_rate: 4.5000e-05\n",
            "Epoch 60/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - loss: 0.1363 - mae: 0.0840 - mse: 0.0153 - val_loss: 0.1503 - val_mae: 0.1215 - val_mse: 0.0360 - learning_rate: 4.5000e-05\n",
            "Epoch 61/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - loss: 0.1333 - mae: 0.0806 - mse: 0.0145 - val_loss: 0.1517 - val_mae: 0.1302 - val_mse: 0.0404 - learning_rate: 4.5000e-05\n",
            "Epoch 62/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.1305 - mae: 0.0781 - mse: 0.0136 - val_loss: 0.1550 - val_mae: 0.1427 - val_mse: 0.0474 - learning_rate: 4.5000e-05\n",
            "Epoch 63/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - loss: 0.1316 - mae: 0.0877 - mse: 0.0161 - val_loss: 0.1593 - val_mae: 0.1553 - val_mse: 0.0570 - learning_rate: 4.5000e-05\n",
            "Epoch 64/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1297 - mae: 0.0878 - mse: 0.0159 - val_loss: 0.1443 - val_mae: 0.1262 - val_mse: 0.0371 - learning_rate: 4.5000e-05\n",
            "Epoch 65/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1263 - mae: 0.0810 - mse: 0.0151 - val_loss: 0.1583 - val_mae: 0.1610 - val_mse: 0.0603 - learning_rate: 4.5000e-05\n",
            "Epoch 66/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - loss: 0.1249 - mae: 0.0829 - mse: 0.0146 - val_loss: 0.1449 - val_mae: 0.1335 - val_mse: 0.0440 - learning_rate: 4.5000e-05\n",
            "Epoch 67/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - loss: 0.1223 - mae: 0.0792 - mse: 0.0141 - val_loss: 0.1368 - val_mae: 0.1185 - val_mse: 0.0346 - learning_rate: 4.5000e-05\n",
            "Epoch 68/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - loss: 0.1196 - mae: 0.0760 - mse: 0.0130 - val_loss: 0.1461 - val_mae: 0.1436 - val_mse: 0.0500 - learning_rate: 4.5000e-05\n",
            "Epoch 69/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1188 - mae: 0.0789 - mse: 0.0134 - val_loss: 0.1652 - val_mae: 0.1881 - val_mse: 0.0819 - learning_rate: 4.5000e-05\n",
            "Epoch 70/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523ms/step - loss: 0.1194 - mae: 0.0848 - mse: 0.0158\n",
            "Epoch 70: ReduceLROnPlateau reducing learning rate to 4.050000061397441e-05.\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - loss: 0.1194 - mae: 0.0847 - mse: 0.0157 - val_loss: 0.1377 - val_mae: 0.1306 - val_mse: 0.0423 - learning_rate: 4.5000e-05\n",
            "Epoch 71/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1179 - mae: 0.0843 - mse: 0.0157 - val_loss: 0.1307 - val_mae: 0.1184 - val_mse: 0.0338 - learning_rate: 4.0500e-05\n",
            "Epoch 72/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - loss: 0.1141 - mae: 0.0768 - mse: 0.0131 - val_loss: 0.1370 - val_mae: 0.1352 - val_mse: 0.0450 - learning_rate: 4.0500e-05\n",
            "Epoch 73/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.1129 - mae: 0.0765 - mse: 0.0134 - val_loss: 0.1308 - val_mae: 0.1237 - val_mse: 0.0377 - learning_rate: 4.0500e-05\n",
            "Epoch 74/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - loss: 0.1111 - mae: 0.0752 - mse: 0.0127 - val_loss: 0.1480 - val_mae: 0.1655 - val_mse: 0.0648 - learning_rate: 4.0500e-05\n",
            "Epoch 75/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1089 - mae: 0.0723 - mse: 0.0115 - val_loss: 0.1314 - val_mae: 0.1305 - val_mse: 0.0418 - learning_rate: 4.0500e-05\n",
            "Epoch 76/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1098 - mae: 0.0785 - mse: 0.0134 - val_loss: 0.1421 - val_mae: 0.1579 - val_mse: 0.0587 - learning_rate: 4.0500e-05\n",
            "Epoch 77/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1077 - mae: 0.0752 - mse: 0.0125 - val_loss: 0.1364 - val_mae: 0.1488 - val_mse: 0.0511 - learning_rate: 4.0500e-05\n",
            "Epoch 78/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.1071 - mae: 0.0768 - mse: 0.0130 - val_loss: 0.1243 - val_mae: 0.1233 - val_mse: 0.0353 - learning_rate: 4.0500e-05\n",
            "Epoch 79/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 547ms/step - loss: 0.1050 - mae: 0.0736 - mse: 0.0119 - val_loss: 0.1251 - val_mae: 0.1247 - val_mse: 0.0402 - learning_rate: 4.0500e-05\n",
            "Epoch 80/300\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 548ms/step - loss: 0.1036 - mae: 0.0724 - mse: 0.0117 - val_loss: 0.1230 - val_mae: 0.1231 - val_mse: 0.0375 - learning_rate: 4.0500e-05\n",
            "Epoch 80: early stopping\n",
            "Restoring model weights from the end of the best epoch: 30.\n",
            "\n",
            "================================================================================\n",
            "[EVALUATION WITH POST-PROCESSING]\n",
            "================================================================================\n",
            "\n",
            "RESULTS:\n",
            "  MAE:   324.63 (Target: 70-150)\n",
            "  RMSE:  454.22 (Target: 100-200)\n",
            "\n",
            "TARGET ACHIEVEMENT:\n",
            "  MAE  [70-150]:    FAIL\n",
            "  RMSE [100-200]:   FAIL\n",
            "\n",
            "First 25 predictions:\n",
            "   1: pred=   1352, true=    172, error=  1180\n",
            "   2: pred=    833, true=    502, error=   331\n",
            "   3: pred=    213, true=    389, error=   176\n",
            "   4: pred=    276, true=    211, error=    65\n",
            "   5: pred=    332, true=    223, error=   109\n",
            "   6: pred=     23, true=    431, error=   408\n",
            "   7: pred=    433, true=   1175, error=   742\n",
            "   8: pred=     76, true=    265, error=   189\n",
            "   9: pred=    973, true=   1232, error=   259\n",
            "  10: pred=     92, true=    289, error=   197\n",
            "  11: pred=    216, true=    182, error=    34\n",
            "  12: pred=   1352, true=    379, error=   973\n",
            "  13: pred=    752, true=   1068, error=   316\n",
            "  14: pred=    667, true=   1021, error=   354\n",
            "  15: pred=    130, true=    452, error=   322\n",
            "  16: pred=   1238, true=    256, error=   982\n",
            "  17: pred=    221, true=     66, error=   155\n",
            "  18: pred=    113, true=    141, error=    28\n",
            "  19: pred=    418, true=   1191, error=   773\n",
            "  20: pred=    648, true=    288, error=   360\n",
            "  21: pred=    614, true=   1603, error=   989\n",
            "  22: pred=    470, true=    241, error=   229\n",
            "  23: pred=    148, true=    250, error=   102\n",
            "  24: pred=      0, true=    321, error=   321\n",
            "  25: pred=   1352, true=    133, error=  1219\n",
            "\n",
            "✓ Results saved to: results/csrnet_ultimate_ensemble\n",
            "================================================================================\n",
            "\n",
            "Results: MAE 324.63, RMSE 454.22\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_code = '''#!/usr/bin/env python3\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import sys, pickle, numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"CSRNet - OPTIMIZED ADVANCED\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(f\"\\\\nGPU: {len(gpus) > 0}\")\n",
        "\n",
        "CONFIG = {\n",
        "    'batch_size': 16,\n",
        "    'epochs': 150,\n",
        "    'learning_rate': 8e-5,\n",
        "    'weight_decay': 5e-5,\n",
        "    'data_path': 'processed_dataset_fixed/part_A_fixed.pkl',\n",
        "    'results_path': 'results/csrnet_optimized_advanced',\n",
        "}\n",
        "\n",
        "# LOAD\n",
        "print(\"\\\\n[LOADING DATA]\")\n",
        "with open(CONFIG['data_path'], 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "X_train = data['X_train'].astype('float32')\n",
        "y_density_train = data['y_density_train'][..., np.newaxis].astype('float32')\n",
        "X_test = data['X_test'].astype('float32')\n",
        "y_count_test = data['y_count_test']\n",
        "\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_density_train, test_size=0.15, random_state=42)\n",
        "\n",
        "print(f\"✓ Train: {X_tr.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "# MODEL\n",
        "print(\"\\\\n[BUILDING MODEL]\")\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(256, 256, 3)),\n",
        "\n",
        "    # Block 1\n",
        "    layers.Conv2D(64, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(64, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D(2),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    # Block 2\n",
        "    layers.Conv2D(128, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(128, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D(2),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    # Block 3\n",
        "    layers.Conv2D(256, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(256, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(256, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    # layers.MaxPooling2D(2), # REMOVED THIS LAYER TO MATCH 64x64 GROUND TRUTH\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    # Block 4\n",
        "    layers.Conv2D(512, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(512, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(512, 3, padding='same', kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "\n",
        "    # Backend\n",
        "    layers.Conv2D(512, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Dropout(0.15),\n",
        "    layers.Conv2D(512, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Dropout(0.15),\n",
        "    layers.Conv2D(512, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(256, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(128, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(64, 3, padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(CONFIG['weight_decay']), activation=None),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "\n",
        "    layers.Conv2D(1, 1, padding='same', activation='relu'),\n",
        "], name='CSRNet-Optimized-Advanced')\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    mse = tf.keras.losses.mse(y_true, y_pred)\n",
        "    mae = tf.keras.losses.mae(y_true, y_pred)\n",
        "    return 0.7 * mse + 0.3 * mae\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=CONFIG['learning_rate'])\n",
        "model.compile(optimizer=optimizer, loss=combined_loss, metrics=['mae', 'mse'])\n",
        "print(f\"✓ Parameters: {model.count_params():,}\")\n",
        "\n",
        "# PHASE 1\n",
        "print(\"\\\\n[PHASE 1: INITIAL TRAINING]\\\\n\")\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_mae', patience=25, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_mae', factor=0.6, patience=12, min_lr=1e-6, verbose=1),\n",
        "]\n",
        "\n",
        "history1 = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    epochs=75,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# PHASE 2\n",
        "print(\"\\\\n[PHASE 2: FINE-TUNING]\\\\n\")\n",
        "\n",
        "optimizer2 = optimizers.Adam(learning_rate=CONFIG['learning_rate'] / 3)\n",
        "model.compile(optimizer=optimizer2, loss=combined_loss, metrics=['mae', 'mse'])\n",
        "\n",
        "callbacks2 = [\n",
        "    EarlyStopping(monitor='val_mae', patience=30, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_mae', factor=0.7, patience=15, min_lr=1e-7, verbose=1),\n",
        "]\n",
        "\n",
        "history2 = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    epochs=75,\n",
        "    callbacks=callbacks2,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# EVALUATE\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"[EVALUATION ON TEST SET]\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "predictions = []\n",
        "for i in range(0, len(X_test), 50):\n",
        "    batch = X_test[i:i+50]\n",
        "    pred_batch = model.predict(batch, verbose=0)\n",
        "    predictions.extend([max(0, p.sum()) for p in pred_batch])\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "\n",
        "mae = np.mean(np.abs(predictions - y_count_test))\n",
        "rmse = np.sqrt(np.mean((predictions - y_count_test) ** 2))\n",
        "\n",
        "print(f\"\\\\nRESULTS:\")\n",
        "print(f\"  MAE:   {mae:.2f} (Target: 70-150)\")\n",
        "print(f\"  RMSE:  {rmse:.2f} (Target: 100-200)\")\n",
        "\n",
        "mae_pass = 70 <= mae <= 150\n",
        "rmse_pass = 100 <= rmse <= 200\n",
        "\n",
        "print(f\"\\\\nTARGET ACHIEVEMENT:\")\n",
        "print(f\"  MAE  [70-150]:    {'PASS' if mae_pass else 'FAIL'}\")\n",
        "print(f\"  RMSE [100-200]:   {'PASS' if rmse_pass else 'FAIL'}\")\n",
        "\n",
        "# SAVE\n",
        "results_dir = Path(CONFIG['results_path'])\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(results_dir / 'results.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'mae': float(mae),\n",
        "        'rmse': float(rmse),\n",
        "        'predictions': predictions.tolist(),\n",
        "        'ground_truth': y_count_test.tolist(),\n",
        "        'model_params': model.count_params(),\n",
        "        'mae_pass': bool(mae_pass),\n",
        "        'rmse_pass': bool(rmse_pass),\n",
        "    }, f)\n",
        "\n",
        "model.save(results_dir / 'model.keras')\n",
        "\n",
        "print(f\"\\\\n✓ Results saved to: {results_dir}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if mae_pass and rmse_pass:\n",
        "    print(\"\\\\n✓✓✓ SUCCESS! TARGETS ACHIEVED! ✓✓✓\\\\n\")\n",
        "else:\n",
        "    print(f\"\\\\nResults: MAE {mae:.2f}, RMSE {rmse:.2f}\\\\n\")\n",
        "'''\n",
        "\n",
        "with open('train_gpu_advanced_v2.py', 'w') as f:\n",
        "    f.write(training_code)\n",
        "\n",
        "print(\"✓ Optimized Advanced training script created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v76r7NEKbDv6",
        "outputId": "04e05bc7-d161-4813-dcc8-0ba5a7654683"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Optimized Advanced training script created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_gpu_advanced_v2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbPW2YUmje7h",
        "outputId": "5e9c1adf-dd30-4039-fde8-89bede8baf64"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 16:41:38.272004: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765039298.291575   63569 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765039298.297644   63569 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765039298.312443   63569 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765039298.312472   63569 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765039298.312476   63569 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765039298.312481   63569 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\n",
            "================================================================================\n",
            "CSRNet - OPTIMIZED ADVANCED\n",
            "================================================================================\n",
            "\n",
            "GPU: True\n",
            "\n",
            "[LOADING DATA]\n",
            "✓ Train: (255, 256, 256, 3), Val: (45, 256, 256, 3), Test: (182, 256, 256, 3)\n",
            "\n",
            "[BUILDING MODEL]\n",
            "I0000 00:00:1765039303.747508   63569 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "✓ Parameters: 16,282,177\n",
            "\n",
            "[PHASE 1: INITIAL TRAINING]\n",
            "\n",
            "Epoch 1/75\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1765039315.007623   63615 service.cc:152] XLA service 0x7ffa10015a10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1765039315.007677   63615 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "I0000 00:00:1765039317.043911   63615 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
            "2025-12-06 16:42:23.730879: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng4{k11=2} for conv %cudnn-conv-bw-input.18 = (f32[64,512,34,34]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,512,32,32]{3,2,1,0} %bitcast.28820, f32[512,512,3,3]{3,2,1,0} %bitcast.27905), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Optimized-Advanced_1/conv2d_12_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 16:42:23.972626: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.243902788s\n",
            "Trying algorithm eng4{k11=2} for conv %cudnn-conv-bw-input.18 = (f32[64,512,34,34]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,512,32,32]{3,2,1,0} %bitcast.28820, f32[512,512,3,3]{3,2,1,0} %bitcast.27905), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Optimized-Advanced_1/conv2d_12_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 16:42:33.919412: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:42:34.222293: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:42:35.619406: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:42:35.848307: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:42:38.139687: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:42:38.363688: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng4{k11=1} for conv %cudnn-conv-bw-input.27 = (f32[16,128,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,128,128,128]{3,2,1,0} %bitcast.31470, f32[128,128,3,3]{3,2,1,0} %bitcast.25556), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Optimized-Advanced_1/conv2d_3_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 16:42:38.592249: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:42:38.609741: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.246135202s\n",
            "Trying algorithm eng4{k11=1} for conv %cudnn-conv-bw-input.27 = (f32[16,128,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,128,128,128]{3,2,1,0} %bitcast.31470, f32[128,128,3,3]{3,2,1,0} %bitcast.25556), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Optimized-Advanced_1/conv2d_3_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 16:42:40.016964: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:42:40.356343: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "I0000 00:00:1765039389.701854   63615 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[1m15/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 960ms/step - loss: 0.4884 - mae: 0.2989 - mse: 0.25072025-12-06 16:43:31.804834: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:43:32.034614: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:43:33.803521: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:43:34.113894: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:03.653337: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:03.964116: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:04.687453: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:04.998519: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:06.340576: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:06.575593: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:06.961890: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:07.197714: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:09.587787: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:09.792960: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng4{k11=1} for conv %cudnn-conv-bw-input.27 = (f32[15,128,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[15,128,128,128]{3,2,1,0} %bitcast.31470, f32[128,128,3,3]{3,2,1,0} %bitcast.25556), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Optimized-Advanced_1/conv2d_3_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 16:44:09.981191: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:10.381610: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:10.397857: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.605002239s\n",
            "Trying algorithm eng4{k11=1} for conv %cudnn-conv-bw-input.27 = (f32[15,128,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[15,128,128,128]{3,2,1,0} %bitcast.31470, f32[128,128,3,3]{3,2,1,0} %bitcast.25556), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Optimized-Advanced_1/conv2d_3_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 16:44:11.885217: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:12.252645: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:12.329129: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng4{k11=1} for conv %cudnn-conv-bw-input.28 = (f32[15,64,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[15,128,128,128]{3,2,1,0} %bitcast.31228, f32[128,64,3,3]{3,2,1,0} %bitcast.25482), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Optimized-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 16:44:12.603865: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:12.940142: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:12.948935: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.61990823s\n",
            "Trying algorithm eng4{k11=1} for conv %cudnn-conv-bw-input.28 = (f32[15,64,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[15,128,128,128]{3,2,1,0} %bitcast.31228, f32[128,64,3,3]{3,2,1,0} %bitcast.25482), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Optimized-Advanced_1/conv2d_2_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - loss: 0.4837 - mae: 0.2951 - mse: 0.2456   2025-12-06 16:44:49.448269: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:49.673975: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:51.264355: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 16:44:51.571073: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 8s/step - loss: 0.4796 - mae: 0.2918 - mse: 0.2412 - val_loss: 0.3094 - val_mae: 0.1173 - val_mse: 0.0729 - learning_rate: 8.0000e-05\n",
            "Epoch 2/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - loss: 0.2957 - mae: 0.1220 - mse: 0.0513 - val_loss: 0.2964 - val_mae: 0.1046 - val_mse: 0.0600 - learning_rate: 8.0000e-05\n",
            "Epoch 3/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2771 - mae: 0.0993 - mse: 0.0347 - val_loss: 0.2973 - val_mae: 0.1127 - val_mse: 0.0582 - learning_rate: 8.0000e-05\n",
            "Epoch 4/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2808 - mae: 0.0988 - mse: 0.0406 - val_loss: 0.2970 - val_mae: 0.1079 - val_mse: 0.0603 - learning_rate: 8.0000e-05\n",
            "Epoch 5/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2722 - mae: 0.0914 - mse: 0.0320 - val_loss: 0.2987 - val_mae: 0.1021 - val_mse: 0.0657 - learning_rate: 8.0000e-05\n",
            "Epoch 6/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2633 - mae: 0.0800 - mse: 0.0247 - val_loss: 0.2968 - val_mae: 0.1052 - val_mse: 0.0622 - learning_rate: 8.0000e-05\n",
            "Epoch 7/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2666 - mae: 0.0819 - mse: 0.0291 - val_loss: 0.3064 - val_mae: 0.1430 - val_mse: 0.0603 - learning_rate: 8.0000e-05\n",
            "Epoch 8/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2616 - mae: 0.0795 - mse: 0.0237 - val_loss: 0.3208 - val_mae: 0.1745 - val_mse: 0.0680 - learning_rate: 8.0000e-05\n",
            "Epoch 9/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2600 - mae: 0.0761 - mse: 0.0235 - val_loss: 0.3131 - val_mae: 0.1343 - val_mse: 0.0749 - learning_rate: 8.0000e-05\n",
            "Epoch 10/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2571 - mae: 0.0747 - mse: 0.0206 - val_loss: 0.2987 - val_mae: 0.1079 - val_mse: 0.0664 - learning_rate: 8.0000e-05\n",
            "Epoch 11/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2549 - mae: 0.0684 - mse: 0.0210 - val_loss: 0.2995 - val_mae: 0.1086 - val_mse: 0.0679 - learning_rate: 8.0000e-05\n",
            "Epoch 12/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2607 - mae: 0.0787 - mse: 0.0255 - val_loss: 0.2971 - val_mae: 0.1074 - val_mse: 0.0658 - learning_rate: 8.0000e-05\n",
            "Epoch 13/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2554 - mae: 0.0716 - mse: 0.0218 - val_loss: 0.2888 - val_mae: 0.1084 - val_mse: 0.0544 - learning_rate: 8.0000e-05\n",
            "Epoch 14/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2596 - mae: 0.0820 - mse: 0.0242 - val_loss: 0.2902 - val_mae: 0.1056 - val_mse: 0.0584 - learning_rate: 8.0000e-05\n",
            "Epoch 15/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2539 - mae: 0.0731 - mse: 0.0206 - val_loss: 0.3115 - val_mae: 0.1505 - val_mse: 0.0705 - learning_rate: 8.0000e-05\n",
            "Epoch 16/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2510 - mae: 0.0701 - mse: 0.0186 - val_loss: 0.3707 - val_mae: 0.2195 - val_mse: 0.1263 - learning_rate: 8.0000e-05\n",
            "Epoch 17/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986ms/step - loss: 0.2495 - mae: 0.0685 - mse: 0.0180\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 4.7999998787418005e-05.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2498 - mae: 0.0690 - mse: 0.0183 - val_loss: 0.3845 - val_mae: 0.2194 - val_mse: 0.1470 - learning_rate: 8.0000e-05\n",
            "Epoch 18/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2508 - mae: 0.0726 - mse: 0.0190 - val_loss: 0.3161 - val_mae: 0.1511 - val_mse: 0.0791 - learning_rate: 4.8000e-05\n",
            "Epoch 19/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2502 - mae: 0.0683 - mse: 0.0206 - val_loss: 0.2854 - val_mae: 0.1194 - val_mse: 0.0494 - learning_rate: 4.8000e-05\n",
            "Epoch 20/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2475 - mae: 0.0678 - mse: 0.0176 - val_loss: 0.3048 - val_mae: 0.1480 - val_mse: 0.0655 - learning_rate: 4.8000e-05\n",
            "Epoch 21/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2412 - mae: 0.0566 - mse: 0.0139 - val_loss: 0.3886 - val_mae: 0.2247 - val_mse: 0.1528 - learning_rate: 4.8000e-05\n",
            "Epoch 22/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2452 - mae: 0.0656 - mse: 0.0164 - val_loss: 0.2904 - val_mae: 0.1263 - val_mse: 0.0555 - learning_rate: 4.8000e-05\n",
            "Epoch 23/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2407 - mae: 0.0583 - mse: 0.0136 - val_loss: 0.3217 - val_mae: 0.1662 - val_mse: 0.0835 - learning_rate: 4.8000e-05\n",
            "Epoch 24/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2442 - mae: 0.0648 - mse: 0.0165 - val_loss: 0.3621 - val_mae: 0.1988 - val_mse: 0.1279 - learning_rate: 4.8000e-05\n",
            "Epoch 25/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2419 - mae: 0.0612 - mse: 0.0154 - val_loss: 0.3163 - val_mae: 0.1522 - val_mse: 0.0831 - learning_rate: 4.8000e-05\n",
            "Epoch 26/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2383 - mae: 0.0566 - mse: 0.0129 - val_loss: 0.3182 - val_mae: 0.1548 - val_mse: 0.0854 - learning_rate: 4.8000e-05\n",
            "Epoch 27/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2444 - mae: 0.0670 - mse: 0.0178 - val_loss: 0.3320 - val_mae: 0.1784 - val_mse: 0.0958 - learning_rate: 4.8000e-05\n",
            "Epoch 28/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2386 - mae: 0.0572 - mse: 0.0145 - val_loss: 0.4244 - val_mae: 0.2682 - val_mse: 0.1900 - learning_rate: 4.8000e-05\n",
            "Epoch 29/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987ms/step - loss: 0.2390 - mae: 0.0603 - mse: 0.0143\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 2.8799998835893346e-05.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2391 - mae: 0.0604 - mse: 0.0144 - val_loss: 0.4601 - val_mae: 0.3051 - val_mse: 0.2259 - learning_rate: 4.8000e-05\n",
            "Epoch 30/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2402 - mae: 0.0634 - mse: 0.0154 - val_loss: 0.3700 - val_mae: 0.2207 - val_mse: 0.1337 - learning_rate: 2.8800e-05\n",
            "Epoch 30: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "\n",
            "[PHASE 2: FINE-TUNING]\n",
            "\n",
            "Epoch 1/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.2668 - mae: 0.0874 - mse: 0.0266 - val_loss: 0.3053 - val_mae: 0.1133 - val_mse: 0.0712 - learning_rate: 2.6667e-05\n",
            "Epoch 2/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - loss: 0.2614 - mae: 0.0792 - mse: 0.0234 - val_loss: 0.3078 - val_mae: 0.1192 - val_mse: 0.0734 - learning_rate: 2.6667e-05\n",
            "Epoch 3/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - loss: 0.2596 - mae: 0.0754 - mse: 0.0237 - val_loss: 0.3068 - val_mae: 0.1187 - val_mse: 0.0733 - learning_rate: 2.6667e-05\n",
            "Epoch 4/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2555 - mae: 0.0742 - mse: 0.0194 - val_loss: 0.3045 - val_mae: 0.1165 - val_mse: 0.0721 - learning_rate: 2.6667e-05\n",
            "Epoch 5/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2582 - mae: 0.0749 - mse: 0.0241 - val_loss: 0.2910 - val_mae: 0.0994 - val_mse: 0.0614 - learning_rate: 2.6667e-05\n",
            "Epoch 6/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2517 - mae: 0.0690 - mse: 0.0185 - val_loss: 0.3030 - val_mae: 0.1161 - val_mse: 0.0726 - learning_rate: 2.6667e-05\n",
            "Epoch 7/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2545 - mae: 0.0727 - mse: 0.0222 - val_loss: 0.2913 - val_mae: 0.1024 - val_mse: 0.0629 - learning_rate: 2.6667e-05\n",
            "Epoch 8/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2510 - mae: 0.0698 - mse: 0.0196 - val_loss: 0.2940 - val_mae: 0.1079 - val_mse: 0.0657 - learning_rate: 2.6667e-05\n",
            "Epoch 9/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2492 - mae: 0.0672 - mse: 0.0193 - val_loss: 0.2928 - val_mae: 0.1086 - val_mse: 0.0648 - learning_rate: 2.6667e-05\n",
            "Epoch 10/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2481 - mae: 0.0701 - mse: 0.0178 - val_loss: 0.2850 - val_mae: 0.1023 - val_mse: 0.0577 - learning_rate: 2.6667e-05\n",
            "Epoch 11/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2445 - mae: 0.0653 - mse: 0.0159 - val_loss: 0.2778 - val_mae: 0.0948 - val_mse: 0.0517 - learning_rate: 2.6667e-05\n",
            "Epoch 12/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2460 - mae: 0.0683 - mse: 0.0180 - val_loss: 0.2830 - val_mae: 0.1043 - val_mse: 0.0563 - learning_rate: 2.6667e-05\n",
            "Epoch 13/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2494 - mae: 0.0752 - mse: 0.0211 - val_loss: 0.2930 - val_mae: 0.1219 - val_mse: 0.0643 - learning_rate: 2.6667e-05\n",
            "Epoch 14/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2419 - mae: 0.0645 - mse: 0.0162 - val_loss: 0.3259 - val_mae: 0.1645 - val_mse: 0.0941 - learning_rate: 2.6667e-05\n",
            "Epoch 15/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2414 - mae: 0.0655 - mse: 0.0162 - val_loss: 0.3019 - val_mae: 0.1391 - val_mse: 0.0720 - learning_rate: 2.6667e-05\n",
            "Epoch 16/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2424 - mae: 0.0677 - mse: 0.0179 - val_loss: 0.3087 - val_mae: 0.1476 - val_mse: 0.0793 - learning_rate: 2.6667e-05\n",
            "Epoch 17/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2409 - mae: 0.0661 - mse: 0.0177 - val_loss: 0.3834 - val_mae: 0.2261 - val_mse: 0.1535 - learning_rate: 2.6667e-05\n",
            "Epoch 18/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2368 - mae: 0.0627 - mse: 0.0143 - val_loss: 0.3966 - val_mae: 0.2259 - val_mse: 0.1736 - learning_rate: 2.6667e-05\n",
            "Epoch 19/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2353 - mae: 0.0616 - mse: 0.0138 - val_loss: 0.3678 - val_mae: 0.2084 - val_mse: 0.1410 - learning_rate: 2.6667e-05\n",
            "Epoch 20/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2353 - mae: 0.0608 - mse: 0.0153 - val_loss: 0.3832 - val_mae: 0.2253 - val_mse: 0.1570 - learning_rate: 2.6667e-05\n",
            "Epoch 21/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2311 - mae: 0.0566 - mse: 0.0124 - val_loss: 0.3752 - val_mae: 0.2194 - val_mse: 0.1493 - learning_rate: 2.6667e-05\n",
            "Epoch 22/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2333 - mae: 0.0623 - mse: 0.0142 - val_loss: 0.4098 - val_mae: 0.2520 - val_mse: 0.1859 - learning_rate: 2.6667e-05\n",
            "Epoch 23/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2305 - mae: 0.0568 - mse: 0.0137 - val_loss: 0.4833 - val_mae: 0.3134 - val_mse: 0.2658 - learning_rate: 2.6667e-05\n",
            "Epoch 24/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2313 - mae: 0.0599 - mse: 0.0147 - val_loss: 0.3890 - val_mae: 0.2368 - val_mse: 0.1650 - learning_rate: 2.6667e-05\n",
            "Epoch 25/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2270 - mae: 0.0553 - mse: 0.0116 - val_loss: 0.4447 - val_mae: 0.2891 - val_mse: 0.2232 - learning_rate: 2.6667e-05\n",
            "Epoch 26/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988ms/step - loss: 0.2284 - mae: 0.0590 - mse: 0.0131\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 1.8666666619537864e-05.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2284 - mae: 0.0591 - mse: 0.0132 - val_loss: 0.3701 - val_mae: 0.2222 - val_mse: 0.1465 - learning_rate: 2.6667e-05\n",
            "Epoch 27/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - loss: 0.2275 - mae: 0.0568 - mse: 0.0140 - val_loss: 0.3649 - val_mae: 0.2164 - val_mse: 0.1424 - learning_rate: 1.8667e-05\n",
            "Epoch 28/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2237 - mae: 0.0526 - mse: 0.0111 - val_loss: 0.4216 - val_mae: 0.2617 - val_mse: 0.2048 - learning_rate: 1.8667e-05\n",
            "Epoch 29/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2233 - mae: 0.0533 - mse: 0.0110 - val_loss: 0.3924 - val_mae: 0.2345 - val_mse: 0.1755 - learning_rate: 1.8667e-05\n",
            "Epoch 30/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2254 - mae: 0.0573 - mse: 0.0131 - val_loss: 0.3226 - val_mae: 0.1704 - val_mse: 0.1041 - learning_rate: 1.8667e-05\n",
            "Epoch 31/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2272 - mae: 0.0603 - mse: 0.0151 - val_loss: 0.3397 - val_mae: 0.1932 - val_mse: 0.1195 - learning_rate: 1.8667e-05\n",
            "Epoch 32/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2229 - mae: 0.0568 - mse: 0.0113 - val_loss: 0.4884 - val_mae: 0.3204 - val_mse: 0.2782 - learning_rate: 1.8667e-05\n",
            "Epoch 33/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2220 - mae: 0.0548 - mse: 0.0117 - val_loss: 0.4704 - val_mae: 0.3032 - val_mse: 0.2607 - learning_rate: 1.8667e-05\n",
            "Epoch 34/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2215 - mae: 0.0550 - mse: 0.0117 - val_loss: 0.4086 - val_mae: 0.2555 - val_mse: 0.1935 - learning_rate: 1.8667e-05\n",
            "Epoch 35/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2223 - mae: 0.0588 - mse: 0.0119 - val_loss: 0.3999 - val_mae: 0.2492 - val_mse: 0.1846 - learning_rate: 1.8667e-05\n",
            "Epoch 36/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2209 - mae: 0.0559 - mse: 0.0119 - val_loss: 0.4687 - val_mae: 0.3031 - val_mse: 0.2606 - learning_rate: 1.8667e-05\n",
            "Epoch 37/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2202 - mae: 0.0555 - mse: 0.0118 - val_loss: 0.4378 - val_mae: 0.2801 - val_mse: 0.2270 - learning_rate: 1.8667e-05\n",
            "Epoch 38/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2172 - mae: 0.0514 - mse: 0.0101 - val_loss: 0.3986 - val_mae: 0.2426 - val_mse: 0.1878 - learning_rate: 1.8667e-05\n",
            "Epoch 39/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2179 - mae: 0.0531 - mse: 0.0111 - val_loss: 0.3504 - val_mae: 0.2089 - val_mse: 0.1342 - learning_rate: 1.8667e-05\n",
            "Epoch 40/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2195 - mae: 0.0568 - mse: 0.0126 - val_loss: 0.3491 - val_mae: 0.2013 - val_mse: 0.1364 - learning_rate: 1.8667e-05\n",
            "Epoch 41/75\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986ms/step - loss: 0.2168 - mae: 0.0519 - mse: 0.0116\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 1.3066665997030213e-05.\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - loss: 0.2169 - mae: 0.0520 - mse: 0.0116 - val_loss: 0.3450 - val_mae: 0.2028 - val_mse: 0.1305 - learning_rate: 1.8667e-05\n",
            "Epoch 41: early stopping\n",
            "Restoring model weights from the end of the best epoch: 11.\n",
            "\n",
            "================================================================================\n",
            "[EVALUATION ON TEST SET]\n",
            "================================================================================\n",
            "2025-12-06 17:06:12.016181: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.61 = (f32[128,512,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,34,34]{3,2,1,0} %bitcast.1995, f32[512,512,3,3]{3,2,1,0} %bitcast.2002, f32[512]{0} %bitcast.2004), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet-Optimized-Advanced_1/conv2d_10_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 17:06:12.969232: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.953149814s\n",
            "Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.61 = (f32[128,512,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,34,34]{3,2,1,0} %bitcast.1995, f32[512,512,3,3]{3,2,1,0} %bitcast.2002, f32[512]{0} %bitcast.2004), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet-Optimized-Advanced_1/conv2d_10_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 17:06:19.204534: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.64 = (f32[128,256,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,34,34]{3,2,1,0} %bitcast.2154, f32[256,512,3,3]{3,2,1,0} %bitcast.2161, f32[256]{0} %bitcast.2163), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet-Optimized-Advanced_1/conv2d_13_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 17:06:19.841906: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.63742422s\n",
            "Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.64 = (f32[128,256,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,34,34]{3,2,1,0} %bitcast.2154, f32[256,512,3,3]{3,2,1,0} %bitcast.2161, f32[256]{0} %bitcast.2163), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet-Optimized-Advanced_1/conv2d_13_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 17:06:31.659142: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 17:06:31.891921: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 17:06:34.012048: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 17:06:34.326572: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 17:06:45.011788: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.61 = (f32[72,512,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[72,512,34,34]{3,2,1,0} %bitcast.1995, f32[512,512,3,3]{3,2,1,0} %bitcast.2002, f32[512]{0} %bitcast.2004), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet-Optimized-Advanced_1/conv2d_10_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 17:06:45.043008: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.031304114s\n",
            "Trying algorithm eng12{k11=2} for conv %cudnn-conv-bias-activation.61 = (f32[72,512,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[72,512,34,34]{3,2,1,0} %bitcast.1995, f32[512,512,3,3]{3,2,1,0} %bitcast.2002, f32[512]{0} %bitcast.2004), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"CSRNet-Optimized-Advanced_1/conv2d_10_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "\n",
            "RESULTS:\n",
            "  MAE:   265.43 (Target: 70-150)\n",
            "  RMSE:  342.17 (Target: 100-200)\n",
            "\n",
            "TARGET ACHIEVEMENT:\n",
            "  MAE  [70-150]:    FAIL\n",
            "  RMSE [100-200]:   FAIL\n",
            "\n",
            "✓ Results saved to: results/csrnet_optimized_advanced\n",
            "================================================================================\n",
            "\n",
            "Results: MAE 265.43, RMSE 342.17\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_code = '''#!/usr/bin/env python3\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import pickle, numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"CSRNet - DIRECT APPROACH\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(f\"GPU: {len(gpus) > 0}\\\\n\")\n",
        "\n",
        "CONFIG = {\n",
        "    'batch_size': 32,\n",
        "    'epochs': 200,\n",
        "    'learning_rate': 1e-4,\n",
        "    'data_path': 'processed_dataset_fixed/part_A_fixed.pkl',\n",
        "    'results_path': 'results/csrnet_direct',\n",
        "}\n",
        "\n",
        "print(\"[LOADING DATA]\")\n",
        "with open(CONFIG['data_path'], 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "X_train = data['X_train'].astype('float32') / 255.0\n",
        "y_density_train = data['y_density_train'].astype('float32')\n",
        "X_test = data['X_test'].astype('float32') / 255.0\n",
        "y_count_test = data['y_count_test'].astype('float32')\n",
        "\n",
        "y_count_train = y_density_train.reshape(len(y_density_train), -1).sum(axis=1)\n",
        "\n",
        "print(f\"Train: {X_train.shape}, counts: {y_count_train.min():.0f}-{y_count_train.max():.0f}\")\n",
        "\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_count_train, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Split: train={X_tr.shape[0]}, val={X_val.shape[0]}\\\\n\")\n",
        "\n",
        "print(\"[BUILDING MODEL]\")\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(256, 256, 3)),\n",
        "\n",
        "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(2),\n",
        "\n",
        "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(2),\n",
        "\n",
        "    layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(2),\n",
        "\n",
        "    layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(2),\n",
        "\n",
        "    layers.Conv2D(512, 3, padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(512, 3, padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(1, activation='relu'),\n",
        "], name='CSRNet-Direct')\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=CONFIG['learning_rate'])\n",
        "model.compile(optimizer=optimizer, loss='mae', metrics=['mae', 'mse'])\n",
        "\n",
        "print(f\"Parameters: {model.count_params():,}\\\\n\")\n",
        "\n",
        "print(\"[TRAINING]\\\\n\")\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_mae', patience=40, restore_best_weights=True, verbose=1),\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    epochs=CONFIG['epochs'],\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"[EVALUATION]\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "predictions = model.predict(X_test, verbose=0).flatten()\n",
        "predictions = np.maximum(predictions, 0)\n",
        "\n",
        "mae = np.mean(np.abs(predictions - y_count_test))\n",
        "rmse = np.sqrt(np.mean((predictions - y_count_test) ** 2))\n",
        "\n",
        "print(f\"\\\\nRESULTS:\")\n",
        "print(f\"  MAE:   {mae:.2f} (Target: 70-150)\")\n",
        "print(f\"  RMSE:  {rmse:.2f} (Target: 100-200)\")\n",
        "\n",
        "mae_pass = 70 <= mae <= 150\n",
        "rmse_pass = 100 <= rmse <= 200\n",
        "\n",
        "print(f\"\\\\nTARGET ACHIEVEMENT:\")\n",
        "print(f\"  MAE  [70-150]:    {'PASS' if mae_pass else 'FAIL'}\")\n",
        "print(f\"  RMSE [100-200]:   {'PASS' if rmse_pass else 'FAIL'}\")\n",
        "\n",
        "results_dir = Path(CONFIG['results_path'])\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(results_dir / 'results.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'mae': float(mae),\n",
        "        'rmse': float(rmse),\n",
        "        'predictions': predictions.tolist(),\n",
        "        'ground_truth': y_count_test.tolist(),\n",
        "        'model_params': model.count_params(),\n",
        "        'mae_pass': bool(mae_pass),\n",
        "        'rmse_pass': bool(rmse_pass),\n",
        "    }, f)\n",
        "\n",
        "model.save(results_dir / 'model.keras')\n",
        "\n",
        "print(f\"\\\\n✓ Saved to: {results_dir}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if mae_pass and rmse_pass:\n",
        "    print(\"\\\\n✓✓✓ SUCCESS! TARGETS ACHIEVED! ✓✓✓\\\\n\")\n",
        "else:\n",
        "    print(f\"\\\\nMAE {mae:.2f}, RMSE {rmse:.2f}\\\\n\")\n",
        "'''\n",
        "\n",
        "with open('train_gpu_direct.py', 'w') as f:\n",
        "    f.write(training_code)\n",
        "\n",
        "print(\"✓ Direct training script created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZf0X6uOjlWu",
        "outputId": "a599767f-8f0f-437f-ed58-1ef80c7db059"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Direct training script created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_gpu_direct.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmC47MTb0BmU",
        "outputId": "57641d03-ed32-426e-ba39-eb39b28f5e96"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 17:52:57.810582: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765043577.831343   82786 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765043577.837527   82786 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765043577.853833   82786 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765043577.853858   82786 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765043577.853864   82786 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765043577.853867   82786 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\n",
            "================================================================================\n",
            "CSRNet - DIRECT APPROACH\n",
            "================================================================================\n",
            "GPU: True\n",
            "\n",
            "[LOADING DATA]\n",
            "Train: (300, 256, 256, 3), counts: 33-3131\n",
            "Split: train=240, val=60\n",
            "\n",
            "[BUILDING MODEL]\n",
            "I0000 00:00:1765043584.045281   82786 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "Parameters: 4,884,513\n",
            "\n",
            "[TRAINING]\n",
            "\n",
            "Epoch 1/200\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1765043591.040855   82840 service.cc:152] XLA service 0x7cc9d4003f50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1765043591.040893   82840 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "I0000 00:00:1765043592.107058   82840 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
            "2025-12-06 17:53:20.694162: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 17:53:20.884956: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 17:53:21.759771: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 17:53:21.930167: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 17:53:23.612335: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 17:53:23.936791: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "I0000 00:00:1765043618.072498   82840 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - loss: 559.9925 - mae: 559.9925 - mse: 557096.31252025-12-06 17:53:45.969031: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 17:53:46.151536: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 17:53:46.675703: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 17:53:46.839756: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 17:53:47.825781: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 17:53:48.145214: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 17:53:48.354949: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng4{k11=1} for conv %cudnn-conv-bw-input.15 = (f32[16,64,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,64,128,128]{3,2,1,0} %bitcast.17085, f32[64,64,3,3]{3,2,1,0} %bitcast.14350), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Direct_1/conv2d_3_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "2025-12-06 17:53:48.445266: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-06 17:53:48.454533: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.099706156s\n",
            "Trying algorithm eng4{k11=1} for conv %cudnn-conv-bw-input.15 = (f32[16,64,128,128]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,64,128,128]{3,2,1,0} %bitcast.17085, f32[64,64,3,3]{3,2,1,0} %bitcast.14350), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/CSRNet-Direct_1/conv2d_3_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 5s/step - loss: 559.4820 - mae: 559.4820 - mse: 562609.3125 - val_loss: 471.0463 - val_mae: 471.0463 - val_mse: 399880.8438\n",
            "Epoch 2/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 227ms/step - loss: 535.6310 - mae: 535.6310 - mse: 514509.9062 - val_loss: 471.0062 - val_mae: 471.0062 - val_mse: 399843.0625\n",
            "Epoch 3/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 227ms/step - loss: 566.2687 - mae: 566.2687 - mse: 605448.8125 - val_loss: 470.7946 - val_mae: 470.7946 - val_mse: 399643.8125\n",
            "Epoch 4/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 227ms/step - loss: 499.0703 - mae: 499.0703 - mse: 468971.2188 - val_loss: 470.2738 - val_mae: 470.2738 - val_mse: 399153.5312\n",
            "Epoch 5/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 231ms/step - loss: 543.6727 - mae: 543.6727 - mse: 539986.6875 - val_loss: 469.7310 - val_mae: 469.7310 - val_mse: 398643.4375\n",
            "Epoch 6/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 549.7861 - mae: 549.7861 - mse: 571587.1875 - val_loss: 468.7076 - val_mae: 468.7076 - val_mse: 397683.0312\n",
            "Epoch 7/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 228ms/step - loss: 576.2017 - mae: 576.2017 - mse: 654824.5000 - val_loss: 466.9393 - val_mae: 466.9393 - val_mse: 396028.5312\n",
            "Epoch 8/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 228ms/step - loss: 535.6962 - mae: 535.6962 - mse: 569677.3750 - val_loss: 464.4525 - val_mae: 464.4525 - val_mse: 393712.4062\n",
            "Epoch 9/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 230ms/step - loss: 489.3546 - mae: 489.3546 - mse: 461391.8125 - val_loss: 462.0668 - val_mae: 462.0668 - val_mse: 391502.0000\n",
            "Epoch 10/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 229ms/step - loss: 581.1669 - mae: 581.1669 - mse: 658078.8750 - val_loss: 458.7484 - val_mae: 458.7484 - val_mse: 388446.3750\n",
            "Epoch 11/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 229ms/step - loss: 537.4791 - mae: 537.4791 - mse: 568634.8750 - val_loss: 455.1554 - val_mae: 455.1554 - val_mse: 385162.6875\n",
            "Epoch 12/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 502.3958 - mae: 502.3958 - mse: 513164.6562 - val_loss: 449.4058 - val_mae: 449.4058 - val_mse: 379961.8750\n",
            "Epoch 13/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 237ms/step - loss: 504.4326 - mae: 504.4326 - mse: 507592.2812 - val_loss: 439.6746 - val_mae: 439.6746 - val_mse: 371310.0625\n",
            "Epoch 14/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 466.3853 - mae: 466.3853 - mse: 449093.1562 - val_loss: 430.4223 - val_mae: 430.4223 - val_mse: 363181.8125\n",
            "Epoch 15/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 230ms/step - loss: 511.9270 - mae: 511.9270 - mse: 561499.7500 - val_loss: 421.0463 - val_mae: 421.0463 - val_mse: 354652.9375\n",
            "Epoch 16/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 231ms/step - loss: 496.0326 - mae: 496.0326 - mse: 514377.8125 - val_loss: 405.3374 - val_mae: 405.3374 - val_mse: 340285.4062\n",
            "Epoch 17/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 227ms/step - loss: 532.5723 - mae: 532.5723 - mse: 584375.8125 - val_loss: 407.5342 - val_mae: 407.5342 - val_mse: 342259.5000\n",
            "Epoch 18/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 489.0576 - mae: 489.0576 - mse: 536309.8125 - val_loss: 380.4618 - val_mae: 380.4618 - val_mse: 318780.0000\n",
            "Epoch 19/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 236ms/step - loss: 417.9797 - mae: 417.9797 - mse: 421884.6875 - val_loss: 366.2382 - val_mae: 366.2382 - val_mse: 306812.3750\n",
            "Epoch 20/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 244ms/step - loss: 396.5627 - mae: 396.5627 - mse: 364148.7500 - val_loss: 344.6462 - val_mae: 344.6462 - val_mse: 288447.9062\n",
            "Epoch 21/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 418.3155 - mae: 418.3155 - mse: 435985.2812 - val_loss: 336.9919 - val_mae: 336.9919 - val_mse: 281532.1562\n",
            "Epoch 22/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 392.1605 - mae: 392.1605 - mse: 433094.6562 - val_loss: 330.3333 - val_mae: 330.3333 - val_mse: 275103.0625\n",
            "Epoch 23/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 401.0721 - mae: 401.0721 - mse: 412150.6875 - val_loss: 309.0729 - val_mae: 309.0729 - val_mse: 255138.8438\n",
            "Epoch 24/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 387.3708 - mae: 387.3708 - mse: 401007.6250 - val_loss: 291.2898 - val_mae: 291.2898 - val_mse: 239520.9219\n",
            "Epoch 25/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 235ms/step - loss: 349.0338 - mae: 349.0338 - mse: 327500.8438 - val_loss: 286.2212 - val_mae: 286.2212 - val_mse: 235165.7656\n",
            "Epoch 26/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 244ms/step - loss: 358.9872 - mae: 358.9872 - mse: 347616.6875 - val_loss: 277.0035 - val_mae: 277.0035 - val_mse: 226509.5000\n",
            "Epoch 27/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 237ms/step - loss: 315.5143 - mae: 315.5143 - mse: 260975.9219 - val_loss: 448.1747 - val_mae: 448.1747 - val_mse: 378883.6562\n",
            "Epoch 28/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 340.1311 - mae: 340.1311 - mse: 297200.6875 - val_loss: 435.7439 - val_mae: 435.7439 - val_mse: 367778.0000\n",
            "Epoch 29/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 328.5052 - mae: 328.5052 - mse: 289019.8125 - val_loss: 415.3110 - val_mae: 415.3110 - val_mse: 349218.7188\n",
            "Epoch 30/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 300.1288 - mae: 300.1288 - mse: 265228.5000 - val_loss: 351.1285 - val_mae: 351.1285 - val_mse: 294087.0938\n",
            "Epoch 31/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 311.4442 - mae: 311.4442 - mse: 271528.8438 - val_loss: 350.5054 - val_mae: 350.5054 - val_mse: 293637.3750\n",
            "Epoch 32/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 235ms/step - loss: 298.8520 - mae: 298.8520 - mse: 261476.9375 - val_loss: 411.4991 - val_mae: 411.4991 - val_mse: 346776.1562\n",
            "Epoch 33/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 243ms/step - loss: 333.5743 - mae: 333.5743 - mse: 302784.5938 - val_loss: 433.6434 - val_mae: 433.6434 - val_mse: 366431.1250\n",
            "Epoch 34/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 238ms/step - loss: 282.0782 - mae: 282.0782 - mse: 221939.9688 - val_loss: 279.1690 - val_mae: 279.1690 - val_mse: 228820.4688\n",
            "Epoch 35/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 240ms/step - loss: 307.5108 - mae: 307.5108 - mse: 238137.1406 - val_loss: 262.8178 - val_mae: 262.8178 - val_mse: 204842.9688\n",
            "Epoch 36/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 237ms/step - loss: 252.4120 - mae: 252.4120 - mse: 167471.3750 - val_loss: 330.4555 - val_mae: 330.4555 - val_mse: 275304.0000\n",
            "Epoch 37/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 235ms/step - loss: 255.2776 - mae: 255.2776 - mse: 173039.3438 - val_loss: 270.6296 - val_mae: 270.6296 - val_mse: 220490.6406\n",
            "Epoch 38/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 236ms/step - loss: 283.0739 - mae: 283.0739 - mse: 211496.4688 - val_loss: 360.4170 - val_mae: 360.4170 - val_mse: 299016.4062\n",
            "Epoch 39/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 241ms/step - loss: 274.6487 - mae: 274.6487 - mse: 205108.3125 - val_loss: 288.1992 - val_mae: 288.1992 - val_mse: 236929.8594\n",
            "Epoch 40/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 266ms/step - loss: 268.3366 - mae: 268.3366 - mse: 172270.1562 - val_loss: 372.4266 - val_mae: 372.4266 - val_mse: 311907.1875\n",
            "Epoch 41/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 238ms/step - loss: 256.9175 - mae: 256.9175 - mse: 166652.2969 - val_loss: 382.8249 - val_mae: 382.8249 - val_mse: 321459.0625\n",
            "Epoch 42/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 236ms/step - loss: 268.9059 - mae: 268.9059 - mse: 166995.0625 - val_loss: 340.9934 - val_mae: 340.9934 - val_mse: 284836.9375\n",
            "Epoch 43/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 235ms/step - loss: 223.3589 - mae: 223.3589 - mse: 139047.3438 - val_loss: 388.0305 - val_mae: 388.0305 - val_mse: 325546.5000\n",
            "Epoch 44/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 235ms/step - loss: 247.4872 - mae: 247.4872 - mse: 131997.1875 - val_loss: 388.9376 - val_mae: 388.9376 - val_mse: 325236.2188\n",
            "Epoch 45/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 223.4586 - mae: 223.4586 - mse: 134934.5000 - val_loss: 359.2171 - val_mae: 359.2171 - val_mse: 301673.5312\n",
            "Epoch 46/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 239ms/step - loss: 204.7446 - mae: 204.7446 - mse: 99871.0234 - val_loss: 315.5964 - val_mae: 315.5964 - val_mse: 262006.5938\n",
            "Epoch 47/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 245ms/step - loss: 255.6869 - mae: 255.6869 - mse: 145841.9375 - val_loss: 367.5720 - val_mae: 367.5720 - val_mse: 308630.9375\n",
            "Epoch 48/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 195.3926 - mae: 195.3926 - mse: 95213.3281 - val_loss: 308.3530 - val_mae: 308.3530 - val_mse: 183330.0312\n",
            "Epoch 49/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 236ms/step - loss: 262.1118 - mae: 262.1118 - mse: 183394.0312 - val_loss: 286.3206 - val_mae: 286.3206 - val_mse: 232987.2500\n",
            "Epoch 50/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 220.7058 - mae: 220.7058 - mse: 110446.6016 - val_loss: 319.5687 - val_mae: 319.5687 - val_mse: 266048.3438\n",
            "Epoch 51/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 222.8039 - mae: 222.8039 - mse: 106513.5391 - val_loss: 348.7557 - val_mae: 348.7557 - val_mse: 291145.1562\n",
            "Epoch 52/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 227.9296 - mae: 227.9296 - mse: 125139.4062 - val_loss: 275.4195 - val_mae: 275.4195 - val_mse: 226636.7969\n",
            "Epoch 53/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 237ms/step - loss: 201.3992 - mae: 201.3992 - mse: 99484.9453 - val_loss: 265.4096 - val_mae: 265.4096 - val_mse: 215829.1719\n",
            "Epoch 54/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 236ms/step - loss: 228.1353 - mae: 228.1353 - mse: 110007.6250 - val_loss: 274.3092 - val_mae: 274.3092 - val_mse: 224616.3281\n",
            "Epoch 55/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 190.6894 - mae: 190.6894 - mse: 80541.8516 - val_loss: 311.9618 - val_mae: 311.9618 - val_mse: 257828.7031\n",
            "Epoch 56/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 194.8085 - mae: 194.8085 - mse: 90993.6328 - val_loss: 314.6651 - val_mae: 314.6651 - val_mse: 260347.7031\n",
            "Epoch 57/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 189.5855 - mae: 189.5855 - mse: 83194.4844 - val_loss: 299.7669 - val_mae: 299.7669 - val_mse: 246064.2031\n",
            "Epoch 58/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 236ms/step - loss: 267.1765 - mae: 267.1765 - mse: 168260.3438 - val_loss: 257.4169 - val_mae: 257.4169 - val_mse: 196472.2969\n",
            "Epoch 59/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 236ms/step - loss: 218.2378 - mae: 218.2378 - mse: 102424.4766 - val_loss: 255.1929 - val_mae: 255.1929 - val_mse: 194991.7969\n",
            "Epoch 60/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 237ms/step - loss: 193.6958 - mae: 193.6958 - mse: 90129.3203 - val_loss: 328.6486 - val_mae: 328.6486 - val_mse: 274026.0312\n",
            "Epoch 61/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 184.7071 - mae: 184.7071 - mse: 82386.7500 - val_loss: 345.1691 - val_mae: 345.1691 - val_mse: 286853.7188\n",
            "Epoch 62/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 231ms/step - loss: 158.4868 - mae: 158.4868 - mse: 61312.6953 - val_loss: 337.4902 - val_mae: 337.4902 - val_mse: 280497.6875\n",
            "Epoch 63/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 199.8854 - mae: 199.8854 - mse: 90543.1250 - val_loss: 430.8828 - val_mae: 430.8828 - val_mse: 360721.8438\n",
            "Epoch 64/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 172.9595 - mae: 172.9595 - mse: 79317.4922 - val_loss: 414.5092 - val_mae: 414.5092 - val_mse: 347020.4688\n",
            "Epoch 65/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 193.7659 - mae: 193.7659 - mse: 98417.2578 - val_loss: 430.4962 - val_mae: 430.4962 - val_mse: 363120.4062\n",
            "Epoch 66/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 162.3030 - mae: 162.3030 - mse: 64178.5273 - val_loss: 440.7290 - val_mae: 440.7290 - val_mse: 372351.5312\n",
            "Epoch 67/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 238ms/step - loss: 209.7652 - mae: 209.7652 - mse: 118266.4297 - val_loss: 372.6216 - val_mae: 372.6216 - val_mse: 313317.3438\n",
            "Epoch 68/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 183.6160 - mae: 183.6160 - mse: 77450.6562 - val_loss: 381.7570 - val_mae: 381.7570 - val_mse: 321013.8125\n",
            "Epoch 69/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 231ms/step - loss: 204.4931 - mae: 204.4931 - mse: 96764.2578 - val_loss: 421.9137 - val_mae: 421.9137 - val_mse: 355478.9375\n",
            "Epoch 70/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 180.1168 - mae: 180.1168 - mse: 82890.5312 - val_loss: 417.5341 - val_mae: 417.5341 - val_mse: 351010.3750\n",
            "Epoch 71/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 188.8474 - mae: 188.8474 - mse: 72482.5859 - val_loss: 353.0424 - val_mae: 353.0424 - val_mse: 287337.1250\n",
            "Epoch 72/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 191.5916 - mae: 191.5916 - mse: 83170.3750 - val_loss: 373.3671 - val_mae: 373.3671 - val_mse: 309826.5312\n",
            "Epoch 73/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 259ms/step - loss: 152.8111 - mae: 152.8111 - mse: 53164.4531 - val_loss: 406.3414 - val_mae: 406.3414 - val_mse: 339048.4062\n",
            "Epoch 74/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 238ms/step - loss: 186.5364 - mae: 186.5364 - mse: 89470.6172 - val_loss: 313.6200 - val_mae: 313.6200 - val_mse: 265341.6875\n",
            "Epoch 75/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 237.7142 - mae: 237.7142 - mse: 130262.4609 - val_loss: 366.1999 - val_mae: 366.1999 - val_mse: 303442.0000\n",
            "Epoch 76/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 234.8997 - mae: 234.8997 - mse: 107104.8906 - val_loss: 350.4195 - val_mae: 350.4195 - val_mse: 286883.6562\n",
            "Epoch 77/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 231ms/step - loss: 215.0148 - mae: 215.0148 - mse: 103263.5078 - val_loss: 307.6725 - val_mae: 307.6725 - val_mse: 249318.5625\n",
            "Epoch 78/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 250.6467 - mae: 250.6467 - mse: 150354.2812 - val_loss: 365.2656 - val_mae: 365.2656 - val_mse: 304293.7812\n",
            "Epoch 79/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 213.3405 - mae: 213.3405 - mse: 97096.3906 - val_loss: 376.2042 - val_mae: 376.2042 - val_mse: 316495.8750\n",
            "Epoch 80/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 260ms/step - loss: 197.0736 - mae: 197.0736 - mse: 100155.7734 - val_loss: 299.8015 - val_mae: 299.8015 - val_mse: 234294.8281\n",
            "Epoch 81/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 183.3409 - mae: 183.3409 - mse: 70270.1562 - val_loss: 369.8098 - val_mae: 369.8098 - val_mse: 299777.1875\n",
            "Epoch 82/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 189.2230 - mae: 189.2230 - mse: 84830.5938 - val_loss: 367.6690 - val_mae: 367.6690 - val_mse: 300733.0000\n",
            "Epoch 83/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 236ms/step - loss: 184.5849 - mae: 184.5849 - mse: 83197.0781 - val_loss: 225.2068 - val_mae: 225.2068 - val_mse: 169659.3438\n",
            "Epoch 84/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 236ms/step - loss: 147.4048 - mae: 147.4048 - mse: 46772.7461 - val_loss: 215.5377 - val_mae: 215.5377 - val_mse: 147634.8594\n",
            "Epoch 85/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 158.8652 - mae: 158.8652 - mse: 61696.3711 - val_loss: 247.3266 - val_mae: 247.3266 - val_mse: 184151.2969\n",
            "Epoch 86/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 145.4851 - mae: 145.4851 - mse: 56209.1562 - val_loss: 282.3633 - val_mae: 282.3633 - val_mse: 211817.7812\n",
            "Epoch 87/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 244ms/step - loss: 142.7323 - mae: 142.7323 - mse: 45672.2734 - val_loss: 202.6362 - val_mae: 202.6362 - val_mse: 141186.5938\n",
            "Epoch 88/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 236ms/step - loss: 155.3582 - mae: 155.3582 - mse: 53473.6445 - val_loss: 213.8724 - val_mae: 213.8724 - val_mse: 159227.5312\n",
            "Epoch 89/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 235ms/step - loss: 149.7482 - mae: 149.7482 - mse: 47876.5977 - val_loss: 259.5981 - val_mae: 259.5981 - val_mse: 197117.9688\n",
            "Epoch 90/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 238ms/step - loss: 182.2212 - mae: 182.2212 - mse: 91304.3281 - val_loss: 180.6575 - val_mae: 180.6575 - val_mse: 130776.6719\n",
            "Epoch 91/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 158.9579 - mae: 158.9579 - mse: 61458.0859 - val_loss: 218.8626 - val_mae: 218.8626 - val_mse: 166834.1562\n",
            "Epoch 92/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 154.7767 - mae: 154.7767 - mse: 58696.1211 - val_loss: 289.8026 - val_mae: 289.8026 - val_mse: 222595.1719\n",
            "Epoch 93/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 263ms/step - loss: 150.4252 - mae: 150.4252 - mse: 58816.9688 - val_loss: 230.3422 - val_mae: 230.3422 - val_mse: 183602.5625\n",
            "Epoch 94/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 265ms/step - loss: 126.1909 - mae: 126.1909 - mse: 37935.5156 - val_loss: 260.4575 - val_mae: 260.4575 - val_mse: 224388.7188\n",
            "Epoch 95/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 146.6958 - mae: 146.6958 - mse: 50179.8711 - val_loss: 279.2110 - val_mae: 279.2110 - val_mse: 213956.8594\n",
            "Epoch 96/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 159.1595 - mae: 159.1595 - mse: 66937.4062 - val_loss: 244.6574 - val_mae: 244.6574 - val_mse: 194224.8594\n",
            "Epoch 97/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 170.0634 - mae: 170.0634 - mse: 73548.5938 - val_loss: 251.6413 - val_mae: 251.6413 - val_mse: 211140.2500\n",
            "Epoch 98/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 136.5882 - mae: 136.5882 - mse: 48352.0938 - val_loss: 254.3680 - val_mae: 254.3680 - val_mse: 200882.1875\n",
            "Epoch 99/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 154.0063 - mae: 154.0063 - mse: 65772.7031 - val_loss: 237.4581 - val_mae: 237.4581 - val_mse: 186035.0312\n",
            "Epoch 100/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 241ms/step - loss: 139.6350 - mae: 139.6350 - mse: 41867.9727 - val_loss: 246.0614 - val_mae: 246.0614 - val_mse: 184934.1094\n",
            "Epoch 101/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 237ms/step - loss: 137.5349 - mae: 137.5349 - mse: 41326.0352 - val_loss: 249.7435 - val_mae: 249.7435 - val_mse: 216306.5625\n",
            "Epoch 102/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 124.6377 - mae: 124.6377 - mse: 37593.5820 - val_loss: 236.4057 - val_mae: 236.4057 - val_mse: 159678.8906\n",
            "Epoch 103/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 167.3359 - mae: 167.3359 - mse: 68401.7344 - val_loss: 256.6068 - val_mae: 256.6068 - val_mse: 192391.5000\n",
            "Epoch 104/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 140.0584 - mae: 140.0584 - mse: 53434.8086 - val_loss: 227.2518 - val_mae: 227.2518 - val_mse: 183576.4688\n",
            "Epoch 105/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 231ms/step - loss: 132.3273 - mae: 132.3273 - mse: 40058.9648 - val_loss: 271.7384 - val_mae: 271.7384 - val_mse: 245255.8594\n",
            "Epoch 106/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 141.1056 - mae: 141.1056 - mse: 50315.4922 - val_loss: 228.7775 - val_mae: 228.7775 - val_mse: 140531.1875\n",
            "Epoch 107/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 238ms/step - loss: 142.6586 - mae: 142.6586 - mse: 45054.8945 - val_loss: 221.3840 - val_mae: 221.3840 - val_mse: 155489.8281\n",
            "Epoch 108/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 125.1913 - mae: 125.1913 - mse: 40375.3906 - val_loss: 227.1616 - val_mae: 227.1616 - val_mse: 147370.8438\n",
            "Epoch 109/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 231ms/step - loss: 115.0817 - mae: 115.0817 - mse: 30553.0000 - val_loss: 232.5349 - val_mae: 232.5349 - val_mse: 151184.1562\n",
            "Epoch 110/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 235ms/step - loss: 118.1518 - mae: 118.1518 - mse: 34818.5000 - val_loss: 199.9939 - val_mae: 199.9939 - val_mse: 126783.3984\n",
            "Epoch 111/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 231ms/step - loss: 122.8915 - mae: 122.8915 - mse: 35784.3672 - val_loss: 224.0601 - val_mae: 224.0601 - val_mse: 161112.2031\n",
            "Epoch 112/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 231ms/step - loss: 130.6758 - mae: 130.6758 - mse: 38686.5820 - val_loss: 231.1884 - val_mae: 231.1884 - val_mse: 158943.1406\n",
            "Epoch 113/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 236ms/step - loss: 108.0167 - mae: 108.0167 - mse: 28579.8828 - val_loss: 280.0933 - val_mae: 280.0933 - val_mse: 253141.5625\n",
            "Epoch 114/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 260ms/step - loss: 138.6710 - mae: 138.6710 - mse: 39170.0898 - val_loss: 242.3072 - val_mae: 242.3072 - val_mse: 186288.5000\n",
            "Epoch 115/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 141.0786 - mae: 141.0786 - mse: 54414.9961 - val_loss: 238.1119 - val_mae: 238.1119 - val_mse: 186646.4844\n",
            "Epoch 116/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 231ms/step - loss: 123.4874 - mae: 123.4874 - mse: 35270.4688 - val_loss: 266.3157 - val_mae: 266.3157 - val_mse: 217367.9688\n",
            "Epoch 117/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 134.7675 - mae: 134.7675 - mse: 38957.0703 - val_loss: 240.1492 - val_mae: 240.1492 - val_mse: 175975.5938\n",
            "Epoch 118/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 113.9548 - mae: 113.9548 - mse: 30383.4883 - val_loss: 240.5992 - val_mae: 240.5992 - val_mse: 173350.2344\n",
            "Epoch 119/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 121.2256 - mae: 121.2256 - mse: 34537.0742 - val_loss: 247.9402 - val_mae: 247.9402 - val_mse: 184820.8594\n",
            "Epoch 120/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 259ms/step - loss: 112.2356 - mae: 112.2356 - mse: 30135.7734 - val_loss: 231.7548 - val_mae: 231.7548 - val_mse: 164044.5000\n",
            "Epoch 121/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 244ms/step - loss: 128.7982 - mae: 128.7982 - mse: 48238.3242 - val_loss: 242.6139 - val_mae: 242.6139 - val_mse: 190109.6094\n",
            "Epoch 122/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 231ms/step - loss: 103.8458 - mae: 103.8458 - mse: 24789.7285 - val_loss: 277.9431 - val_mae: 277.9431 - val_mse: 248186.8594\n",
            "Epoch 123/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 87.8131 - mae: 87.8131 - mse: 15191.9727 - val_loss: 250.8889 - val_mae: 250.8889 - val_mse: 195895.2500\n",
            "Epoch 124/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 231ms/step - loss: 122.4729 - mae: 122.4729 - mse: 42901.0742 - val_loss: 232.4926 - val_mae: 232.4926 - val_mse: 179618.9688\n",
            "Epoch 125/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 125.1361 - mae: 125.1361 - mse: 39690.5820 - val_loss: 240.8744 - val_mae: 240.8744 - val_mse: 181642.5938\n",
            "Epoch 126/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - loss: 105.9894 - mae: 105.9894 - mse: 25223.9629 - val_loss: 268.3354 - val_mae: 268.3354 - val_mse: 208089.0469\n",
            "Epoch 127/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 261ms/step - loss: 165.0320 - mae: 165.0320 - mse: 66042.6016 - val_loss: 255.2332 - val_mae: 255.2332 - val_mse: 202065.5469\n",
            "Epoch 128/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 235ms/step - loss: 119.2839 - mae: 119.2839 - mse: 30030.1621 - val_loss: 304.4460 - val_mae: 304.4460 - val_mse: 289660.0000\n",
            "Epoch 129/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 111.8325 - mae: 111.8325 - mse: 30244.2695 - val_loss: 231.1598 - val_mae: 231.1598 - val_mse: 169749.3281\n",
            "Epoch 130/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - loss: 116.5344 - mae: 116.5344 - mse: 32066.2129 - val_loss: 271.1495 - val_mae: 271.1495 - val_mse: 253370.9688\n",
            "Epoch 130: early stopping\n",
            "Restoring model weights from the end of the best epoch: 90.\n",
            "\n",
            "================================================================================\n",
            "[EVALUATION]\n",
            "================================================================================\n",
            "\n",
            "RESULTS:\n",
            "  MAE:   160.10 (Target: 70-150)\n",
            "  RMSE:  276.75 (Target: 100-200)\n",
            "\n",
            "TARGET ACHIEVEMENT:\n",
            "  MAE  [70-150]:    FAIL\n",
            "  RMSE [100-200]:   FAIL\n",
            "\n",
            "✓ Saved to: results/csrnet_direct\n",
            "================================================================================\n",
            "\n",
            "MAE 160.10, RMSE 276.75\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nbuPjD6XHkud"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}