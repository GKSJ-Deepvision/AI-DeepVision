{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13858945,"sourceType":"datasetVersion","datasetId":8828840}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install numpy==1.23.5 scipy==1.10.1\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nimport numpy as np\nimport cv2\nimport scipy.io as sio\nfrom pathlib import Path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T04:33:44.999575Z","iopub.execute_input":"2025-11-27T04:33:44.999862Z","iopub.status.idle":"2025-11-27T04:33:45.135564Z","shell.execute_reply.started":"2025-11-27T04:33:44.999839Z","shell.execute_reply":"2025-11-27T04:33:45.134606Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"BASE = Path(\"/kaggle/input/csrnet-dataset/part_B_final/part_B_final\")\n\ntrain_img_dir = BASE / \"train_data/images\"\ntrain_gt_dir  = BASE / \"train_data/ground_truth\"\n\nprint(\"Train images:\", len(os.listdir(train_img_dir)))\nprint(\"Train GT:\", len(os.listdir(train_gt_dir)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T04:33:52.020939Z","iopub.execute_input":"2025-11-27T04:33:52.021248Z","iopub.status.idle":"2025-11-27T04:33:52.030568Z","shell.execute_reply.started":"2025-11-27T04:33:52.021224Z","shell.execute_reply":"2025-11-27T04:33:52.029890Z"}},"outputs":[{"name":"stdout","text":"Train images: 400\nTrain GT: 400\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"train_image_paths = sorted(list(train_img_dir.glob(\"*.jpg\")))\n\ntrain_gt_paths = [\n    train_gt_dir / f\"GT_{p.stem}.mat\"\n    for p in train_image_paths\n]\n\nprint(train_image_paths[0])\nprint(train_gt_paths[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T04:33:59.188953Z","iopub.execute_input":"2025-11-27T04:33:59.189238Z","iopub.status.idle":"2025-11-27T04:33:59.198382Z","shell.execute_reply.started":"2025-11-27T04:33:59.189215Z","shell.execute_reply":"2025-11-27T04:33:59.197600Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/csrnet-dataset/part_B_final/part_B_final/train_data/images/IMG_1.jpg\n/kaggle/input/csrnet-dataset/part_B_final/part_B_final/train_data/ground_truth/GT_IMG_1.mat\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def load_points(gt_path):\n    mat = sio.loadmat(gt_path)\n    pts = mat[\"image_info\"][0][0][0][0][0]\n    return pts\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T04:34:10.132481Z","iopub.execute_input":"2025-11-27T04:34:10.133003Z","iopub.status.idle":"2025-11-27T04:34:10.137016Z","shell.execute_reply.started":"2025-11-27T04:34:10.132976Z","shell.execute_reply":"2025-11-27T04:34:10.136224Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"test_gt = train_gt_paths[0]\nprint(\"Testing:\", test_gt)\n\npts = load_points(test_gt)\nprint(\"Shape:\", pts.shape)\nprint(pts[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T04:34:22.367791Z","iopub.execute_input":"2025-11-27T04:34:22.368432Z","iopub.status.idle":"2025-11-27T04:34:22.377863Z","shell.execute_reply.started":"2025-11-27T04:34:22.368404Z","shell.execute_reply":"2025-11-27T04:34:22.377242Z"}},"outputs":[{"name":"stdout","text":"Testing: /kaggle/input/csrnet-dataset/part_B_final/part_B_final/train_data/ground_truth/GT_IMG_1.mat\nShape: (234, 2)\n[[ 32.67954877 753.52165403]\n [184.60695349 690.79933098]\n [344.89733461 577.8991495 ]\n [328.1713818  536.08426746]\n [329.5652112  473.36194441]]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\n\ndef generate_density_map(shape, points, sigma=5):\n    h, w = shape\n    density = np.zeros((h, w), dtype=np.float32)\n    for x, y in points:\n        x = min(w - 1, max(0, int(x)))\n        y = min(h - 1, max(0, int(y)))\n        density[y, x] += 1\n    density = cv2.GaussianBlur(density, (15, 15), sigma)\n    return density\n\nclass CSRNetDataset(Dataset):\n    def __init__(self, img_paths, gt_paths, transform=None, sigma=5):\n        self.img_paths = img_paths\n        self.gt_paths = gt_paths\n        self.transform = transform\n        self.sigma = sigma\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        gt_path  = self.gt_paths[idx]\n\n        img = cv2.imread(str(img_path))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        h, w, _ = img.shape\n\n        points = load_points(gt_path)\n        density = generate_density_map((h, w), points, sigma=self.sigma)\n\n        if self.transform:\n            img_t = self.transform(img)\n        else:\n            img_t = T.ToTensor()(img)\n\n        dens_t = torch.from_numpy(density).unsqueeze(0)\n\n        return img_t.float(), dens_t.float()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T04:35:47.371849Z","iopub.execute_input":"2025-11-27T04:35:47.372580Z","iopub.status.idle":"2025-11-27T04:35:50.479122Z","shell.execute_reply.started":"2025-11-27T04:35:47.372548Z","shell.execute_reply":"2025-11-27T04:35:50.478527Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"n = len(train_image_paths)\nval_size = int(0.15 * n)\n\ntrain_img = train_image_paths[:-val_size]\ntrain_gt  = train_gt_paths[:-val_size]\n\nval_img   = train_image_paths[-val_size:]\nval_gt    = train_gt_paths[-val_size:]\n\nprint(\"Train:\", len(train_img))\nprint(\"Val:\", len(val_img))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T04:36:02.937421Z","iopub.execute_input":"2025-11-27T04:36:02.938261Z","iopub.status.idle":"2025-11-27T04:36:02.942866Z","shell.execute_reply.started":"2025-11-27T04:36:02.938231Z","shell.execute_reply":"2025-11-27T04:36:02.942197Z"}},"outputs":[{"name":"stdout","text":"Train: 340\nVal: 60\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"transform = T.Compose([\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225])\n])\n\ntrain_ds = CSRNetDataset(train_img, train_gt, transform)\nval_ds   = CSRNetDataset(val_img, val_gt, transform)\n\ntrain_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\nval_loader   = DataLoader(val_ds, batch_size=4, shuffle=False)\n\n# Test batch\nimgs, dens = next(iter(train_loader))\n\nprint(\"Batch images:\", imgs.shape)\nprint(\"Batch density:\", dens.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T04:36:13.573567Z","iopub.execute_input":"2025-11-27T04:36:13.573861Z","iopub.status.idle":"2025-11-27T04:36:13.786404Z","shell.execute_reply.started":"2025-11-27T04:36:13.573838Z","shell.execute_reply":"2025-11-27T04:36:13.785697Z"}},"outputs":[{"name":"stdout","text":"Batch images: torch.Size([4, 3, 768, 1024])\nBatch density: torch.Size([4, 1, 768, 1024])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\nclass CSRNet(nn.Module):\n    def __init__(self):\n        super(CSRNet, self).__init__()\n\n        vgg = models.vgg16_bn(weights=models.VGG16_BN_Weights.IMAGENET1K_V1)\n\n        self.frontend = nn.Sequential(*list(vgg.features.children())[:33])\n\n        self.backend = nn.Sequential(\n            nn.Conv2d(512, 512, 3, padding=2, dilation=2), nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, 3, padding=2, dilation=2), nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, 3, padding=2, dilation=2), nn.ReLU(inplace=True),\n            nn.Conv2d(512, 256, 3, padding=2, dilation=2), nn.ReLU(inplace=True),\n            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(128, 1, 1)\n        )\n\n    def forward(self, x):\n        x = self.frontend(x)\n        x = self.backend(x)\n        x = F.interpolate(x, scale_factor=8, mode='bilinear', align_corners=False)\n        return x\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CSRNet().to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T04:37:38.203315Z","iopub.execute_input":"2025-11-27T04:37:38.203687Z","iopub.status.idle":"2025-11-27T04:37:42.708483Z","shell.execute_reply.started":"2025-11-27T04:37:38.203665Z","shell.execute_reply":"2025-11-27T04:37:42.707660Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n100%|██████████| 528M/528M [00:02<00:00, 206MB/s] \n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.MSELoss(reduction='sum')\noptimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T04:38:02.728629Z","iopub.execute_input":"2025-11-27T04:38:02.729390Z","iopub.status.idle":"2025-11-27T04:38:02.733564Z","shell.execute_reply.started":"2025-11-27T04:38:02.729357Z","shell.execute_reply":"2025-11-27T04:38:02.732772Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def validate(loader):\n    model.eval()\n    mae = 0\n    mse = 0\n\n    with torch.no_grad():\n        for imgs, dens in loader:\n            imgs = imgs.to(device)\n            dens = dens.to(device)\n\n            out = model(imgs)\n\n            pred = out.view(out.size(0), -1).sum(dim=1).cpu()\n            gt   = dens.view(dens.size(0), -1).sum(dim=1).cpu()\n\n            mae += torch.sum(torch.abs(pred - gt)).item()\n            mse += torch.sum((pred - gt)**2).item()\n\n    n = len(loader.dataset)\n    return mae/n, (mse/n)**0.5\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T04:38:21.916689Z","iopub.execute_input":"2025-11-27T04:38:21.917218Z","iopub.status.idle":"2025-11-27T04:38:21.922361Z","shell.execute_reply.started":"2025-11-27T04:38:21.917196Z","shell.execute_reply":"2025-11-27T04:38:21.921528Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import time\n\nprint(\"Running timing test...\")\n\nstart = time.time()\nimgs, dens = next(iter(train_loader))\nimgs, dens = imgs.to(device), dens.to(device)\n\nout = model(imgs)\nloss = criterion(out, dens)\n\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n\nelapsed = time.time() - start\n\nprint(f\"Time for 1 batch: {elapsed:.3f} sec\")\nprint(f\"Estimated epoch time: {(elapsed*len(train_loader))/60:.2f} min\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T04:38:35.015684Z","iopub.execute_input":"2025-11-27T04:38:35.016313Z","iopub.status.idle":"2025-11-27T04:38:36.666607Z","shell.execute_reply.started":"2025-11-27T04:38:35.016288Z","shell.execute_reply":"2025-11-27T04:38:36.665939Z"}},"outputs":[{"name":"stdout","text":"Running timing test...\nTime for 1 batch: 1.646 sec\nEstimated epoch time: 2.33 min\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"best_mae = 1e9\nepochs = 20\n\nfor epoch in range(1, epochs+1):\n    model.train()\n    total_loss = 0\n    t0 = time.time()\n\n    for imgs, dens in train_loader:\n        imgs = imgs.to(device)\n        dens = dens.to(device)\n\n        out = model(imgs)\n        loss = criterion(out, dens)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    val_mae, val_rmse = validate(val_loader)\n    epoch_time = (time.time() - t0) / 60\n\n    print(f\"Epoch {epoch}/{epochs} | Time: {epoch_time:.2f} min | \"\n          f\"Loss: {total_loss:.2f} | MAE: {val_mae:.2f} | RMSE: {val_rmse:.2f}\")\n\n    torch.save(model.state_dict(), f\"/kaggle/working/epoch_{epoch}.pth\")\n\n    if val_mae < best_mae:\n        best_mae = val_mae\n        torch.save(model.state_dict(), \"/kaggle/working/partB_best.pth\")\n        print(\"Saved BEST model!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T04:39:19.105379Z","iopub.execute_input":"2025-11-27T04:39:19.105955Z","iopub.status.idle":"2025-11-27T05:08:04.304175Z","shell.execute_reply.started":"2025-11-27T04:39:19.105932Z","shell.execute_reply":"2025-11-27T05:08:04.303404Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20 | Time: 1.53 min | Loss: 271026.54 | MAE: 931.06 | RMSE: 1106.04\nSaved BEST model!\nEpoch 2/20 | Time: 1.44 min | Loss: 6022.49 | MAE: 724.06 | RMSE: 790.29\nSaved BEST model!\nEpoch 3/20 | Time: 1.43 min | Loss: 1748.19 | MAE: 380.40 | RMSE: 439.47\nSaved BEST model!\nEpoch 4/20 | Time: 1.43 min | Loss: 1275.07 | MAE: 792.22 | RMSE: 815.32\nEpoch 5/20 | Time: 1.43 min | Loss: 912.77 | MAE: 175.05 | RMSE: 213.95\nSaved BEST model!\nEpoch 6/20 | Time: 1.43 min | Loss: 804.83 | MAE: 305.94 | RMSE: 336.03\nEpoch 7/20 | Time: 1.43 min | Loss: 669.50 | MAE: 104.22 | RMSE: 132.22\nSaved BEST model!\nEpoch 8/20 | Time: 1.43 min | Loss: 555.41 | MAE: 212.90 | RMSE: 234.52\nEpoch 9/20 | Time: 1.43 min | Loss: 566.48 | MAE: 90.63 | RMSE: 111.84\nSaved BEST model!\nEpoch 10/20 | Time: 1.43 min | Loss: 467.94 | MAE: 96.64 | RMSE: 120.11\nEpoch 11/20 | Time: 1.43 min | Loss: 441.94 | MAE: 156.62 | RMSE: 174.28\nEpoch 12/20 | Time: 1.43 min | Loss: 412.59 | MAE: 116.95 | RMSE: 131.56\nEpoch 13/20 | Time: 1.43 min | Loss: 411.22 | MAE: 94.96 | RMSE: 109.53\nEpoch 14/20 | Time: 1.43 min | Loss: 366.35 | MAE: 196.74 | RMSE: 207.59\nEpoch 15/20 | Time: 1.43 min | Loss: 363.36 | MAE: 304.87 | RMSE: 311.78\nEpoch 16/20 | Time: 1.43 min | Loss: 353.25 | MAE: 51.20 | RMSE: 67.19\nSaved BEST model!\nEpoch 17/20 | Time: 1.43 min | Loss: 321.62 | MAE: 113.05 | RMSE: 126.89\nEpoch 18/20 | Time: 1.43 min | Loss: 318.43 | MAE: 351.17 | RMSE: 355.01\nEpoch 19/20 | Time: 1.43 min | Loss: 315.18 | MAE: 114.37 | RMSE: 122.00\nEpoch 20/20 | Time: 1.43 min | Loss: 315.23 | MAE: 176.59 | RMSE: 184.20\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"test_img_dir = BASE / \"test_data/images\"\ntest_gt_dir  = BASE / \"test_data/ground_truth\"\n\ntest_image_paths = sorted(list(test_img_dir.glob(\"*.jpg\")))\ntest_gt_paths = [\n    test_gt_dir / f\"GT_{p.stem}.mat\"\n    for p in test_image_paths\n]\n\nprint(\"Test images:\", len(test_image_paths))\nprint(\"Test GT:\", len(test_gt_paths))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:13:24.084222Z","iopub.execute_input":"2025-11-27T05:13:24.084578Z","iopub.status.idle":"2025-11-27T05:13:24.095943Z","shell.execute_reply.started":"2025-11-27T05:13:24.084555Z","shell.execute_reply":"2025-11-27T05:13:24.095350Z"}},"outputs":[{"name":"stdout","text":"Test images: 316\nTest GT: 316\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"test_ds = CSRNetDataset(test_image_paths, test_gt_paths, transform)\ntest_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:13:31.759818Z","iopub.execute_input":"2025-11-27T05:13:31.760580Z","iopub.status.idle":"2025-11-27T05:13:31.764504Z","shell.execute_reply.started":"2025-11-27T05:13:31.760555Z","shell.execute_reply":"2025-11-27T05:13:31.763669Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"best_model_path = \"/kaggle/working/partB_best.pth\"\nmodel.load_state_dict(torch.load(best_model_path))\nmodel.eval()\n\nprint(\"Loaded best model successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:13:41.892793Z","iopub.execute_input":"2025-11-27T05:13:41.893493Z","iopub.status.idle":"2025-11-27T05:13:41.948851Z","shell.execute_reply.started":"2025-11-27T05:13:41.893464Z","shell.execute_reply":"2025-11-27T05:13:41.948213Z"}},"outputs":[{"name":"stdout","text":"Loaded best model successfully!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def evaluate_test(loader):\n    model.eval()\n    mae = 0\n    mse = 0\n    results = []\n\n    with torch.no_grad():\n        for i, (imgs, dens) in enumerate(loader):\n            imgs = imgs.to(device)\n            dens = dens.to(device)\n\n            out = model(imgs)\n\n            pred = out.view(-1).sum().item()\n            gt   = dens.view(-1).sum().item()\n\n            mae += abs(pred - gt)\n            mse += (pred - gt)**2\n\n            results.append((i, pred, gt))\n\n    mae /= len(loader.dataset)\n    rmse = (mse / len(loader.dataset))**0.5\n    return mae, rmse, results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:13:49.446141Z","iopub.execute_input":"2025-11-27T05:13:49.446530Z","iopub.status.idle":"2025-11-27T05:13:49.451997Z","shell.execute_reply.started":"2025-11-27T05:13:49.446505Z","shell.execute_reply":"2025-11-27T05:13:49.451230Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"test_mae, test_rmse, test_results = evaluate_test(test_loader)\n\nprint(\"============== PART B TEST RESULTS ==============\")\nprint(f\"Test MAE  = {test_mae:.2f}\")\nprint(f\"Test RMSE = {test_rmse:.2f}\")\nprint(\"=================================================\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:13:59.931877Z","iopub.execute_input":"2025-11-27T05:13:59.932460Z","iopub.status.idle":"2025-11-27T05:14:34.236174Z","shell.execute_reply.started":"2025-11-27T05:13:59.932434Z","shell.execute_reply":"2025-11-27T05:14:34.235510Z"}},"outputs":[{"name":"stdout","text":"============== PART B TEST RESULTS ==============\nTest MAE  = 49.66\nTest RMSE = 66.20\n=================================================\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ---------- Continue training safely (resume) ----------\nimport time, math, torch\n\n# CONFIG — change if you want\nstart_epoch = 21               # next epoch number (you have finished 20)\ntarget_epochs = 50             # desired final epoch (e.g. 50)\nbatch_time_estimate = None     # will be measured\nquota_limit_hours = 5.0        # conservative weekly GPU quota you saw earlier (change if you know different)\nsafety_margin_hours = 0.5      # leave this much safety margin\n\n# training objects assumed to exist: model, optimizer, criterion, train_loader, val_loader, device\nbest_mae = 1e9\n# if you already have previous best_mae from run, keep it:\ntry:\n    # try to read best_mae from saved value if present\n    import os\n    if os.path.exists(\"/kaggle/working/partB_best.pth\"):\n        # load model temporarily to compute val_mae if you want; otherwise keep 1e9\n        pass\nexcept Exception:\n    pass\n\n# early stopping settings (optional)\nuse_early_stopping = True\npatience = 6   # stop if val MAE doesn't improve for this many epochs\nno_improve = 0\n\n# measure one-batch time (re-run quick batch to estimate)\nimgs, dens = next(iter(train_loader))\nimgs, dens = imgs.to(device), dens.to(device)\ntorch.cuda.synchronize() if torch.cuda.is_available() else None\nt0 = time.time()\nout = model(imgs)\nloss = criterion(out, dens)\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\ntorch.cuda.synchronize() if torch.cuda.is_available() else None\nbatch_seconds = time.time() - t0\nbatches_per_epoch = len(train_loader)\nepoch_seconds = batch_seconds * batches_per_epoch\nprint(f\"Measured: {batch_seconds:.3f}s per batch → est {epoch_seconds/60:.2f} min / epoch\")\n\n# project remaining time if we run until target_epochs\nepochs_left = max(0, target_epochs - (start_epoch-1))\nprojected_hours = (epoch_seconds * epochs_left) / 3600.0\nprint(f\"Projected time for {epochs_left} more epochs: {projected_hours:.2f} hours\")\n\n# check against quota\nif projected_hours + safety_margin_hours > quota_limit_hours:\n    print(\"WARNING: projected run may exceed your quota limit. Reduce target_epochs or run fewer epochs.\")\n    # you can choose to abort here by uncommenting:\n    # raise SystemExit(\"Aborting to avoid exceeding quota. Reduce target_epochs.\")\nelse:\n    print(\"Within quota — starting training...\")\n\n# training loop resume\nfor epoch in range(start_epoch, target_epochs+1):\n    model.train()\n    total_loss = 0.0\n    t_ep = time.time()\n    for imgs, dens in train_loader:\n        imgs = imgs.to(device)\n        dens = dens.to(device)\n\n        out = model(imgs)\n        loss = criterion(out, dens)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    val_mae, val_rmse = validate(val_loader)\n    epoch_time_min = (time.time() - t_ep) / 60.0\n\n    print(f\"Epoch {epoch}/{target_epochs} | Time: {epoch_time_min:.2f} min | \"\n          f\"Loss: {total_loss:.2f} | MAE: {val_mae:.2f} | RMSE: {val_rmse:.2f}\")\n\n    # Save checkpoint\n    torch.save(model.state_dict(), f\"/kaggle/working/epoch_{epoch}.pth\")\n\n    # Save best\n    if val_mae < best_mae:\n        best_mae = val_mae\n        torch.save(model.state_dict(), \"/kaggle/working/partB_best.pth\")\n        print(\"➡️ Saved BEST model!\")\n        no_improve = 0\n    else:\n        no_improve += 1\n\n    # optional early stopping\n    if use_early_stopping and no_improve >= patience:\n        print(f\"No improvement for {patience} epochs — stopping early at epoch {epoch}.\")\n        break\n\nprint(\"Continuing training finished (or stopped). Best MAE:\", best_mae)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:22:26.270994Z","iopub.execute_input":"2025-11-27T05:22:26.271340Z","iopub.status.idle":"2025-11-27T05:36:58.574616Z","shell.execute_reply.started":"2025-11-27T05:22:26.271302Z","shell.execute_reply":"2025-11-27T05:36:58.573782Z"}},"outputs":[{"name":"stdout","text":"Measured: 0.852s per batch → est 1.21 min / epoch\nProjected time for 30 more epochs: 0.60 hours\nWithin quota — starting training...\nEpoch 21/50 | Time: 1.45 min | Loss: 360.05 | MAE: 238.82 | RMSE: 246.31\n➡️ Saved BEST model!\nEpoch 22/50 | Time: 1.45 min | Loss: 325.40 | MAE: 170.80 | RMSE: 179.85\n➡️ Saved BEST model!\nEpoch 23/50 | Time: 1.45 min | Loss: 315.38 | MAE: 426.10 | RMSE: 429.14\nEpoch 24/50 | Time: 1.45 min | Loss: 310.18 | MAE: 53.33 | RMSE: 67.22\n➡️ Saved BEST model!\nEpoch 25/50 | Time: 1.45 min | Loss: 293.27 | MAE: 166.70 | RMSE: 173.25\nEpoch 26/50 | Time: 1.45 min | Loss: 303.59 | MAE: 235.32 | RMSE: 239.56\nEpoch 27/50 | Time: 1.45 min | Loss: 316.82 | MAE: 567.63 | RMSE: 569.58\nEpoch 28/50 | Time: 1.45 min | Loss: 293.28 | MAE: 74.04 | RMSE: 86.08\nEpoch 29/50 | Time: 1.45 min | Loss: 265.15 | MAE: 139.77 | RMSE: 145.55\nEpoch 30/50 | Time: 1.45 min | Loss: 262.90 | MAE: 57.95 | RMSE: 66.27\nNo improvement for 6 epochs — stopping early at epoch 30.\nContinuing training finished (or stopped). Best MAE: 53.32893435160319\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"/kaggle/working/partB_best.pth\"))\nmodel.to(device)\nmodel.eval()\n\ntest_mae, test_rmse, test_results = evaluate_test(test_loader)\n\nprint(\"============== PART B TEST RESULTS ==============\")\nprint(f\"Test MAE  = {test_mae:.2f}\")\nprint(f\"Test RMSE = {test_rmse:.2f}\")\nprint(\"=================================================\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T05:39:54.820953Z","iopub.execute_input":"2025-11-27T05:39:54.821737Z","iopub.status.idle":"2025-11-27T05:40:23.564122Z","shell.execute_reply.started":"2025-11-27T05:39:54.821710Z","shell.execute_reply":"2025-11-27T05:40:23.563387Z"}},"outputs":[{"name":"stdout","text":"============== PART B TEST RESULTS ==============\nTest MAE  = 53.27\nTest RMSE = 66.71\n=================================================\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}