{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "354f5a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ARTI\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Input size: 768 x 1024\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import gradio as gr\n",
    "\n",
    "# Device (CPU-only for your setup)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Input resolution used during training\n",
    "TARGET_H = 768\n",
    "TARGET_W = 1024\n",
    "\n",
    "# ImageNet normalization used in training\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "print(\"Device:\", device)\n",
    "print(\"Input size:\", TARGET_H, \"x\", TARGET_W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c81983b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSRNet model class ready.\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class CSRNet(nn.Module):\n",
    "    def __init__(self, load_pretrained_vgg: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        vgg = models.vgg16(weights=None)\n",
    "\n",
    "        # Front-end (VGG-16 up to Conv4_3)\n",
    "        self.frontend = nn.Sequential(*list(vgg.features.children())[:23])\n",
    "\n",
    "        # Back-end (Dilated CNN layers)\n",
    "        self.backend = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Output: 1-channel density map\n",
    "        self.output_layer = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "print(\"CSRNet model class ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6924db8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from: csrnet_weights.pth\n",
      "Missing keys: []\n",
      "Unexpected keys: []\n",
      "Model loaded and ready for inference.\n"
     ]
    }
   ],
   "source": [
    "# Path to your trained model weights (from training notebook)\n",
    "WEIGHTS_PATH = \"csrnet_weights.pth\"\n",
    "\n",
    "model = CSRNet().to(device)\n",
    "\n",
    "print(\"Loading weights from:\", WEIGHTS_PATH)\n",
    "state = torch.load(WEIGHTS_PATH, map_location=device)\n",
    "\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "model.eval()\n",
    "print(\"Model loaded and ready for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5baaec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame_bgr):\n",
    "    \"\"\"\n",
    "    Convert BGR webcam frame into model-ready tensor.\n",
    "    EXACT same processing as training notebook.\n",
    "    \"\"\"\n",
    "\n",
    "    # BGR → RGB → float32\n",
    "    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "\n",
    "    # Resize to fixed training shape\n",
    "    resized = cv2.resize(rgb, (TARGET_W, TARGET_H))\n",
    "\n",
    "    # Convert to tensor CHW\n",
    "    tensor = torch.from_numpy(resized.transpose(2, 0, 1)).float()\n",
    "\n",
    "    # Apply ImageNet normalization\n",
    "    tensor = TF.normalize(tensor, mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "    return tensor.unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae2eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_count(frame_bgr):\n",
    "    \"\"\"\n",
    "    Perform CSRNet inference:\n",
    "    - Input frame (BGR)\n",
    "    - Output: (count, full-resolution density map)\n",
    "    \"\"\"\n",
    "\n",
    "    inp = preprocess_frame(frame_bgr)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dm_small = model(inp)[0,0].cpu().numpy()\n",
    "\n",
    "    # Upsample density map to original frame size\n",
    "    dm_full = cv2.resize(dm_small, (TARGET_W, TARGET_H), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    # Count = sum of density\n",
    "    count = float(dm_full.sum())\n",
    "\n",
    "    return count, dm_full\n",
    "\n",
    "\n",
    "def generate_heatmap(frame_bgr, density_map):\n",
    "    \"\"\"\n",
    "    Overlay heatmap on the frame.\n",
    "    \"\"\"\n",
    "\n",
    "    dm = density_map.copy()\n",
    "    if dm.max() > 0:\n",
    "        dm = dm / dm.max()\n",
    "\n",
    "    heatmap = (dm * 255).astype(np.uint8)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "    blended = cv2.addWeighted(frame_bgr, 0.6, heatmap, 0.6, 0)\n",
    "\n",
    "    return blended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c34819b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame_rgb):\n",
    "    \"\"\"\n",
    "    Gradio provides RGB frame.\n",
    "    Convert to BGR for OpenCV processing.\n",
    "    \"\"\"\n",
    "\n",
    "    frame_bgr = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Resize for display & processing\n",
    "    frame_bgr = cv2.resize(frame_bgr, (TARGET_W, TARGET_H))\n",
    "\n",
    "    # Predict\n",
    "    count, dm = predict_count(frame_bgr)\n",
    "\n",
    "    # Heatmap overlay\n",
    "    heatmap_bgr = generate_heatmap(frame_bgr, dm)\n",
    "\n",
    "    # Convert outputs back to RGB\n",
    "    live_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    heatmap_rgb = cv2.cvtColor(heatmap_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Alert message\n",
    "    if count > 50:\n",
    "        alert = f\"⚠ ALERT: Crowd exceeds limit! Count = {count:.1f}\"\n",
    "    else:\n",
    "        alert = f\"Count = {count:.1f}\"\n",
    "\n",
    "    return live_rgb, heatmap_rgb, alert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206aab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio\\flagged\\dataset1.csv\n",
      "Error while flagging: field larger than field limit (131072)\n"
     ]
    }
   ],
   "source": [
    "gr.Interface(\n",
    "    fn=process_frame,\n",
    "    inputs=gr.Image(sources=[\"webcam\"], streaming=True),   # <-- FIXED\n",
    "    outputs=[\n",
    "        gr.Image(label=\"Live Webcam Feed\"),\n",
    "        gr.Image(label=\"Density Heatmap\"),\n",
    "        gr.Textbox(label=\"Crowd Status\")\n",
    "    ],\n",
    "    title=\"Real-Time Crowd Monitoring (CSRNet)\",\n",
    "    description=\"Webcam-based crowd estimation using CSRNet.\",\n",
    "    live=True\n",
    ").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
