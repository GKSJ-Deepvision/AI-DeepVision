{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQZ2RPh3XQFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b25fefc-457a-4349-a455-a34870796958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from pathlib import Path\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "class CSRNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = models.vgg16_bn(weights=models.VGG16_BN_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        self.frontend = nn.Sequential(*list(vgg.features.children())[:33])\n",
        "        self.backend = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, 3, padding=2, dilation=2), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, 3, padding=2, dilation=2), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, 3, padding=2, dilation=2), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, 3, padding=2, dilation=2), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, 3, padding=1),            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 1,   1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.frontend(x)\n",
        "        x = self.backend(x)\n",
        "        return x\n",
        "\n",
        "# ⭐ Use your real checkpoint path here\n",
        "WEIGHTS_PATH = Path(\"/content/drive/MyDrive/deepvision/checkpoints_partB/partB_best.pth\")\n",
        "assert WEIGHTS_PATH.exists(), f\"Weights not found: {WEIGHTS_PATH}\"\n",
        "\n",
        "model = CSRNet().to(device)\n",
        "state_dict = torch.load(WEIGHTS_PATH, map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "print(\"✅ Loaded weights from:\", WEIGHTS_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iRxkcwPZNhu",
        "outputId": "c4d3456c-2cae-4d7e-b99e-39514be0030d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 528M/528M [00:02<00:00, 199MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded weights from: /content/drive/MyDrive/deepvision/checkpoints_partB/partB_best.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready: YOLOv8 video annotator with dashboard-style summary (Gradio 5.50.0 compatible)\n",
        "!pip install -q ultralytics==8.1.20\n",
        "!pip install -q matplotlib\n",
        "!pip install -q gradio\n",
        "\n",
        "import os, cv2, tempfile, time, numpy as np, matplotlib.pyplot as plt\n",
        "import gradio as gr\n",
        "import torch\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "YOLO_MODEL = \"yolov8n.pt\"\n",
        "OUT_W, OUT_H = 640, 480\n",
        "\n",
        "print(\"Device:\", DEVICE, \"| Gradio:\", gr.__version__)\n",
        "print(\"Security: forcing torch.load(weights_only=False) to load YOLO (trusted checkpoint)\")\n",
        "\n",
        "# ---------------- SAFE LOAD YOLO ----------------\n",
        "orig_load = torch.load\n",
        "def _forced_load(*a, **k):\n",
        "    k[\"weights_only\"] = False\n",
        "    return orig_load(*a, **k)\n",
        "torch.load = _forced_load\n",
        "from ultralytics import YOLO\n",
        "yolo = YOLO(YOLO_MODEL)\n",
        "torch.load = orig_load\n",
        "print(\"YOLO loaded ✔\")\n",
        "\n",
        "# ------------- helper functions -------------\n",
        "def run_yolo(frame_rgb, conf=0.35):\n",
        "    res = yolo.predict(source=frame_rgb, imgsz=(OUT_W, OUT_H), conf=conf, verbose=False, device=DEVICE)\n",
        "    preds = res[0]\n",
        "    boxes = []\n",
        "    if getattr(preds, \"boxes\", None) is not None:\n",
        "        for box, cls, confv in zip(preds.boxes.xyxy.cpu().numpy(),\n",
        "                                   preds.boxes.cls.cpu().numpy(),\n",
        "                                   preds.boxes.conf.cpu().numpy()):\n",
        "            if preds.names[int(cls)] != \"person\":\n",
        "                continue\n",
        "            x1, y1, x2, y2 = map(int, box[:4])\n",
        "            boxes.append((x1, y1, x2, y2, float(confv)))\n",
        "    return boxes\n",
        "\n",
        "def annotate(frame_rgb, boxes, count):\n",
        "    img = frame_rgb.copy()\n",
        "    for (x1, y1, x2, y2, c) in boxes:\n",
        "        cv2.rectangle(img, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "        cv2.putText(img, f\"{c:.2f}\", (x1, max(12,y1-6)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)\n",
        "    cv2.putText(img, f\"Count: {count}\", (10,35),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
        "    return img\n",
        "\n",
        "def process_video_file(video_file, conf, skip, alert_thr):\n",
        "    cap = cv2.VideoCapture(video_file)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 20.0\n",
        "    out_path = tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False).name\n",
        "    writer = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (OUT_W, OUT_H))\n",
        "\n",
        "    counts = []\n",
        "    frame_idx = 0\n",
        "\n",
        "    while True:\n",
        "        ok, frame_bgr = cap.read()\n",
        "        if not ok:\n",
        "            break\n",
        "        frame_idx += 1\n",
        "        if frame_idx % skip != 0:\n",
        "            writer.write(cv2.resize(frame_bgr, (OUT_W, OUT_H)))\n",
        "            continue\n",
        "\n",
        "        rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "        rgb = cv2.resize(rgb, (OUT_W, OUT_H))\n",
        "\n",
        "        boxes = run_yolo(rgb, conf)\n",
        "        num = len(boxes)\n",
        "        counts.append(num)\n",
        "\n",
        "        annotated_rgb = annotate(rgb, boxes, num)\n",
        "        annotated_bgr = cv2.cvtColor(annotated_rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        if num >= alert_thr:\n",
        "            cv2.putText(annotated_bgr, \"!!! OVERCROWDED !!!\", (10,75),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 3)\n",
        "\n",
        "        writer.write(annotated_bgr)\n",
        "\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "\n",
        "    # save plot\n",
        "    plot_path = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False).name\n",
        "    plt.figure(figsize=(10,3))\n",
        "    if counts:\n",
        "        plt.plot(counts, marker='o', linewidth=1)\n",
        "    plt.title(\"People Count per Processed Frame\")\n",
        "    plt.xlabel(\"Processed frame index\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(plot_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # compute summary values safely\n",
        "    frames_total = frame_idx\n",
        "    frames_processed = len(counts)\n",
        "    avg_count = float(np.mean(counts)) if counts else 0.0\n",
        "    max_count = int(np.max(counts)) if counts else 0\n",
        "    time_s = 0.0  # we can measure if desired\n",
        "\n",
        "    summary = {\n",
        "        \"frames_total\": frames_total,\n",
        "        \"frames_processed\": frames_processed,\n",
        "        \"avg_count\": avg_count,\n",
        "        \"max_count\": max_count,\n",
        "        \"time_s\": time_s\n",
        "    }\n",
        "\n",
        "    return out_path, plot_path, summary\n",
        "\n",
        "# -------------- UI styling HTML for stat tiles --------------\n",
        "def stat_html(num, label, accent=\"#FF7A1A\"):\n",
        "    return f\"\"\"\n",
        "    <div style=\"display:inline-block; background:#111827; padding:14px 18px; margin:6px; border-radius:8px; min-width:140px;\">\n",
        "      <div style=\"font-size:22px; font-weight:700; color:{accent};\">{num}</div>\n",
        "      <div style=\"color:#9CA3AF; font-size:12px;\">{label}</div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# -------------- Wrapper for UI processing --------------\n",
        "def ui_process(video_file, conf, skip, alert_thr):\n",
        "    if not video_file:\n",
        "        return None, None, \"No file uploaded\", stat_html(0, \"Frames processed\"), stat_html(0.00, \"Avg count\"), stat_html(0, \"Max count\")\n",
        "    out_vid, plot, summary = process_video_file(video_file, float(conf), int(skip), int(alert_thr))\n",
        "    # markdown summary (nice formatting)\n",
        "    md = f\"\"\"### Summary\n",
        "\n",
        "- **Frames total:** {summary['frames_total']}\n",
        "- **Frames processed:** {summary['frames_processed']}\n",
        "- **Average count:** **{summary['avg_count']:.2f}**\n",
        "- **Max count:** **{summary['max_count']}**\n",
        "\n",
        "*Processing produced an annotated MP4 and counts-over-time plot.*\n",
        "\"\"\"\n",
        "    # stat tiles\n",
        "    stat_frames = stat_html(summary['frames_processed'], \"Frames processed\")\n",
        "    stat_avg = stat_html(f\"{summary['avg_count']:.2f}\", \"Average count\")\n",
        "    stat_max = stat_html(summary['max_count'], \"Max count\")\n",
        "    return out_vid, plot, md, stat_frames, stat_avg, stat_max\n",
        "\n",
        "# ---------------- Build Gradio UI ----------------\n",
        "with gr.Blocks() as demo:\n",
        "    gr.HTML(\"<div style='font-size:26px; font-weight:700; color:#fff; margin-bottom:4px;'>Crowd Counting — Annotated Video</div>\")\n",
        "    gr.HTML(\"<div style='color:#9CA3AF; margin-bottom:12px;'>Upload a video and click Process. Stats update in the dashboard on the right.</div>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            vid_in = gr.Video(label=\"Upload video (mp4/mov)\")\n",
        "            conf = gr.Slider(0.1, 0.9, value=0.35, label=\"YOLO Confidence\")\n",
        "            skip = gr.Slider(1, 10, value=1, step=1, label=\"Process every N frames\")\n",
        "            alert_thr = gr.Slider(1, 50, value=8, step=1, label=\"Overcrowd Threshold\")\n",
        "            process_btn = gr.Button(\"Process\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            # stat tiles area (HTML components that will be updated)\n",
        "            st_frames = gr.HTML(stat_html(0, \"Frames processed\"), label=\"Frames\")\n",
        "            st_avg = gr.HTML(stat_html(0.00, \"Avg count\"), label=\"Avg\")\n",
        "            st_max = gr.HTML(stat_html(0, \"Max count\"), label=\"Max\")\n",
        "            gr.Markdown(\"### Results\")\n",
        "            out_plot = gr.Image(label=\"Counts over time\")\n",
        "            out_summary = gr.Markdown(\"### Summary\\n\\n_No results yet._\")\n",
        "\n",
        "    out_video = gr.Video(label=\"Annotated video (MP4)\")\n",
        "\n",
        "    process_btn.click(fn=ui_process, inputs=[vid_in, conf, skip, alert_thr],\n",
        "                      outputs=[out_video, out_plot, out_summary, st_frames, st_avg, st_max])\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "P53j6KZ1aiXM",
        "outputId": "fd5425b5-c069-4a5c-ca09-c61392ec237e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | Gradio: 5.50.0\n",
            "Security: forcing torch.load(weights_only=False) to load YOLO (trusted checkpoint)\n",
            "YOLO loaded ✔\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3a78e26b62b67b4b24.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3a78e26b62b67b4b24.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}